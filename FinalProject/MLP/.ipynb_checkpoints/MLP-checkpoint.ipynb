{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron for classification\n",
    "\n",
    "## Health Insurance Cross Sell Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40454.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38294.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>28619.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>27496.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n",
       "0   1    Male   44                1         28.0                   0   \n",
       "1   2    Male   76                1          3.0                   0   \n",
       "2   3    Male   47                1         28.0                   0   \n",
       "3   4    Male   21                1         11.0                   1   \n",
       "4   5  Female   29                1         41.0                   1   \n",
       "\n",
       "  Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Vintage  \\\n",
       "0   > 2 Years            Yes         40454.0                  26.0      217   \n",
       "1    1-2 Year             No         33536.0                  26.0      183   \n",
       "2   > 2 Years            Yes         38294.0                  26.0       27   \n",
       "3    < 1 Year             No         28619.0                 152.0      203   \n",
       "4    < 1 Year             No         27496.0                 152.0       39   \n",
       "\n",
       "   Response  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>190555.000000</td>\n",
       "      <td>38.822584</td>\n",
       "      <td>0.997869</td>\n",
       "      <td>26.388807</td>\n",
       "      <td>0.458210</td>\n",
       "      <td>30564.389581</td>\n",
       "      <td>112.034295</td>\n",
       "      <td>154.347397</td>\n",
       "      <td>0.122563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>110016.836208</td>\n",
       "      <td>15.511611</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>13.229888</td>\n",
       "      <td>0.498251</td>\n",
       "      <td>17213.155057</td>\n",
       "      <td>54.203995</td>\n",
       "      <td>83.671304</td>\n",
       "      <td>0.327936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2630.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>95278.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24405.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>190555.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31669.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>285832.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39400.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>381109.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>540165.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id            Age  Driving_License    Region_Code  \\\n",
       "count  381109.000000  381109.000000    381109.000000  381109.000000   \n",
       "mean   190555.000000      38.822584         0.997869      26.388807   \n",
       "std    110016.836208      15.511611         0.046110      13.229888   \n",
       "min         1.000000      20.000000         0.000000       0.000000   \n",
       "25%     95278.000000      25.000000         1.000000      15.000000   \n",
       "50%    190555.000000      36.000000         1.000000      28.000000   \n",
       "75%    285832.000000      49.000000         1.000000      35.000000   \n",
       "max    381109.000000      85.000000         1.000000      52.000000   \n",
       "\n",
       "       Previously_Insured  Annual_Premium  Policy_Sales_Channel  \\\n",
       "count       381109.000000   381109.000000         381109.000000   \n",
       "mean             0.458210    30564.389581            112.034295   \n",
       "std              0.498251    17213.155057             54.203995   \n",
       "min              0.000000     2630.000000              1.000000   \n",
       "25%              0.000000    24405.000000             29.000000   \n",
       "50%              0.000000    31669.000000            133.000000   \n",
       "75%              1.000000    39400.000000            152.000000   \n",
       "max              1.000000   540165.000000            163.000000   \n",
       "\n",
       "             Vintage       Response  \n",
       "count  381109.000000  381109.000000  \n",
       "mean      154.347397       0.122563  \n",
       "std        83.671304       0.327936  \n",
       "min        10.000000       0.000000  \n",
       "25%        82.000000       0.000000  \n",
       "50%       154.000000       0.000000  \n",
       "75%       227.000000       0.000000  \n",
       "max       299.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0    106415\n",
       "8.0      33877\n",
       "46.0     19749\n",
       "41.0     18263\n",
       "15.0     13308\n",
       "30.0     12191\n",
       "29.0     11065\n",
       "50.0     10243\n",
       "3.0       9251\n",
       "11.0      9232\n",
       "36.0      8797\n",
       "33.0      7654\n",
       "47.0      7436\n",
       "35.0      6942\n",
       "6.0       6280\n",
       "45.0      5605\n",
       "37.0      5501\n",
       "18.0      5153\n",
       "48.0      4681\n",
       "14.0      4678\n",
       "39.0      4644\n",
       "10.0      4374\n",
       "21.0      4266\n",
       "2.0       4038\n",
       "13.0      4036\n",
       "7.0       3279\n",
       "12.0      3198\n",
       "9.0       3101\n",
       "27.0      2823\n",
       "32.0      2787\n",
       "43.0      2639\n",
       "17.0      2617\n",
       "26.0      2587\n",
       "25.0      2503\n",
       "24.0      2415\n",
       "38.0      2026\n",
       "0.0       2021\n",
       "16.0      2007\n",
       "31.0      1960\n",
       "23.0      1960\n",
       "20.0      1935\n",
       "49.0      1832\n",
       "4.0       1801\n",
       "34.0      1664\n",
       "19.0      1535\n",
       "22.0      1309\n",
       "40.0      1295\n",
       "5.0       1279\n",
       "1.0       1008\n",
       "44.0       808\n",
       "42.0       591\n",
       "52.0       267\n",
       "51.0       183\n",
       "Name: Region_Code, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Region_Code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26., 152., 160., 124.,  14.,  13.,  30., 156., 163., 157., 122.,\n",
       "        19.,  22.,  15., 154.,  16.,  52., 155.,  11., 151., 125.,  25.,\n",
       "        61.,   1.,  86.,  31., 150.,  23.,  60.,  21., 121.,   3., 139.,\n",
       "        12.,  29.,  55.,   7.,  47., 127., 153.,  78., 158.,  89.,  32.,\n",
       "         8.,  10., 120.,  65.,   4.,  42.,  83., 136.,  24.,  18.,  56.,\n",
       "        48., 106.,  54.,  93., 116.,  91.,  45.,   9., 145., 147.,  44.,\n",
       "       109.,  37., 140., 107., 128., 131., 114., 118., 159., 119., 105.,\n",
       "       135.,  62., 138., 129.,  88.,  92., 111., 113.,  73.,  36.,  28.,\n",
       "        35.,  59.,  53., 148., 133., 108.,  64.,  39.,  94., 132.,  46.,\n",
       "        81., 103.,  90.,  51.,  27., 146.,  63.,  96.,  40.,  66., 100.,\n",
       "        95., 123.,  98.,  75.,  69., 130., 134.,  49.,  97.,  38.,  17.,\n",
       "       110.,  80.,  71., 117.,  58.,  20.,  76., 104.,  87.,  84., 137.,\n",
       "       126.,  68.,  67., 101., 115.,  57.,  82.,  79., 112.,  99.,  70.,\n",
       "         2.,  34.,  33.,  74., 102., 149.,  43.,   6.,  50., 144., 143.,\n",
       "        41.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Policy_Sales_Channel.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([134784,  79700,  73995,  21779,  10661,   9930,   6684,   5993,\n",
       "         3885,   2893,   1865,   1848,   1598,   1515,   1410,   1264,\n",
       "         1234,   1203,   1074,   1055,   1026,    888,    843,    783,\n",
       "          769,    750,    631,    622,    607,    579,    523,    517,\n",
       "          509,    492,    422,    332,    312,    264,    222,    185,\n",
       "          184,    175,    174,    169,    167,    158,    154,    152,\n",
       "          148,    143,    137,    132,    127,    124,    121,    110,\n",
       "          107,    104,    103,    101,    100,     89,     85,     77,\n",
       "           75,     72,     68,     65,     64,     63,     62,     59,\n",
       "           54,     52,     51,     48,     47,     46,     44,     38,\n",
       "           34,     32,     28,     27,     26,     24,     23,     22,\n",
       "           21,     20,     19,     18,     16,     15,     14,     13,\n",
       "           12,     11,     10,      9,      8,      7,      6,      5,\n",
       "            4,      3,      2,      1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-2 Year     200316\n",
       "< 1 Year     164786\n",
       "> 2 Years     16007\n",
       "Name: Vehicle_Age, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Vehicle_Age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    334399\n",
       "1     46710\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Response.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    380297\n",
       "0       812\n",
       "Name: Driving_License, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Driving_License.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    206481\n",
       "1    174628\n",
       "Name: Previously_Insured, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Previously_Insured.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((train_df.Vintage))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(train_df.Age))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(train_df.Annual_Premium))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_onehot = onehot.fit_transform(np.array(train_df.Gender).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40454.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38294.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>28619.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>27496.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381104</th>\n",
       "      <td>381105</td>\n",
       "      <td>Male</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>30170.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381105</th>\n",
       "      <td>381106</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>40016.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381106</th>\n",
       "      <td>381107</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>35118.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381107</th>\n",
       "      <td>381108</td>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>44617.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381108</th>\n",
       "      <td>381109</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>41777.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381109 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n",
       "0            1    Male   44                1         28.0                   0   \n",
       "1            2    Male   76                1          3.0                   0   \n",
       "2            3    Male   47                1         28.0                   0   \n",
       "3            4    Male   21                1         11.0                   1   \n",
       "4            5  Female   29                1         41.0                   1   \n",
       "...        ...     ...  ...              ...          ...                 ...   \n",
       "381104  381105    Male   74                1         26.0                   1   \n",
       "381105  381106    Male   30                1         37.0                   1   \n",
       "381106  381107    Male   21                1         30.0                   1   \n",
       "381107  381108  Female   68                1         14.0                   0   \n",
       "381108  381109    Male   46                1         29.0                   0   \n",
       "\n",
       "       Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  \\\n",
       "0        > 2 Years            Yes         40454.0                  26.0   \n",
       "1         1-2 Year             No         33536.0                  26.0   \n",
       "2        > 2 Years            Yes         38294.0                  26.0   \n",
       "3         < 1 Year             No         28619.0                 152.0   \n",
       "4         < 1 Year             No         27496.0                 152.0   \n",
       "...            ...            ...             ...                   ...   \n",
       "381104    1-2 Year             No         30170.0                  26.0   \n",
       "381105    < 1 Year             No         40016.0                 152.0   \n",
       "381106    < 1 Year             No         35118.0                 160.0   \n",
       "381107   > 2 Years            Yes         44617.0                 124.0   \n",
       "381108    1-2 Year             No         41777.0                  26.0   \n",
       "\n",
       "        Vintage  Response  \n",
       "0           217         1  \n",
       "1           183         0  \n",
       "2            27         1  \n",
       "3           203         0  \n",
       "4            39         0  \n",
       "...         ...       ...  \n",
       "381104       88         0  \n",
       "381105      131         0  \n",
       "381106      161         0  \n",
       "381107       74         0  \n",
       "381108      237         0  \n",
       "\n",
       "[381109 rows x 12 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         44\n",
       "1         76\n",
       "2         47\n",
       "3         21\n",
       "4         29\n",
       "          ..\n",
       "381104    74\n",
       "381105    30\n",
       "381106    21\n",
       "381107    68\n",
       "381108    46\n",
       "Name: Age, Length: 381109, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_age_onehot = onehot.transform(np.array(train_df.Vehicle_Age).reshape(-1,1))\n",
    "region_code_onehot = onehot.fit_transform(np.array(train_df.Region_Code).reshape(-1,1))\n",
    "policy_sales_onehot = onehot.fit_transform(np.array(train_df.Policy_Sales_Channel).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_onehot = np.array([i=='Male' for i in train_df.Gender ],dtype=np.int).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vehicle_Damage_onehot = np.array([i=='Yes' for i in train_df.Vehicle_Damage],dtype=np.int).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_scaled = minmax.fit_transform(np.array(train_df.Age).reshape(-1,1))\n",
    "annual_premium_scaled = minmax.fit_transform(np.log(np.array(train_df.Annual_Premium)).reshape(-1,1))\n",
    "vintage_scaled = minmax.fit_transform(np.array(train_df.Vintage).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_license = np.array(train_df.Driving_License).reshape(-1,1)\n",
    "previously_insured = np.array(train_df.Previously_Insured).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df.Response).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = np.concatenate((vehicle_age_onehot, region_code_onehot, policy_sales_onehot, Gender_onehot, Vehicle_Damage_onehot, age_scaled, annual_premium_scaled, vintage_scaled, driving_license, previously_insured, y_train),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_1 = clean_df[clean_df[:,218]==1]\n",
    "class_0 = clean_df[clean_df[:,218]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = np.array(pd.DataFrame(class_1).sample(n =  150000, replace = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = np.array(pd.DataFrame(class_0).sample(n =  150000, replace = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = np.concatenate((class_1, class_0), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[:,218].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_df[:,:218], final_df[:,218], test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation = \"relu\", input_shape = (1,218)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 1, 512)            112128    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1, 1024)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1, 256)            131328    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1, 64)             16448     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1, 32)             2080      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 1,312,129\n",
      "Trainable params: 1,312,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adagrad\", loss = \"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train,axis = 2)\n",
    "X_test = np.expand_dims(X_test, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1,218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1,1,218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229500 samples, validate on 25500 samples\n",
      "Epoch 1/50\n",
      "229500/229500 [==============================] - 23s 100us/step - loss: 0.4329 - accuracy: 0.7977 - val_loss: 0.4208 - val_accuracy: 0.8025\n",
      "Epoch 2/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4234 - accuracy: 0.8006 - val_loss: 0.4184 - val_accuracy: 0.8030\n",
      "Epoch 3/50\n",
      "229500/229500 [==============================] - 22s 97us/step - loss: 0.4196 - accuracy: 0.8023 - val_loss: 0.4153 - val_accuracy: 0.8026\n",
      "Epoch 4/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4166 - accuracy: 0.8041 - val_loss: 0.4135 - val_accuracy: 0.8056\n",
      "Epoch 5/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.4137 - accuracy: 0.8059 - val_loss: 0.4124 - val_accuracy: 0.8045\n",
      "Epoch 6/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4115 - accuracy: 0.8067 - val_loss: 0.4127 - val_accuracy: 0.8056\n",
      "Epoch 7/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4093 - accuracy: 0.8073 - val_loss: 0.4110 - val_accuracy: 0.8057\n",
      "Epoch 8/50\n",
      "229500/229500 [==============================] - 23s 101us/step - loss: 0.4077 - accuracy: 0.8085 - val_loss: 0.4102 - val_accuracy: 0.8064\n",
      "Epoch 9/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4052 - accuracy: 0.8093 - val_loss: 0.4094 - val_accuracy: 0.8077\n",
      "Epoch 10/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4039 - accuracy: 0.8104 - val_loss: 0.4081 - val_accuracy: 0.8078\n",
      "Epoch 11/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4014 - accuracy: 0.8121 - val_loss: 0.4069 - val_accuracy: 0.8090\n",
      "Epoch 12/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.4004 - accuracy: 0.8119 - val_loss: 0.4068 - val_accuracy: 0.8087\n",
      "Epoch 13/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3989 - accuracy: 0.8131 - val_loss: 0.4067 - val_accuracy: 0.8095\n",
      "Epoch 14/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.3969 - accuracy: 0.8136 - val_loss: 0.4056 - val_accuracy: 0.8096\n",
      "Epoch 15/50\n",
      "229500/229500 [==============================] - 22s 98us/step - loss: 0.3957 - accuracy: 0.8148 - val_loss: 0.4052 - val_accuracy: 0.8101\n",
      "Epoch 16/50\n",
      "229500/229500 [==============================] - 23s 101us/step - loss: 0.3943 - accuracy: 0.8156 - val_loss: 0.4053 - val_accuracy: 0.8107\n",
      "Epoch 17/50\n",
      "229500/229500 [==============================] - 22s 97us/step - loss: 0.3926 - accuracy: 0.8160 - val_loss: 0.4035 - val_accuracy: 0.8109\n",
      "Epoch 18/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3914 - accuracy: 0.8165 - val_loss: 0.4036 - val_accuracy: 0.8109\n",
      "Epoch 19/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3904 - accuracy: 0.8173 - val_loss: 0.4045 - val_accuracy: 0.8110\n",
      "Epoch 20/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.3894 - accuracy: 0.8183 - val_loss: 0.4033 - val_accuracy: 0.8122\n",
      "Epoch 21/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3882 - accuracy: 0.8188 - val_loss: 0.4030 - val_accuracy: 0.8107\n",
      "Epoch 22/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3869 - accuracy: 0.8197 - val_loss: 0.4022 - val_accuracy: 0.8132\n",
      "Epoch 23/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3857 - accuracy: 0.8203 - val_loss: 0.4023 - val_accuracy: 0.8139\n",
      "Epoch 24/50\n",
      "229500/229500 [==============================] - 22s 94us/step - loss: 0.3852 - accuracy: 0.8207 - val_loss: 0.4017 - val_accuracy: 0.8142\n",
      "Epoch 25/50\n",
      "145312/229500 [=================>............] - ETA: 7s - loss: 0.3836 - accuracy: 0.8207"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-246-fd271a585c4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 50, callbacks=callbacks, batch_size = 32, validation_split= 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation = \"relu\", input_shape = (1,218)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 1, 1024)           224256    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1, 1024)           1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1, 256)            131328    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1, 128)            32896     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1, 64)             8256      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1, 1)              65        \n",
      "=================================================================\n",
      "Total params: 1,979,393\n",
      "Trainable params: 1,975,297\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = adagrad , loss = \"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229500 samples, validate on 25500 samples\n",
      "Epoch 1/50\n",
      "229500/229500 [==============================] - 14s 60us/step - loss: 0.4327 - accuracy: 0.7950 - val_loss: 0.4255 - val_accuracy: 0.7982\n",
      "Epoch 2/50\n",
      "  2880/229500 [..............................] - ETA: 12s - loss: 0.4406 - accuracy: 0.7878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpcar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4283 - accuracy: 0.7973 - val_loss: 0.4208 - val_accuracy: 0.8006\n",
      "Epoch 3/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4253 - accuracy: 0.7982 - val_loss: 0.4178 - val_accuracy: 0.8030\n",
      "Epoch 4/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4237 - accuracy: 0.7996 - val_loss: 0.4206 - val_accuracy: 0.7998\n",
      "Epoch 5/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4223 - accuracy: 0.7995 - val_loss: 0.4155 - val_accuracy: 0.8038\n",
      "Epoch 6/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4208 - accuracy: 0.8004 - val_loss: 0.4158 - val_accuracy: 0.8025\n",
      "Epoch 7/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4202 - accuracy: 0.8008 - val_loss: 0.4162 - val_accuracy: 0.8016\n",
      "Epoch 8/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4185 - accuracy: 0.8012 - val_loss: 0.4162 - val_accuracy: 0.8028\n",
      "Epoch 9/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4174 - accuracy: 0.8020 - val_loss: 0.4145 - val_accuracy: 0.8050\n",
      "Epoch 10/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4165 - accuracy: 0.8023 - val_loss: 0.4126 - val_accuracy: 0.8055\n",
      "Epoch 11/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4157 - accuracy: 0.8025 - val_loss: 0.4138 - val_accuracy: 0.8052\n",
      "Epoch 12/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4146 - accuracy: 0.8036 - val_loss: 0.4148 - val_accuracy: 0.8029\n",
      "Epoch 13/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4137 - accuracy: 0.8039 - val_loss: 0.4124 - val_accuracy: 0.8045\n",
      "Epoch 14/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4127 - accuracy: 0.8049 - val_loss: 0.4121 - val_accuracy: 0.8052\n",
      "Epoch 15/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4119 - accuracy: 0.8053 - val_loss: 0.4123 - val_accuracy: 0.8054\n",
      "Epoch 16/50\n",
      "229500/229500 [==============================] - 13s 59us/step - loss: 0.4110 - accuracy: 0.8061 - val_loss: 0.4101 - val_accuracy: 0.8077\n",
      "Epoch 17/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4101 - accuracy: 0.8061 - val_loss: 0.4106 - val_accuracy: 0.8054\n",
      "Epoch 18/50\n",
      "229500/229500 [==============================] - 13s 59us/step - loss: 0.4091 - accuracy: 0.8069 - val_loss: 0.4098 - val_accuracy: 0.8070\n",
      "Epoch 19/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4083 - accuracy: 0.8072 - val_loss: 0.4095 - val_accuracy: 0.8078\n",
      "Epoch 20/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4070 - accuracy: 0.8079 - val_loss: 0.4071 - val_accuracy: 0.8098\n",
      "Epoch 21/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4058 - accuracy: 0.8080 - val_loss: 0.4090 - val_accuracy: 0.8078\n",
      "Epoch 22/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4047 - accuracy: 0.8091 - val_loss: 0.4070 - val_accuracy: 0.8096\n",
      "Epoch 23/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4039 - accuracy: 0.8093 - val_loss: 0.4084 - val_accuracy: 0.8070\n",
      "Epoch 24/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4031 - accuracy: 0.8098 - val_loss: 0.4069 - val_accuracy: 0.8080\n",
      "Epoch 25/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4021 - accuracy: 0.8102 - val_loss: 0.4061 - val_accuracy: 0.8100\n",
      "Epoch 26/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4007 - accuracy: 0.8112 - val_loss: 0.4057 - val_accuracy: 0.8085\n",
      "Epoch 27/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4000 - accuracy: 0.8110 - val_loss: 0.4052 - val_accuracy: 0.8095\n",
      "Epoch 28/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3998 - accuracy: 0.8113 - val_loss: 0.4077 - val_accuracy: 0.8096\n",
      "Epoch 29/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3990 - accuracy: 0.8116 - val_loss: 0.4060 - val_accuracy: 0.8107\n",
      "Epoch 30/50\n",
      "229500/229500 [==============================] - 13s 59us/step - loss: 0.3979 - accuracy: 0.8123 - val_loss: 0.4060 - val_accuracy: 0.8091\n",
      "Epoch 31/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3967 - accuracy: 0.8128 - val_loss: 0.4047 - val_accuracy: 0.8109\n",
      "Epoch 32/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3956 - accuracy: 0.8135 - val_loss: 0.4047 - val_accuracy: 0.8108\n",
      "Epoch 33/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3951 - accuracy: 0.8141 - val_loss: 0.4054 - val_accuracy: 0.8108\n",
      "Epoch 34/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3946 - accuracy: 0.8143 - val_loss: 0.4044 - val_accuracy: 0.8116\n",
      "Epoch 35/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3929 - accuracy: 0.8149 - val_loss: 0.4040 - val_accuracy: 0.8110\n",
      "Epoch 36/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3923 - accuracy: 0.8155 - val_loss: 0.4036 - val_accuracy: 0.8122\n",
      "Epoch 37/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3914 - accuracy: 0.8158 - val_loss: 0.4047 - val_accuracy: 0.8118\n",
      "Epoch 38/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.3901 - accuracy: 0.8159 - val_loss: 0.4040 - val_accuracy: 0.8127\n",
      "Epoch 39/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3899 - accuracy: 0.8166 - val_loss: 0.4046 - val_accuracy: 0.8115\n",
      "Epoch 40/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3890 - accuracy: 0.8173 - val_loss: 0.4046 - val_accuracy: 0.8136\n",
      "Epoch 41/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3883 - accuracy: 0.8177 - val_loss: 0.4035 - val_accuracy: 0.8136\n",
      "Epoch 42/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3868 - accuracy: 0.8188 - val_loss: 0.4041 - val_accuracy: 0.8131\n",
      "Epoch 43/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3867 - accuracy: 0.8182 - val_loss: 0.4033 - val_accuracy: 0.8139\n",
      "Epoch 44/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3856 - accuracy: 0.8184 - val_loss: 0.4034 - val_accuracy: 0.8134\n",
      "Epoch 45/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3840 - accuracy: 0.8197 - val_loss: 0.4044 - val_accuracy: 0.8120\n",
      "Epoch 46/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3842 - accuracy: 0.8195 - val_loss: 0.4040 - val_accuracy: 0.8141\n",
      "Epoch 47/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3830 - accuracy: 0.8204 - val_loss: 0.4018 - val_accuracy: 0.8148\n",
      "Epoch 48/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.3821 - accuracy: 0.8206 - val_loss: 0.4040 - val_accuracy: 0.8144\n",
      "Epoch 49/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3811 - accuracy: 0.8217 - val_loss: 0.4022 - val_accuracy: 0.8154\n",
      "Epoch 50/50\n",
      "229500/229500 [==============================] - 13s 59us/step - loss: 0.3805 - accuracy: 0.8214 - val_loss: 0.4037 - val_accuracy: 0.8160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f4c6fd3b48>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 50, callbacks=callbacks, batch_size = 64, validation_split= 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229500 samples, validate on 25500 samples\n",
      "Epoch 1/50\n",
      "229500/229500 [==============================] - 4s 16us/step - loss: 0.3762 - accuracy: 0.8238 - val_loss: 0.4029 - val_accuracy: 0.8164\n",
      "Epoch 2/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3758 - accuracy: 0.8250 - val_loss: 0.4051 - val_accuracy: 0.8173\n",
      "Epoch 3/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3741 - accuracy: 0.8249 - val_loss: 0.4014 - val_accuracy: 0.8178\n",
      "Epoch 4/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3746 - accuracy: 0.8255 - val_loss: 0.3982 - val_accuracy: 0.8187\n",
      "Epoch 5/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3723 - accuracy: 0.8255 - val_loss: 0.3985 - val_accuracy: 0.8181\n",
      "Epoch 6/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3715 - accuracy: 0.8263 - val_loss: 0.4001 - val_accuracy: 0.8174\n",
      "Epoch 7/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3711 - accuracy: 0.8266 - val_loss: 0.4017 - val_accuracy: 0.8182\n",
      "Epoch 8/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3709 - accuracy: 0.8265 - val_loss: 0.4040 - val_accuracy: 0.8177\n",
      "Epoch 9/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3699 - accuracy: 0.8266 - val_loss: 0.4004 - val_accuracy: 0.8199\n",
      "Epoch 10/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3692 - accuracy: 0.8275 - val_loss: 0.4055 - val_accuracy: 0.8200\n",
      "Epoch 11/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3697 - accuracy: 0.8278 - val_loss: 0.4095 - val_accuracy: 0.8185\n",
      "Epoch 12/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3690 - accuracy: 0.8279 - val_loss: 0.4006 - val_accuracy: 0.8190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f4c9d94888>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 50, callbacks=callbacks, batch_size = 512, validation_split= 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation = \"relu\", input_shape = (1,218), activity_regularizer= tf.keras.regularizers.l1(0.0000025), bias_regularizer = tf.keras.regularizers.l1(0.0000025)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation = \"relu\", activity_regularizer= tf.keras.regularizers.l1(0.0000025), bias_regularizer= tf.keras.regularizers.l1(0.0000025)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_240 (Dense)            (None, 1, 1024)           224256    \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 1, 512)            2048      \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 1, 256)            131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_151 (Bat (None, 1, 256)            1024      \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 1, 128)            32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 1, 128)            512       \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 1, 64)             8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_153 (Bat (None, 1, 64)             256       \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 1, 32)             2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_154 (Bat (None, 1, 32)             128       \n",
      "_________________________________________________________________\n",
      "dense_246 (Dense)            (None, 1, 16)             528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_155 (Bat (None, 1, 16)             64        \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 1, 1)              17        \n",
      "=================================================================\n",
      "Total params: 932,289\n",
      "Trainable params: 928,225\n",
      "Non-trainable params: 4,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216750 samples, validate on 38250 samples\n",
      "Epoch 1/1000\n",
      "216750/216750 [==============================] - 2s 11us/step - loss: 0.7026 - accuracy: 0.7696 - val_loss: 0.7035 - val_accuracy: 0.5032\n",
      "Epoch 2/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4531 - accuracy: 0.7947 - val_loss: 0.7067 - val_accuracy: 0.5032\n",
      "Epoch 3/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4394 - accuracy: 0.7989 - val_loss: 0.7017 - val_accuracy: 0.5032\n",
      "Epoch 4/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4319 - accuracy: 0.8014 - val_loss: 0.6982 - val_accuracy: 0.5032\n",
      "Epoch 5/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4261 - accuracy: 0.8047 - val_loss: 0.7138 - val_accuracy: 0.4968\n",
      "Epoch 6/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4216 - accuracy: 0.8061 - val_loss: 0.7041 - val_accuracy: 0.4972\n",
      "Epoch 7/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4180 - accuracy: 0.8077 - val_loss: 0.6636 - val_accuracy: 0.5531\n",
      "Epoch 8/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4141 - accuracy: 0.8098 - val_loss: 0.6685 - val_accuracy: 0.5277\n",
      "Epoch 9/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4104 - accuracy: 0.8113 - val_loss: 0.6407 - val_accuracy: 0.6738\n",
      "Epoch 10/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4073 - accuracy: 0.8122 - val_loss: 0.6311 - val_accuracy: 0.6180\n",
      "Epoch 11/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4046 - accuracy: 0.8141 - val_loss: 0.5489 - val_accuracy: 0.7693\n",
      "Epoch 12/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4027 - accuracy: 0.8152 - val_loss: 0.4847 - val_accuracy: 0.7953\n",
      "Epoch 13/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.4001 - accuracy: 0.8161 - val_loss: 0.4660 - val_accuracy: 0.7959\n",
      "Epoch 14/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3986 - accuracy: 0.8178 - val_loss: 0.4201 - val_accuracy: 0.8088\n",
      "Epoch 15/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3952 - accuracy: 0.8195 - val_loss: 0.4209 - val_accuracy: 0.8083\n",
      "Epoch 16/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3941 - accuracy: 0.8195 - val_loss: 0.4254 - val_accuracy: 0.8094\n",
      "Epoch 17/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3912 - accuracy: 0.8213 - val_loss: 0.4142 - val_accuracy: 0.8122\n",
      "Epoch 18/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3894 - accuracy: 0.8220 - val_loss: 0.4132 - val_accuracy: 0.8105\n",
      "Epoch 19/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3883 - accuracy: 0.8234 - val_loss: 0.4133 - val_accuracy: 0.8120\n",
      "Epoch 20/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3860 - accuracy: 0.8233 - val_loss: 0.4118 - val_accuracy: 0.8130\n",
      "Epoch 21/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3843 - accuracy: 0.8251 - val_loss: 0.4345 - val_accuracy: 0.8107\n",
      "Epoch 22/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3823 - accuracy: 0.8256 - val_loss: 0.4154 - val_accuracy: 0.8135\n",
      "Epoch 23/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3815 - accuracy: 0.8260 - val_loss: 0.4152 - val_accuracy: 0.8122\n",
      "Epoch 24/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3806 - accuracy: 0.8268 - val_loss: 0.4140 - val_accuracy: 0.8158\n",
      "Epoch 25/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3788 - accuracy: 0.8278 - val_loss: 0.4157 - val_accuracy: 0.8139\n",
      "Epoch 26/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3765 - accuracy: 0.8283 - val_loss: 0.4173 - val_accuracy: 0.8126\n",
      "Epoch 27/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3768 - accuracy: 0.8288 - val_loss: 0.4153 - val_accuracy: 0.8156\n",
      "Epoch 28/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3752 - accuracy: 0.8296 - val_loss: 0.4139 - val_accuracy: 0.8151\n",
      "Epoch 29/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3731 - accuracy: 0.8298 - val_loss: 0.4158 - val_accuracy: 0.8167\n",
      "Epoch 30/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3722 - accuracy: 0.8309 - val_loss: 0.4155 - val_accuracy: 0.8166\n",
      "Epoch 31/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3712 - accuracy: 0.8309 - val_loss: 0.4245 - val_accuracy: 0.8148\n",
      "Epoch 32/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3701 - accuracy: 0.8322 - val_loss: 0.4128 - val_accuracy: 0.8175\n",
      "Epoch 33/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3686 - accuracy: 0.8324 - val_loss: 0.4203 - val_accuracy: 0.8171\n",
      "Epoch 34/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3680 - accuracy: 0.8332 - val_loss: 0.4166 - val_accuracy: 0.8175\n",
      "Epoch 35/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3669 - accuracy: 0.8332 - val_loss: 0.4168 - val_accuracy: 0.8163\n",
      "Epoch 36/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3666 - accuracy: 0.8337 - val_loss: 0.4151 - val_accuracy: 0.8170\n",
      "Epoch 37/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3655 - accuracy: 0.8337 - val_loss: 0.4150 - val_accuracy: 0.8177\n",
      "Epoch 38/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3644 - accuracy: 0.8346 - val_loss: 0.4195 - val_accuracy: 0.8180\n",
      "Epoch 39/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3630 - accuracy: 0.8349 - val_loss: 0.4190 - val_accuracy: 0.8184\n",
      "Epoch 40/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3622 - accuracy: 0.8359 - val_loss: 0.4215 - val_accuracy: 0.8184\n",
      "Epoch 41/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3613 - accuracy: 0.8358 - val_loss: 0.4175 - val_accuracy: 0.8190\n",
      "Epoch 42/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3601 - accuracy: 0.8366 - val_loss: 0.4200 - val_accuracy: 0.8181\n",
      "Epoch 43/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3592 - accuracy: 0.8369 - val_loss: 0.4178 - val_accuracy: 0.8205\n",
      "Epoch 44/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3586 - accuracy: 0.8371 - val_loss: 0.4237 - val_accuracy: 0.8203\n",
      "Epoch 45/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3578 - accuracy: 0.8373 - val_loss: 0.4215 - val_accuracy: 0.8198\n",
      "Epoch 46/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3575 - accuracy: 0.8379 - val_loss: 0.4194 - val_accuracy: 0.8188\n",
      "Epoch 47/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3558 - accuracy: 0.8383 - val_loss: 0.4236 - val_accuracy: 0.8207\n",
      "Epoch 48/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3552 - accuracy: 0.8387 - val_loss: 0.4228 - val_accuracy: 0.8189\n",
      "Epoch 49/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3551 - accuracy: 0.8392 - val_loss: 0.4199 - val_accuracy: 0.8202\n",
      "Epoch 50/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3542 - accuracy: 0.8397 - val_loss: 0.4278 - val_accuracy: 0.8215\n",
      "Epoch 51/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3533 - accuracy: 0.8397 - val_loss: 0.4208 - val_accuracy: 0.8208\n",
      "Epoch 52/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3530 - accuracy: 0.8397 - val_loss: 0.4231 - val_accuracy: 0.8214\n",
      "Epoch 53/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3530 - accuracy: 0.8399 - val_loss: 0.4255 - val_accuracy: 0.8213\n",
      "Epoch 54/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3516 - accuracy: 0.8407 - val_loss: 0.4471 - val_accuracy: 0.8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3512 - accuracy: 0.8406 - val_loss: 0.4312 - val_accuracy: 0.8207\n",
      "Epoch 56/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3508 - accuracy: 0.8410 - val_loss: 0.4333 - val_accuracy: 0.8214\n",
      "Epoch 57/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3507 - accuracy: 0.8412 - val_loss: 0.4316 - val_accuracy: 0.8212\n",
      "Epoch 58/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3491 - accuracy: 0.8415 - val_loss: 0.4303 - val_accuracy: 0.8199\n",
      "Epoch 59/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3484 - accuracy: 0.8414 - val_loss: 0.4269 - val_accuracy: 0.8233\n",
      "Epoch 60/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3478 - accuracy: 0.8422 - val_loss: 0.4288 - val_accuracy: 0.8216\n",
      "Epoch 61/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3477 - accuracy: 0.8422 - val_loss: 0.4354 - val_accuracy: 0.8214\n",
      "Epoch 62/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3468 - accuracy: 0.8425 - val_loss: 0.4298 - val_accuracy: 0.8236\n",
      "Epoch 63/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3457 - accuracy: 0.8428 - val_loss: 0.4300 - val_accuracy: 0.8199\n",
      "Epoch 64/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3458 - accuracy: 0.8426 - val_loss: 0.4366 - val_accuracy: 0.8221\n",
      "Epoch 65/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3449 - accuracy: 0.8432 - val_loss: 0.4294 - val_accuracy: 0.8230\n",
      "Epoch 66/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3438 - accuracy: 0.8438 - val_loss: 0.4262 - val_accuracy: 0.8228\n",
      "Epoch 67/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3442 - accuracy: 0.8437 - val_loss: 0.4392 - val_accuracy: 0.8213\n",
      "Epoch 68/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3435 - accuracy: 0.8440 - val_loss: 0.4311 - val_accuracy: 0.8213\n",
      "Epoch 69/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3426 - accuracy: 0.8444 - val_loss: 0.4318 - val_accuracy: 0.8225\n",
      "Epoch 70/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3426 - accuracy: 0.8442 - val_loss: 0.4327 - val_accuracy: 0.8223\n",
      "Epoch 71/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3425 - accuracy: 0.8441 - val_loss: 0.4361 - val_accuracy: 0.8233\n",
      "Epoch 72/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3410 - accuracy: 0.8447 - val_loss: 0.4339 - val_accuracy: 0.8237\n",
      "Epoch 73/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3408 - accuracy: 0.8452 - val_loss: 0.4380 - val_accuracy: 0.8230\n",
      "Epoch 74/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3398 - accuracy: 0.8454 - val_loss: 0.4365 - val_accuracy: 0.8213\n",
      "Epoch 75/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3398 - accuracy: 0.8452 - val_loss: 0.4374 - val_accuracy: 0.8235\n",
      "Epoch 76/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3400 - accuracy: 0.8450 - val_loss: 0.4415 - val_accuracy: 0.8229\n",
      "Epoch 77/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3398 - accuracy: 0.8453 - val_loss: 0.4368 - val_accuracy: 0.8242\n",
      "Epoch 78/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3383 - accuracy: 0.8461 - val_loss: 0.4357 - val_accuracy: 0.8231\n",
      "Epoch 79/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3384 - accuracy: 0.8459 - val_loss: 0.4325 - val_accuracy: 0.8234\n",
      "Epoch 80/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3378 - accuracy: 0.8466 - val_loss: 0.4400 - val_accuracy: 0.8238\n",
      "Epoch 81/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3379 - accuracy: 0.8460 - val_loss: 0.4371 - val_accuracy: 0.8231\n",
      "Epoch 82/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3366 - accuracy: 0.8468 - val_loss: 0.4364 - val_accuracy: 0.8233\n",
      "Epoch 83/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3361 - accuracy: 0.8468 - val_loss: 0.4393 - val_accuracy: 0.8188\n",
      "Epoch 84/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3365 - accuracy: 0.8464 - val_loss: 0.4402 - val_accuracy: 0.8203\n",
      "Epoch 85/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3359 - accuracy: 0.8468 - val_loss: 0.4381 - val_accuracy: 0.8237\n",
      "Epoch 86/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3354 - accuracy: 0.8478 - val_loss: 0.4341 - val_accuracy: 0.8255\n",
      "Epoch 87/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3340 - accuracy: 0.8484 - val_loss: 0.4409 - val_accuracy: 0.8189\n",
      "Epoch 88/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3346 - accuracy: 0.8480 - val_loss: 0.4369 - val_accuracy: 0.8250\n",
      "Epoch 89/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3335 - accuracy: 0.8479 - val_loss: 0.4407 - val_accuracy: 0.8209\n",
      "Epoch 90/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3338 - accuracy: 0.8478 - val_loss: 0.4407 - val_accuracy: 0.8249\n",
      "Epoch 91/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3324 - accuracy: 0.8483 - val_loss: 0.4413 - val_accuracy: 0.8236\n",
      "Epoch 92/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3327 - accuracy: 0.8488 - val_loss: 0.4398 - val_accuracy: 0.8219\n",
      "Epoch 93/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3328 - accuracy: 0.8485 - val_loss: 0.4443 - val_accuracy: 0.8242\n",
      "Epoch 94/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3314 - accuracy: 0.8494 - val_loss: 0.4433 - val_accuracy: 0.8244\n",
      "Epoch 95/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3313 - accuracy: 0.8490 - val_loss: 0.4599 - val_accuracy: 0.8230\n",
      "Epoch 96/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3319 - accuracy: 0.8486 - val_loss: 0.4412 - val_accuracy: 0.8253\n",
      "Epoch 97/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3315 - accuracy: 0.8488 - val_loss: 0.4434 - val_accuracy: 0.8244\n",
      "Epoch 98/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3307 - accuracy: 0.8496 - val_loss: 0.4408 - val_accuracy: 0.8259\n",
      "Epoch 99/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3300 - accuracy: 0.8498 - val_loss: 0.4409 - val_accuracy: 0.8243\n",
      "Epoch 100/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3307 - accuracy: 0.8493 - val_loss: 0.4453 - val_accuracy: 0.8238\n",
      "Epoch 101/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3299 - accuracy: 0.8492 - val_loss: 0.4486 - val_accuracy: 0.8231\n",
      "Epoch 102/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3295 - accuracy: 0.8501 - val_loss: 0.4531 - val_accuracy: 0.8269\n",
      "Epoch 103/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3288 - accuracy: 0.8505 - val_loss: 0.4491 - val_accuracy: 0.8218\n",
      "Epoch 104/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3294 - accuracy: 0.8504 - val_loss: 0.4590 - val_accuracy: 0.8236\n",
      "Epoch 105/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3277 - accuracy: 0.8505 - val_loss: 0.4551 - val_accuracy: 0.8214\n",
      "Epoch 106/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3279 - accuracy: 0.8506 - val_loss: 0.4452 - val_accuracy: 0.8248\n",
      "Epoch 107/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3283 - accuracy: 0.8507 - val_loss: 0.4464 - val_accuracy: 0.8257\n",
      "Epoch 108/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3271 - accuracy: 0.8513 - val_loss: 0.4452 - val_accuracy: 0.8243\n",
      "Epoch 109/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3274 - accuracy: 0.8513 - val_loss: 0.4505 - val_accuracy: 0.8254\n",
      "Epoch 110/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3271 - accuracy: 0.8516 - val_loss: 0.4559 - val_accuracy: 0.8162\n",
      "Epoch 111/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3270 - accuracy: 0.8517 - val_loss: 0.4484 - val_accuracy: 0.8254\n",
      "Epoch 112/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3264 - accuracy: 0.8520 - val_loss: 0.4517 - val_accuracy: 0.8255\n",
      "Epoch 113/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3264 - accuracy: 0.8519 - val_loss: 0.4536 - val_accuracy: 0.8243\n",
      "Epoch 114/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3259 - accuracy: 0.8521 - val_loss: 0.4565 - val_accuracy: 0.8189\n",
      "Epoch 115/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3253 - accuracy: 0.8525 - val_loss: 0.4557 - val_accuracy: 0.8189\n",
      "Epoch 116/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3260 - accuracy: 0.8527 - val_loss: 0.4526 - val_accuracy: 0.8231\n",
      "Epoch 117/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3255 - accuracy: 0.8530 - val_loss: 0.4505 - val_accuracy: 0.8243\n",
      "Epoch 118/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3253 - accuracy: 0.8524 - val_loss: 0.4627 - val_accuracy: 0.8235\n",
      "Epoch 119/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3242 - accuracy: 0.8527 - val_loss: 0.4508 - val_accuracy: 0.8241\n",
      "Epoch 120/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3242 - accuracy: 0.8535 - val_loss: 0.4530 - val_accuracy: 0.8228\n",
      "Epoch 121/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3231 - accuracy: 0.8537 - val_loss: 0.4627 - val_accuracy: 0.8267\n",
      "Epoch 122/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3225 - accuracy: 0.8538 - val_loss: 0.4566 - val_accuracy: 0.8243\n",
      "Epoch 123/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3236 - accuracy: 0.8537 - val_loss: 0.4577 - val_accuracy: 0.8224\n",
      "Epoch 124/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3220 - accuracy: 0.8545 - val_loss: 0.4574 - val_accuracy: 0.8193\n",
      "Epoch 125/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3224 - accuracy: 0.8547 - val_loss: 0.4561 - val_accuracy: 0.8256\n",
      "Epoch 126/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3227 - accuracy: 0.8540 - val_loss: 0.4585 - val_accuracy: 0.8258\n",
      "Epoch 127/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3233 - accuracy: 0.8545 - val_loss: 0.4597 - val_accuracy: 0.8206\n",
      "Epoch 128/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3230 - accuracy: 0.8540 - val_loss: 0.4654 - val_accuracy: 0.8231\n",
      "Epoch 129/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3228 - accuracy: 0.8542 - val_loss: 0.4533 - val_accuracy: 0.8278\n",
      "Epoch 130/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3214 - accuracy: 0.8551 - val_loss: 0.4572 - val_accuracy: 0.8260\n",
      "Epoch 131/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3210 - accuracy: 0.8554 - val_loss: 0.4737 - val_accuracy: 0.8226\n",
      "Epoch 132/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3212 - accuracy: 0.8552 - val_loss: 0.4591 - val_accuracy: 0.8256\n",
      "Epoch 133/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3220 - accuracy: 0.8550 - val_loss: 0.4582 - val_accuracy: 0.8244\n",
      "Epoch 134/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3211 - accuracy: 0.8559 - val_loss: 0.4563 - val_accuracy: 0.8241\n",
      "Epoch 135/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3210 - accuracy: 0.8554 - val_loss: 0.4642 - val_accuracy: 0.8258\n",
      "Epoch 136/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3202 - accuracy: 0.8559 - val_loss: 0.4606 - val_accuracy: 0.8240\n",
      "Epoch 137/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3210 - accuracy: 0.8552 - val_loss: 0.4632 - val_accuracy: 0.8212\n",
      "Epoch 138/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3202 - accuracy: 0.8560 - val_loss: 0.4600 - val_accuracy: 0.8241\n",
      "Epoch 139/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3200 - accuracy: 0.8556 - val_loss: 0.4628 - val_accuracy: 0.8228\n",
      "Epoch 140/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3197 - accuracy: 0.8564 - val_loss: 0.4621 - val_accuracy: 0.8249\n",
      "Epoch 141/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3189 - accuracy: 0.8566 - val_loss: 0.4654 - val_accuracy: 0.8216\n",
      "Epoch 142/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3190 - accuracy: 0.8565 - val_loss: 0.4609 - val_accuracy: 0.8278\n",
      "Epoch 143/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3182 - accuracy: 0.8567 - val_loss: 0.4630 - val_accuracy: 0.8234\n",
      "Epoch 144/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3185 - accuracy: 0.8569 - val_loss: 0.4734 - val_accuracy: 0.8168\n",
      "Epoch 145/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3183 - accuracy: 0.8567 - val_loss: 0.4578 - val_accuracy: 0.8278\n",
      "Epoch 146/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3181 - accuracy: 0.8571 - val_loss: 0.4610 - val_accuracy: 0.8265\n",
      "Epoch 147/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3179 - accuracy: 0.8573 - val_loss: 0.4628 - val_accuracy: 0.8226\n",
      "Epoch 148/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3177 - accuracy: 0.8572 - val_loss: 0.4770 - val_accuracy: 0.8247\n",
      "Epoch 149/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3161 - accuracy: 0.8587 - val_loss: 0.4785 - val_accuracy: 0.8247\n",
      "Epoch 150/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3166 - accuracy: 0.8584 - val_loss: 0.4650 - val_accuracy: 0.8265\n",
      "Epoch 151/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3175 - accuracy: 0.8577 - val_loss: 0.4641 - val_accuracy: 0.8256\n",
      "Epoch 152/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3168 - accuracy: 0.8577 - val_loss: 0.4666 - val_accuracy: 0.8216\n",
      "Epoch 153/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3166 - accuracy: 0.8576 - val_loss: 0.4637 - val_accuracy: 0.8252\n",
      "Epoch 154/1000\n",
      "216750/216750 [==============================] - ETA: 0s - loss: 0.3161 - accuracy: 0.85 - 1s 5us/step - loss: 0.3164 - accuracy: 0.8581 - val_loss: 0.4645 - val_accuracy: 0.8266\n",
      "Epoch 155/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3161 - accuracy: 0.8586 - val_loss: 0.4684 - val_accuracy: 0.8184\n",
      "Epoch 156/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3158 - accuracy: 0.8584 - val_loss: 0.4673 - val_accuracy: 0.8210\n",
      "Epoch 157/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3170 - accuracy: 0.8582 - val_loss: 0.4667 - val_accuracy: 0.8189\n",
      "Epoch 158/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3155 - accuracy: 0.8591 - val_loss: 0.4733 - val_accuracy: 0.8254\n",
      "Epoch 159/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3162 - accuracy: 0.8581 - val_loss: 0.4665 - val_accuracy: 0.8290\n",
      "Epoch 160/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3165 - accuracy: 0.8584 - val_loss: 0.4618 - val_accuracy: 0.8292\n",
      "Epoch 161/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3152 - accuracy: 0.8588 - val_loss: 0.4677 - val_accuracy: 0.8288\n",
      "Epoch 162/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3147 - accuracy: 0.8592 - val_loss: 0.4627 - val_accuracy: 0.8265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3144 - accuracy: 0.8591 - val_loss: 0.4677 - val_accuracy: 0.8288\n",
      "Epoch 164/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3137 - accuracy: 0.8600 - val_loss: 0.4644 - val_accuracy: 0.8249\n",
      "Epoch 165/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3147 - accuracy: 0.8593 - val_loss: 0.4760 - val_accuracy: 0.8153\n",
      "Epoch 166/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3133 - accuracy: 0.8598 - val_loss: 0.4766 - val_accuracy: 0.8194\n",
      "Epoch 167/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3151 - accuracy: 0.8589 - val_loss: 0.4709 - val_accuracy: 0.8274\n",
      "Epoch 168/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3146 - accuracy: 0.8593 - val_loss: 0.4860 - val_accuracy: 0.8226\n",
      "Epoch 169/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3137 - accuracy: 0.8599 - val_loss: 0.4747 - val_accuracy: 0.8281\n",
      "Epoch 170/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3126 - accuracy: 0.8600 - val_loss: 0.4656 - val_accuracy: 0.8237\n",
      "Epoch 171/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3127 - accuracy: 0.8604 - val_loss: 0.4693 - val_accuracy: 0.8257\n",
      "Epoch 172/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3131 - accuracy: 0.8607 - val_loss: 0.4765 - val_accuracy: 0.8269\n",
      "Epoch 173/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3132 - accuracy: 0.8602 - val_loss: 0.4809 - val_accuracy: 0.8235\n",
      "Epoch 174/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3124 - accuracy: 0.8608 - val_loss: 0.4729 - val_accuracy: 0.8271\n",
      "Epoch 175/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3119 - accuracy: 0.8610 - val_loss: 0.4721 - val_accuracy: 0.8249\n",
      "Epoch 176/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3124 - accuracy: 0.8600 - val_loss: 0.4750 - val_accuracy: 0.8270\n",
      "Epoch 177/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3115 - accuracy: 0.8616 - val_loss: 0.4784 - val_accuracy: 0.8278\n",
      "Epoch 178/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3117 - accuracy: 0.8617 - val_loss: 0.4705 - val_accuracy: 0.8281\n",
      "Epoch 179/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3113 - accuracy: 0.8612 - val_loss: 0.4717 - val_accuracy: 0.8276\n",
      "Epoch 180/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3112 - accuracy: 0.8612 - val_loss: 0.4763 - val_accuracy: 0.8265\n",
      "Epoch 181/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3108 - accuracy: 0.8619 - val_loss: 0.4785 - val_accuracy: 0.8261\n",
      "Epoch 182/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3120 - accuracy: 0.8609 - val_loss: 0.4765 - val_accuracy: 0.8278\n",
      "Epoch 183/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3106 - accuracy: 0.8613 - val_loss: 0.4716 - val_accuracy: 0.8286\n",
      "Epoch 184/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3117 - accuracy: 0.8612 - val_loss: 0.4740 - val_accuracy: 0.8255\n",
      "Epoch 185/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3101 - accuracy: 0.8620 - val_loss: 0.4718 - val_accuracy: 0.8291\n",
      "Epoch 186/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3109 - accuracy: 0.8614 - val_loss: 0.4740 - val_accuracy: 0.8272\n",
      "Epoch 187/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3106 - accuracy: 0.8615 - val_loss: 0.4729 - val_accuracy: 0.8296\n",
      "Epoch 188/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3105 - accuracy: 0.8620 - val_loss: 0.4738 - val_accuracy: 0.8274\n",
      "Epoch 189/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3098 - accuracy: 0.8624 - val_loss: 0.4723 - val_accuracy: 0.8243\n",
      "Epoch 190/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3097 - accuracy: 0.8622 - val_loss: 0.4726 - val_accuracy: 0.8292\n",
      "Epoch 191/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3115 - accuracy: 0.8616 - val_loss: 0.4765 - val_accuracy: 0.8236\n",
      "Epoch 192/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3098 - accuracy: 0.8618 - val_loss: 0.4764 - val_accuracy: 0.8282\n",
      "Epoch 193/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3090 - accuracy: 0.8631 - val_loss: 0.4733 - val_accuracy: 0.8306\n",
      "Epoch 194/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3081 - accuracy: 0.8632 - val_loss: 0.4749 - val_accuracy: 0.8262\n",
      "Epoch 195/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3086 - accuracy: 0.8630 - val_loss: 0.4846 - val_accuracy: 0.8272\n",
      "Epoch 196/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.3094 - accuracy: 0.8624 - val_loss: 0.4770 - val_accuracy: 0.8216\n",
      "Epoch 197/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3083 - accuracy: 0.8621 - val_loss: 0.4770 - val_accuracy: 0.8280\n",
      "Epoch 198/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3090 - accuracy: 0.8632 - val_loss: 0.4861 - val_accuracy: 0.8186\n",
      "Epoch 199/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3094 - accuracy: 0.8632 - val_loss: 0.4743 - val_accuracy: 0.8288\n",
      "Epoch 200/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3088 - accuracy: 0.8632 - val_loss: 0.4743 - val_accuracy: 0.8280\n",
      "Epoch 201/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3097 - accuracy: 0.8626 - val_loss: 0.4760 - val_accuracy: 0.8295\n",
      "Epoch 202/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3082 - accuracy: 0.8633 - val_loss: 0.4766 - val_accuracy: 0.8290\n",
      "Epoch 203/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3070 - accuracy: 0.8633 - val_loss: 0.4767 - val_accuracy: 0.8247\n",
      "Epoch 204/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3074 - accuracy: 0.8635 - val_loss: 0.4993 - val_accuracy: 0.8239\n",
      "Epoch 205/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3078 - accuracy: 0.8639 - val_loss: 0.4767 - val_accuracy: 0.8261\n",
      "Epoch 206/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3073 - accuracy: 0.8636 - val_loss: 0.4798 - val_accuracy: 0.8294\n",
      "Epoch 207/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3069 - accuracy: 0.8639 - val_loss: 0.4802 - val_accuracy: 0.8298\n",
      "Epoch 208/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3063 - accuracy: 0.8644 - val_loss: 0.4805 - val_accuracy: 0.8284\n",
      "Epoch 209/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3070 - accuracy: 0.8642 - val_loss: 0.4768 - val_accuracy: 0.8298\n",
      "Epoch 210/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3065 - accuracy: 0.8642 - val_loss: 0.4798 - val_accuracy: 0.8313\n",
      "Epoch 211/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3050 - accuracy: 0.8649 - val_loss: 0.4747 - val_accuracy: 0.8325\n",
      "Epoch 212/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3066 - accuracy: 0.8636 - val_loss: 0.4822 - val_accuracy: 0.8293\n",
      "Epoch 213/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3058 - accuracy: 0.8646 - val_loss: 0.4906 - val_accuracy: 0.8276\n",
      "Epoch 214/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3058 - accuracy: 0.8644 - val_loss: 0.4815 - val_accuracy: 0.8286\n",
      "Epoch 215/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3056 - accuracy: 0.8647 - val_loss: 0.4977 - val_accuracy: 0.8038\n",
      "Epoch 216/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3054 - accuracy: 0.8649 - val_loss: 0.4842 - val_accuracy: 0.8301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3057 - accuracy: 0.8647 - val_loss: 0.4834 - val_accuracy: 0.8260\n",
      "Epoch 218/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3052 - accuracy: 0.8647 - val_loss: 0.4804 - val_accuracy: 0.8313\n",
      "Epoch 219/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3049 - accuracy: 0.8645 - val_loss: 0.4841 - val_accuracy: 0.8295\n",
      "Epoch 220/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3054 - accuracy: 0.8649 - val_loss: 0.4863 - val_accuracy: 0.8304\n",
      "Epoch 221/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3048 - accuracy: 0.8648 - val_loss: 0.4822 - val_accuracy: 0.8306\n",
      "Epoch 222/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3040 - accuracy: 0.8653 - val_loss: 0.4978 - val_accuracy: 0.8260\n",
      "Epoch 223/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3050 - accuracy: 0.8650 - val_loss: 0.4786 - val_accuracy: 0.8289\n",
      "Epoch 224/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3043 - accuracy: 0.8651 - val_loss: 0.4763 - val_accuracy: 0.8300\n",
      "Epoch 225/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3045 - accuracy: 0.8649 - val_loss: 0.4863 - val_accuracy: 0.8304\n",
      "Epoch 226/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3047 - accuracy: 0.8653 - val_loss: 0.4863 - val_accuracy: 0.8298\n",
      "Epoch 227/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3043 - accuracy: 0.8657 - val_loss: 0.4907 - val_accuracy: 0.8205\n",
      "Epoch 228/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3040 - accuracy: 0.8657 - val_loss: 0.4968 - val_accuracy: 0.8276\n",
      "Epoch 229/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3042 - accuracy: 0.8652 - val_loss: 0.4903 - val_accuracy: 0.8300\n",
      "Epoch 230/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3033 - accuracy: 0.8657 - val_loss: 0.4970 - val_accuracy: 0.8142\n",
      "Epoch 231/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3035 - accuracy: 0.8660 - val_loss: 0.4920 - val_accuracy: 0.8142\n",
      "Epoch 232/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3040 - accuracy: 0.8658 - val_loss: 0.4895 - val_accuracy: 0.8293\n",
      "Epoch 233/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3049 - accuracy: 0.8652 - val_loss: 0.4823 - val_accuracy: 0.8310\n",
      "Epoch 234/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3035 - accuracy: 0.8661 - val_loss: 0.4820 - val_accuracy: 0.8299\n",
      "Epoch 235/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3046 - accuracy: 0.8653 - val_loss: 0.4876 - val_accuracy: 0.8311\n",
      "Epoch 236/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3029 - accuracy: 0.8661 - val_loss: 0.4819 - val_accuracy: 0.8250\n",
      "Epoch 237/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3042 - accuracy: 0.8654 - val_loss: 0.4885 - val_accuracy: 0.8302\n",
      "Epoch 238/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3034 - accuracy: 0.8660 - val_loss: 0.4849 - val_accuracy: 0.8315\n",
      "Epoch 239/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3026 - accuracy: 0.8665 - val_loss: 0.4819 - val_accuracy: 0.8326\n",
      "Epoch 240/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3022 - accuracy: 0.8669 - val_loss: 0.4861 - val_accuracy: 0.8313\n",
      "Epoch 241/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3016 - accuracy: 0.8665 - val_loss: 0.4884 - val_accuracy: 0.8298\n",
      "Epoch 242/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3020 - accuracy: 0.8665 - val_loss: 0.4814 - val_accuracy: 0.8285\n",
      "Epoch 243/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3027 - accuracy: 0.8666 - val_loss: 0.4816 - val_accuracy: 0.8282\n",
      "Epoch 244/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3020 - accuracy: 0.8662 - val_loss: 0.4883 - val_accuracy: 0.8273\n",
      "Epoch 245/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3023 - accuracy: 0.8663 - val_loss: 0.4879 - val_accuracy: 0.8279\n",
      "Epoch 246/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3018 - accuracy: 0.8665 - val_loss: 0.4946 - val_accuracy: 0.8220\n",
      "Epoch 247/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3023 - accuracy: 0.8664 - val_loss: 0.4888 - val_accuracy: 0.8285\n",
      "Epoch 248/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3024 - accuracy: 0.8670 - val_loss: 0.4859 - val_accuracy: 0.8287\n",
      "Epoch 249/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3014 - accuracy: 0.8665 - val_loss: 0.4885 - val_accuracy: 0.8290\n",
      "Epoch 250/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3011 - accuracy: 0.8666 - val_loss: 0.4914 - val_accuracy: 0.8270\n",
      "Epoch 251/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3009 - accuracy: 0.8668 - val_loss: 0.4894 - val_accuracy: 0.8290\n",
      "Epoch 252/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2997 - accuracy: 0.8676 - val_loss: 0.4935 - val_accuracy: 0.8308\n",
      "Epoch 253/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3011 - accuracy: 0.8670 - val_loss: 0.4881 - val_accuracy: 0.8305\n",
      "Epoch 254/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3011 - accuracy: 0.8668 - val_loss: 0.4917 - val_accuracy: 0.8284\n",
      "Epoch 255/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3012 - accuracy: 0.8671 - val_loss: 0.4877 - val_accuracy: 0.8298\n",
      "Epoch 256/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3011 - accuracy: 0.8670 - val_loss: 0.4943 - val_accuracy: 0.8313\n",
      "Epoch 257/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3005 - accuracy: 0.8673 - val_loss: 0.5004 - val_accuracy: 0.8292\n",
      "Epoch 258/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3007 - accuracy: 0.8674 - val_loss: 0.5086 - val_accuracy: 0.8012\n",
      "Epoch 259/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3002 - accuracy: 0.8675 - val_loss: 0.4919 - val_accuracy: 0.8237\n",
      "Epoch 260/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3008 - accuracy: 0.8668 - val_loss: 0.4935 - val_accuracy: 0.8276\n",
      "Epoch 261/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3008 - accuracy: 0.8674 - val_loss: 0.4913 - val_accuracy: 0.8292\n",
      "Epoch 262/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3001 - accuracy: 0.8676 - val_loss: 0.4884 - val_accuracy: 0.8285\n",
      "Epoch 263/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3001 - accuracy: 0.8677 - val_loss: 0.4820 - val_accuracy: 0.8330\n",
      "Epoch 264/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2997 - accuracy: 0.8683 - val_loss: 0.4895 - val_accuracy: 0.8305\n",
      "Epoch 265/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2990 - accuracy: 0.8679 - val_loss: 0.4964 - val_accuracy: 0.8294\n",
      "Epoch 266/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2991 - accuracy: 0.8681 - val_loss: 0.4939 - val_accuracy: 0.8209\n",
      "Epoch 267/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2990 - accuracy: 0.8679 - val_loss: 0.4877 - val_accuracy: 0.8300\n",
      "Epoch 268/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2988 - accuracy: 0.8686 - val_loss: 0.4930 - val_accuracy: 0.8321\n",
      "Epoch 269/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2986 - accuracy: 0.8682 - val_loss: 0.4892 - val_accuracy: 0.8323\n",
      "Epoch 270/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2988 - accuracy: 0.8682 - val_loss: 0.4888 - val_accuracy: 0.8309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2989 - accuracy: 0.8684 - val_loss: 0.5011 - val_accuracy: 0.8311\n",
      "Epoch 272/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2993 - accuracy: 0.8681 - val_loss: 0.4891 - val_accuracy: 0.8270\n",
      "Epoch 273/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2984 - accuracy: 0.8684 - val_loss: 0.5050 - val_accuracy: 0.8302\n",
      "Epoch 274/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2982 - accuracy: 0.8685 - val_loss: 0.4879 - val_accuracy: 0.8305\n",
      "Epoch 275/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2990 - accuracy: 0.8686 - val_loss: 0.5058 - val_accuracy: 0.8278\n",
      "Epoch 276/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2978 - accuracy: 0.8684 - val_loss: 0.4985 - val_accuracy: 0.8300\n",
      "Epoch 277/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2991 - accuracy: 0.8682 - val_loss: 0.4887 - val_accuracy: 0.8319\n",
      "Epoch 278/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2982 - accuracy: 0.8684 - val_loss: 0.4876 - val_accuracy: 0.8318\n",
      "Epoch 279/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2986 - accuracy: 0.8683 - val_loss: 0.4904 - val_accuracy: 0.8297\n",
      "Epoch 280/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2981 - accuracy: 0.8688 - val_loss: 0.5000 - val_accuracy: 0.8315\n",
      "Epoch 281/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2976 - accuracy: 0.8689 - val_loss: 0.4938 - val_accuracy: 0.8266\n",
      "Epoch 282/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2977 - accuracy: 0.8690 - val_loss: 0.5025 - val_accuracy: 0.8168\n",
      "Epoch 283/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2977 - accuracy: 0.8687 - val_loss: 0.4846 - val_accuracy: 0.8324\n",
      "Epoch 284/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2969 - accuracy: 0.8689 - val_loss: 0.4953 - val_accuracy: 0.8285\n",
      "Epoch 285/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2981 - accuracy: 0.8683 - val_loss: 0.4911 - val_accuracy: 0.8330\n",
      "Epoch 286/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2966 - accuracy: 0.8690 - val_loss: 0.4888 - val_accuracy: 0.8321\n",
      "Epoch 287/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2970 - accuracy: 0.8691 - val_loss: 0.4999 - val_accuracy: 0.8293\n",
      "Epoch 288/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2974 - accuracy: 0.8688 - val_loss: 0.4978 - val_accuracy: 0.8282\n",
      "Epoch 289/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2984 - accuracy: 0.8687 - val_loss: 0.4957 - val_accuracy: 0.8292\n",
      "Epoch 290/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2967 - accuracy: 0.8689 - val_loss: 0.4984 - val_accuracy: 0.8256\n",
      "Epoch 291/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2970 - accuracy: 0.8690 - val_loss: 0.5004 - val_accuracy: 0.8310\n",
      "Epoch 292/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2967 - accuracy: 0.8688 - val_loss: 0.4945 - val_accuracy: 0.8311\n",
      "Epoch 293/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2963 - accuracy: 0.8693 - val_loss: 0.4993 - val_accuracy: 0.8308\n",
      "Epoch 294/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2971 - accuracy: 0.8689 - val_loss: 0.4900 - val_accuracy: 0.8329\n",
      "Epoch 295/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2963 - accuracy: 0.8695 - val_loss: 0.4905 - val_accuracy: 0.8326\n",
      "Epoch 296/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2968 - accuracy: 0.8694 - val_loss: 0.4928 - val_accuracy: 0.8319\n",
      "Epoch 297/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2955 - accuracy: 0.8698 - val_loss: 0.4975 - val_accuracy: 0.8330\n",
      "Epoch 298/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2962 - accuracy: 0.8695 - val_loss: 0.5121 - val_accuracy: 0.8273\n",
      "Epoch 299/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2972 - accuracy: 0.8692 - val_loss: 0.5020 - val_accuracy: 0.8292\n",
      "Epoch 300/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2974 - accuracy: 0.8691 - val_loss: 0.4931 - val_accuracy: 0.8324\n",
      "Epoch 301/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2957 - accuracy: 0.8699 - val_loss: 0.4986 - val_accuracy: 0.8322\n",
      "Epoch 302/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2954 - accuracy: 0.8701 - val_loss: 0.4967 - val_accuracy: 0.8319\n",
      "Epoch 303/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2957 - accuracy: 0.8698 - val_loss: 0.4898 - val_accuracy: 0.8320\n",
      "Epoch 304/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2959 - accuracy: 0.8698 - val_loss: 0.4910 - val_accuracy: 0.8312\n",
      "Epoch 305/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2965 - accuracy: 0.8695 - val_loss: 0.5032 - val_accuracy: 0.8280\n",
      "Epoch 306/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2961 - accuracy: 0.8693 - val_loss: 0.4956 - val_accuracy: 0.8327\n",
      "Epoch 307/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2955 - accuracy: 0.8701 - val_loss: 0.5109 - val_accuracy: 0.8302\n",
      "Epoch 308/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2963 - accuracy: 0.8693 - val_loss: 0.5018 - val_accuracy: 0.8312\n",
      "Epoch 309/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2958 - accuracy: 0.8699 - val_loss: 0.4952 - val_accuracy: 0.8312\n",
      "Epoch 310/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2957 - accuracy: 0.8698 - val_loss: 0.4975 - val_accuracy: 0.8330\n",
      "Epoch 311/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2957 - accuracy: 0.8697 - val_loss: 0.5062 - val_accuracy: 0.8213\n",
      "Epoch 312/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2957 - accuracy: 0.8694 - val_loss: 0.5014 - val_accuracy: 0.8313\n",
      "Epoch 313/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2957 - accuracy: 0.8695 - val_loss: 0.5050 - val_accuracy: 0.8244\n",
      "Epoch 314/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2946 - accuracy: 0.8707 - val_loss: 0.5043 - val_accuracy: 0.8326\n",
      "Epoch 315/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2952 - accuracy: 0.8705 - val_loss: 0.5054 - val_accuracy: 0.8275\n",
      "Epoch 316/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2948 - accuracy: 0.8702 - val_loss: 0.5009 - val_accuracy: 0.8310\n",
      "Epoch 317/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2936 - accuracy: 0.8708 - val_loss: 0.5027 - val_accuracy: 0.8259\n",
      "Epoch 318/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2937 - accuracy: 0.8707 - val_loss: 0.4974 - val_accuracy: 0.8334\n",
      "Epoch 319/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2936 - accuracy: 0.8708 - val_loss: 0.5037 - val_accuracy: 0.8337\n",
      "Epoch 320/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2941 - accuracy: 0.8705 - val_loss: 0.5052 - val_accuracy: 0.8312\n",
      "Epoch 321/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2947 - accuracy: 0.8705 - val_loss: 0.5118 - val_accuracy: 0.8294\n",
      "Epoch 322/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2947 - accuracy: 0.8702 - val_loss: 0.5029 - val_accuracy: 0.8328\n",
      "Epoch 323/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2945 - accuracy: 0.8701 - val_loss: 0.4957 - val_accuracy: 0.8319\n",
      "Epoch 324/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2945 - accuracy: 0.8704 - val_loss: 0.5007 - val_accuracy: 0.8312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2940 - accuracy: 0.8703 - val_loss: 0.4970 - val_accuracy: 0.8326\n",
      "Epoch 326/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2941 - accuracy: 0.8704 - val_loss: 0.5036 - val_accuracy: 0.8256\n",
      "Epoch 327/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2943 - accuracy: 0.8704 - val_loss: 0.4973 - val_accuracy: 0.8294\n",
      "Epoch 328/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2940 - accuracy: 0.8703 - val_loss: 0.4943 - val_accuracy: 0.8336\n",
      "Epoch 329/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2936 - accuracy: 0.8706 - val_loss: 0.4993 - val_accuracy: 0.8338\n",
      "Epoch 330/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2928 - accuracy: 0.8713 - val_loss: 0.5001 - val_accuracy: 0.8323\n",
      "Epoch 331/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2928 - accuracy: 0.8711 - val_loss: 0.4977 - val_accuracy: 0.8337\n",
      "Epoch 332/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2937 - accuracy: 0.8706 - val_loss: 0.4960 - val_accuracy: 0.8336\n",
      "Epoch 333/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2931 - accuracy: 0.8710 - val_loss: 0.5073 - val_accuracy: 0.8307\n",
      "Epoch 334/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2945 - accuracy: 0.8710 - val_loss: 0.4978 - val_accuracy: 0.8311\n",
      "Epoch 335/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2922 - accuracy: 0.8717 - val_loss: 0.5014 - val_accuracy: 0.8275\n",
      "Epoch 336/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2924 - accuracy: 0.8714 - val_loss: 0.5071 - val_accuracy: 0.8263\n",
      "Epoch 337/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2940 - accuracy: 0.8708 - val_loss: 0.4984 - val_accuracy: 0.8298\n",
      "Epoch 338/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2925 - accuracy: 0.8716 - val_loss: 0.4995 - val_accuracy: 0.8318\n",
      "Epoch 339/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2926 - accuracy: 0.8708 - val_loss: 0.5022 - val_accuracy: 0.8308\n",
      "Epoch 340/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2934 - accuracy: 0.8708 - val_loss: 0.4962 - val_accuracy: 0.8345\n",
      "Epoch 341/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2928 - accuracy: 0.8710 - val_loss: 0.5061 - val_accuracy: 0.8227\n",
      "Epoch 342/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2923 - accuracy: 0.8717 - val_loss: 0.4970 - val_accuracy: 0.8332\n",
      "Epoch 343/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2921 - accuracy: 0.8714 - val_loss: 0.4956 - val_accuracy: 0.8334\n",
      "Epoch 344/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2915 - accuracy: 0.8718 - val_loss: 0.5058 - val_accuracy: 0.8316\n",
      "Epoch 345/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2918 - accuracy: 0.8716 - val_loss: 0.5096 - val_accuracy: 0.8247\n",
      "Epoch 346/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2922 - accuracy: 0.8717 - val_loss: 0.5021 - val_accuracy: 0.8338\n",
      "Epoch 347/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2923 - accuracy: 0.8713 - val_loss: 0.5019 - val_accuracy: 0.8329\n",
      "Epoch 348/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2925 - accuracy: 0.8718 - val_loss: 0.5021 - val_accuracy: 0.8336\n",
      "Epoch 349/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2928 - accuracy: 0.8715 - val_loss: 0.5090 - val_accuracy: 0.8316\n",
      "Epoch 350/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2916 - accuracy: 0.8715 - val_loss: 0.5002 - val_accuracy: 0.8327\n",
      "Epoch 351/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2919 - accuracy: 0.8718 - val_loss: 0.5023 - val_accuracy: 0.8342\n",
      "Epoch 352/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2920 - accuracy: 0.8720 - val_loss: 0.5085 - val_accuracy: 0.8331\n",
      "Epoch 353/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2911 - accuracy: 0.8720 - val_loss: 0.5119 - val_accuracy: 0.8322\n",
      "Epoch 354/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2910 - accuracy: 0.8724 - val_loss: 0.4996 - val_accuracy: 0.8340\n",
      "Epoch 355/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2907 - accuracy: 0.8723 - val_loss: 0.5005 - val_accuracy: 0.8349\n",
      "Epoch 356/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2912 - accuracy: 0.8725 - val_loss: 0.5003 - val_accuracy: 0.8327\n",
      "Epoch 357/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2913 - accuracy: 0.8720 - val_loss: 0.5021 - val_accuracy: 0.8341\n",
      "Epoch 358/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2906 - accuracy: 0.8725 - val_loss: 0.5032 - val_accuracy: 0.8302\n",
      "Epoch 359/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2904 - accuracy: 0.8730 - val_loss: 0.5068 - val_accuracy: 0.8328\n",
      "Epoch 360/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2920 - accuracy: 0.8723 - val_loss: 0.5018 - val_accuracy: 0.8345\n",
      "Epoch 361/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2905 - accuracy: 0.8725 - val_loss: 0.5035 - val_accuracy: 0.8328\n",
      "Epoch 362/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2908 - accuracy: 0.8722 - val_loss: 0.5145 - val_accuracy: 0.8324\n",
      "Epoch 363/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2905 - accuracy: 0.8721 - val_loss: 0.4962 - val_accuracy: 0.8346\n",
      "Epoch 364/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2905 - accuracy: 0.8723 - val_loss: 0.5022 - val_accuracy: 0.8347\n",
      "Epoch 365/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2898 - accuracy: 0.8723 - val_loss: 0.5012 - val_accuracy: 0.8336\n",
      "Epoch 366/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2898 - accuracy: 0.8728 - val_loss: 0.5154 - val_accuracy: 0.8285\n",
      "Epoch 367/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2892 - accuracy: 0.8734 - val_loss: 0.5080 - val_accuracy: 0.8328\n",
      "Epoch 368/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2888 - accuracy: 0.8732 - val_loss: 0.5147 - val_accuracy: 0.8229\n",
      "Epoch 369/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2900 - accuracy: 0.8728 - val_loss: 0.4974 - val_accuracy: 0.8348\n",
      "Epoch 370/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2906 - accuracy: 0.8720 - val_loss: 0.5021 - val_accuracy: 0.8335\n",
      "Epoch 371/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2891 - accuracy: 0.8734 - val_loss: 0.5096 - val_accuracy: 0.8344\n",
      "Epoch 372/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2902 - accuracy: 0.8726 - val_loss: 0.5055 - val_accuracy: 0.8316\n",
      "Epoch 373/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2904 - accuracy: 0.8727 - val_loss: 0.5108 - val_accuracy: 0.8245\n",
      "Epoch 374/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2895 - accuracy: 0.8726 - val_loss: 0.5060 - val_accuracy: 0.8348\n",
      "Epoch 375/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2893 - accuracy: 0.8730 - val_loss: 0.5066 - val_accuracy: 0.8344\n",
      "Epoch 376/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2904 - accuracy: 0.8729 - val_loss: 0.5026 - val_accuracy: 0.8297\n",
      "Epoch 377/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2901 - accuracy: 0.8733 - val_loss: 0.5070 - val_accuracy: 0.8319\n",
      "Epoch 378/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2896 - accuracy: 0.8734 - val_loss: 0.5029 - val_accuracy: 0.8332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2895 - accuracy: 0.8728 - val_loss: 0.5154 - val_accuracy: 0.8193\n",
      "Epoch 380/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2889 - accuracy: 0.8730 - val_loss: 0.5124 - val_accuracy: 0.8339\n",
      "Epoch 381/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2887 - accuracy: 0.8730 - val_loss: 0.5071 - val_accuracy: 0.8314\n",
      "Epoch 382/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2890 - accuracy: 0.8730 - val_loss: 0.5043 - val_accuracy: 0.8268\n",
      "Epoch 383/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2900 - accuracy: 0.8728 - val_loss: 0.5086 - val_accuracy: 0.8308\n",
      "Epoch 384/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2894 - accuracy: 0.8733 - val_loss: 0.5118 - val_accuracy: 0.8360\n",
      "Epoch 385/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2889 - accuracy: 0.8730 - val_loss: 0.5044 - val_accuracy: 0.8317\n",
      "Epoch 386/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2893 - accuracy: 0.8731 - val_loss: 0.5233 - val_accuracy: 0.8288\n",
      "Epoch 387/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2889 - accuracy: 0.8733 - val_loss: 0.5072 - val_accuracy: 0.8340\n",
      "Epoch 388/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2888 - accuracy: 0.8734 - val_loss: 0.5057 - val_accuracy: 0.8325\n",
      "Epoch 389/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2888 - accuracy: 0.8733 - val_loss: 0.5069 - val_accuracy: 0.8330\n",
      "Epoch 390/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2882 - accuracy: 0.8736 - val_loss: 0.5089 - val_accuracy: 0.8284\n",
      "Epoch 391/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2887 - accuracy: 0.8732 - val_loss: 0.5090 - val_accuracy: 0.8345\n",
      "Epoch 392/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2883 - accuracy: 0.8735 - val_loss: 0.5184 - val_accuracy: 0.8180\n",
      "Epoch 393/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2882 - accuracy: 0.8731 - val_loss: 0.5116 - val_accuracy: 0.8275\n",
      "Epoch 394/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2875 - accuracy: 0.8738 - val_loss: 0.5119 - val_accuracy: 0.8311\n",
      "Epoch 395/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2885 - accuracy: 0.8738 - val_loss: 0.5193 - val_accuracy: 0.8284\n",
      "Epoch 396/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2886 - accuracy: 0.8735 - val_loss: 0.5092 - val_accuracy: 0.8320\n",
      "Epoch 397/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2883 - accuracy: 0.8738 - val_loss: 0.5076 - val_accuracy: 0.8329\n",
      "Epoch 398/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2881 - accuracy: 0.8742 - val_loss: 0.5053 - val_accuracy: 0.8333\n",
      "Epoch 399/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2879 - accuracy: 0.8737 - val_loss: 0.5053 - val_accuracy: 0.8353\n",
      "Epoch 400/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2868 - accuracy: 0.8742 - val_loss: 0.5047 - val_accuracy: 0.8325\n",
      "Epoch 401/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2891 - accuracy: 0.8734 - val_loss: 0.5072 - val_accuracy: 0.8313\n",
      "Epoch 402/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2877 - accuracy: 0.8736 - val_loss: 0.5101 - val_accuracy: 0.8296\n",
      "Epoch 403/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2876 - accuracy: 0.8739 - val_loss: 0.5107 - val_accuracy: 0.8339\n",
      "Epoch 404/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.8740 - val_loss: 0.5083 - val_accuracy: 0.8343\n",
      "Epoch 405/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2877 - accuracy: 0.8737 - val_loss: 0.5205 - val_accuracy: 0.8319\n",
      "Epoch 406/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.8737 - val_loss: 0.5054 - val_accuracy: 0.8334\n",
      "Epoch 407/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2875 - accuracy: 0.8741 - val_loss: 0.5124 - val_accuracy: 0.8343\n",
      "Epoch 408/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2883 - accuracy: 0.8733 - val_loss: 0.5088 - val_accuracy: 0.8337\n",
      "Epoch 409/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2881 - accuracy: 0.8735 - val_loss: 0.5093 - val_accuracy: 0.8331\n",
      "Epoch 410/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2881 - accuracy: 0.8741 - val_loss: 0.5114 - val_accuracy: 0.8315\n",
      "Epoch 411/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2871 - accuracy: 0.8739 - val_loss: 0.5106 - val_accuracy: 0.8337\n",
      "Epoch 412/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2874 - accuracy: 0.8744 - val_loss: 0.5160 - val_accuracy: 0.8331\n",
      "Epoch 413/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2879 - accuracy: 0.8737 - val_loss: 0.5120 - val_accuracy: 0.8344\n",
      "Epoch 414/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2867 - accuracy: 0.8743 - val_loss: 0.5102 - val_accuracy: 0.8320\n",
      "Epoch 415/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2870 - accuracy: 0.8744 - val_loss: 0.5048 - val_accuracy: 0.8350\n",
      "Epoch 416/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2869 - accuracy: 0.8743 - val_loss: 0.5103 - val_accuracy: 0.8325\n",
      "Epoch 417/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2860 - accuracy: 0.8748 - val_loss: 0.5082 - val_accuracy: 0.8357\n",
      "Epoch 418/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2866 - accuracy: 0.8742 - val_loss: 0.5040 - val_accuracy: 0.8339\n",
      "Epoch 419/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2869 - accuracy: 0.8747 - val_loss: 0.5092 - val_accuracy: 0.8332\n",
      "Epoch 420/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2868 - accuracy: 0.8741 - val_loss: 0.5137 - val_accuracy: 0.8340\n",
      "Epoch 421/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2854 - accuracy: 0.8751 - val_loss: 0.5096 - val_accuracy: 0.8323\n",
      "Epoch 422/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2867 - accuracy: 0.8746 - val_loss: 0.5078 - val_accuracy: 0.8321\n",
      "Epoch 423/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2868 - accuracy: 0.8743 - val_loss: 0.5080 - val_accuracy: 0.8336\n",
      "Epoch 424/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2864 - accuracy: 0.8746 - val_loss: 0.5158 - val_accuracy: 0.8296\n",
      "Epoch 425/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2869 - accuracy: 0.8739 - val_loss: 0.5089 - val_accuracy: 0.8312\n",
      "Epoch 426/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2866 - accuracy: 0.8744 - val_loss: 0.5069 - val_accuracy: 0.8335\n",
      "Epoch 427/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2877 - accuracy: 0.8744 - val_loss: 0.5059 - val_accuracy: 0.8333\n",
      "Epoch 428/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.8743 - val_loss: 0.5086 - val_accuracy: 0.8342\n",
      "Epoch 429/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2868 - accuracy: 0.8742 - val_loss: 0.5123 - val_accuracy: 0.8303\n",
      "Epoch 430/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2864 - accuracy: 0.8743 - val_loss: 0.5124 - val_accuracy: 0.8312\n",
      "Epoch 431/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2871 - accuracy: 0.8743 - val_loss: 0.5132 - val_accuracy: 0.8297\n",
      "Epoch 432/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2864 - accuracy: 0.8742 - val_loss: 0.5178 - val_accuracy: 0.8325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2863 - accuracy: 0.8744 - val_loss: 0.5111 - val_accuracy: 0.8349\n",
      "Epoch 434/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2854 - accuracy: 0.8748 - val_loss: 0.5106 - val_accuracy: 0.8352\n",
      "Epoch 435/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2856 - accuracy: 0.8744 - val_loss: 0.5116 - val_accuracy: 0.8339\n",
      "Epoch 436/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2849 - accuracy: 0.8751 - val_loss: 0.5110 - val_accuracy: 0.8357\n",
      "Epoch 437/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2869 - accuracy: 0.8742 - val_loss: 0.5108 - val_accuracy: 0.8354\n",
      "Epoch 438/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2852 - accuracy: 0.8750 - val_loss: 0.5246 - val_accuracy: 0.8239\n",
      "Epoch 439/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2855 - accuracy: 0.8750 - val_loss: 0.5098 - val_accuracy: 0.8361\n",
      "Epoch 440/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2844 - accuracy: 0.8755 - val_loss: 0.5136 - val_accuracy: 0.8332\n",
      "Epoch 441/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2847 - accuracy: 0.8751 - val_loss: 0.5270 - val_accuracy: 0.8155\n",
      "Epoch 442/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2852 - accuracy: 0.8755 - val_loss: 0.5079 - val_accuracy: 0.8315\n",
      "Epoch 443/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2849 - accuracy: 0.8754 - val_loss: 0.5180 - val_accuracy: 0.8326\n",
      "Epoch 444/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2848 - accuracy: 0.8754 - val_loss: 0.5152 - val_accuracy: 0.8332\n",
      "Epoch 445/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2856 - accuracy: 0.8749 - val_loss: 0.5156 - val_accuracy: 0.8354\n",
      "Epoch 446/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2866 - accuracy: 0.8745 - val_loss: 0.5122 - val_accuracy: 0.8308\n",
      "Epoch 447/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2854 - accuracy: 0.8752 - val_loss: 0.5180 - val_accuracy: 0.8333\n",
      "Epoch 448/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2856 - accuracy: 0.8753 - val_loss: 0.5111 - val_accuracy: 0.8324\n",
      "Epoch 449/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2852 - accuracy: 0.8750 - val_loss: 0.5124 - val_accuracy: 0.8324\n",
      "Epoch 450/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2849 - accuracy: 0.8750 - val_loss: 0.5083 - val_accuracy: 0.8350\n",
      "Epoch 451/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2850 - accuracy: 0.8754 - val_loss: 0.5063 - val_accuracy: 0.8337\n",
      "Epoch 452/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2848 - accuracy: 0.8752 - val_loss: 0.5109 - val_accuracy: 0.8265\n",
      "Epoch 453/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2835 - accuracy: 0.8762 - val_loss: 0.5123 - val_accuracy: 0.8329\n",
      "Epoch 454/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2842 - accuracy: 0.8757 - val_loss: 0.5140 - val_accuracy: 0.8344\n",
      "Epoch 455/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2845 - accuracy: 0.8749 - val_loss: 0.5083 - val_accuracy: 0.8346\n",
      "Epoch 456/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2850 - accuracy: 0.8752 - val_loss: 0.5119 - val_accuracy: 0.8344\n",
      "Epoch 457/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2849 - accuracy: 0.8750 - val_loss: 0.5111 - val_accuracy: 0.8347\n",
      "Epoch 458/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2854 - accuracy: 0.8751 - val_loss: 0.5096 - val_accuracy: 0.8334\n",
      "Epoch 459/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2844 - accuracy: 0.8748 - val_loss: 0.5140 - val_accuracy: 0.8332\n",
      "Epoch 460/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2844 - accuracy: 0.8758 - val_loss: 0.5131 - val_accuracy: 0.8353\n",
      "Epoch 461/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2837 - accuracy: 0.8759 - val_loss: 0.5092 - val_accuracy: 0.8333\n",
      "Epoch 462/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2838 - accuracy: 0.8757 - val_loss: 0.5177 - val_accuracy: 0.8336\n",
      "Epoch 463/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2837 - accuracy: 0.8758 - val_loss: 0.5164 - val_accuracy: 0.8332\n",
      "Epoch 464/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2830 - accuracy: 0.8755 - val_loss: 0.5160 - val_accuracy: 0.8345\n",
      "Epoch 465/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2837 - accuracy: 0.8755 - val_loss: 0.5160 - val_accuracy: 0.8358\n",
      "Epoch 466/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2836 - accuracy: 0.8759 - val_loss: 0.5133 - val_accuracy: 0.8352\n",
      "Epoch 467/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2821 - accuracy: 0.8766 - val_loss: 0.5183 - val_accuracy: 0.8340\n",
      "Epoch 468/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2827 - accuracy: 0.8764 - val_loss: 0.5125 - val_accuracy: 0.8341\n",
      "Epoch 469/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2845 - accuracy: 0.8752 - val_loss: 0.5193 - val_accuracy: 0.8364\n",
      "Epoch 470/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2828 - accuracy: 0.8763 - val_loss: 0.5193 - val_accuracy: 0.8355\n",
      "Epoch 471/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2839 - accuracy: 0.8762 - val_loss: 0.5165 - val_accuracy: 0.8351\n",
      "Epoch 472/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2837 - accuracy: 0.8756 - val_loss: 0.5222 - val_accuracy: 0.8305\n",
      "Epoch 473/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2841 - accuracy: 0.8758 - val_loss: 0.5178 - val_accuracy: 0.8372\n",
      "Epoch 474/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2834 - accuracy: 0.8758 - val_loss: 0.5489 - val_accuracy: 0.8219\n",
      "Epoch 475/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2837 - accuracy: 0.8758 - val_loss: 0.5197 - val_accuracy: 0.8272\n",
      "Epoch 476/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2826 - accuracy: 0.8765 - val_loss: 0.5217 - val_accuracy: 0.8336\n",
      "Epoch 477/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2823 - accuracy: 0.8766 - val_loss: 0.5188 - val_accuracy: 0.8355\n",
      "Epoch 478/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2831 - accuracy: 0.8758 - val_loss: 0.5270 - val_accuracy: 0.8240\n",
      "Epoch 479/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2830 - accuracy: 0.8764 - val_loss: 0.5193 - val_accuracy: 0.8340\n",
      "Epoch 480/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2824 - accuracy: 0.8761 - val_loss: 0.5218 - val_accuracy: 0.8338\n",
      "Epoch 481/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2831 - accuracy: 0.8761 - val_loss: 0.5141 - val_accuracy: 0.8357\n",
      "Epoch 482/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2830 - accuracy: 0.8763 - val_loss: 0.5188 - val_accuracy: 0.8358\n",
      "Epoch 483/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2825 - accuracy: 0.8763 - val_loss: 0.5198 - val_accuracy: 0.8344\n",
      "Epoch 484/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2833 - accuracy: 0.8762 - val_loss: 0.5233 - val_accuracy: 0.8285\n",
      "Epoch 485/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2834 - accuracy: 0.8760 - val_loss: 0.5184 - val_accuracy: 0.8357\n",
      "Epoch 486/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2830 - accuracy: 0.8762 - val_loss: 0.5197 - val_accuracy: 0.8341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2828 - accuracy: 0.8761 - val_loss: 0.5192 - val_accuracy: 0.8327\n",
      "Epoch 488/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2826 - accuracy: 0.8767 - val_loss: 0.5222 - val_accuracy: 0.8333\n",
      "Epoch 489/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2822 - accuracy: 0.8765 - val_loss: 0.5265 - val_accuracy: 0.8255\n",
      "Epoch 490/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2819 - accuracy: 0.8767 - val_loss: 0.5203 - val_accuracy: 0.8360\n",
      "Epoch 491/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2820 - accuracy: 0.8767 - val_loss: 0.5182 - val_accuracy: 0.8351\n",
      "Epoch 492/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2832 - accuracy: 0.8761 - val_loss: 0.5273 - val_accuracy: 0.8330\n",
      "Epoch 493/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2821 - accuracy: 0.8764 - val_loss: 0.5210 - val_accuracy: 0.8341\n",
      "Epoch 494/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2818 - accuracy: 0.8766 - val_loss: 0.5187 - val_accuracy: 0.8348\n",
      "Epoch 495/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2816 - accuracy: 0.8767 - val_loss: 0.5187 - val_accuracy: 0.8289\n",
      "Epoch 496/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2816 - accuracy: 0.8771 - val_loss: 0.5206 - val_accuracy: 0.8294\n",
      "Epoch 497/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2812 - accuracy: 0.8769 - val_loss: 0.5191 - val_accuracy: 0.8325\n",
      "Epoch 498/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2811 - accuracy: 0.8770 - val_loss: 0.5189 - val_accuracy: 0.8348\n",
      "Epoch 499/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2814 - accuracy: 0.8770 - val_loss: 0.5238 - val_accuracy: 0.8278\n",
      "Epoch 500/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2808 - accuracy: 0.8771 - val_loss: 0.5288 - val_accuracy: 0.8351\n",
      "Epoch 501/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2820 - accuracy: 0.8765 - val_loss: 0.5166 - val_accuracy: 0.8344\n",
      "Epoch 502/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2810 - accuracy: 0.8769 - val_loss: 0.5159 - val_accuracy: 0.8321\n",
      "Epoch 503/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2799 - accuracy: 0.8779 - val_loss: 0.5140 - val_accuracy: 0.8360\n",
      "Epoch 504/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2803 - accuracy: 0.8774 - val_loss: 0.5155 - val_accuracy: 0.8363\n",
      "Epoch 505/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2819 - accuracy: 0.8767 - val_loss: 0.5213 - val_accuracy: 0.8346\n",
      "Epoch 506/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2812 - accuracy: 0.8775 - val_loss: 0.5180 - val_accuracy: 0.8354\n",
      "Epoch 507/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2810 - accuracy: 0.8771 - val_loss: 0.5231 - val_accuracy: 0.8363\n",
      "Epoch 508/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2807 - accuracy: 0.8777 - val_loss: 0.5253 - val_accuracy: 0.8347\n",
      "Epoch 509/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2803 - accuracy: 0.8775 - val_loss: 0.5191 - val_accuracy: 0.8294\n",
      "Epoch 510/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2822 - accuracy: 0.8765 - val_loss: 0.5308 - val_accuracy: 0.8253\n",
      "Epoch 511/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2816 - accuracy: 0.8769 - val_loss: 0.5226 - val_accuracy: 0.8365\n",
      "Epoch 512/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2814 - accuracy: 0.8770 - val_loss: 0.5222 - val_accuracy: 0.8308\n",
      "Epoch 513/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2819 - accuracy: 0.8767 - val_loss: 0.5203 - val_accuracy: 0.8345\n",
      "Epoch 514/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2802 - accuracy: 0.8775 - val_loss: 0.5235 - val_accuracy: 0.8322\n",
      "Epoch 515/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2807 - accuracy: 0.8776 - val_loss: 0.5252 - val_accuracy: 0.8341\n",
      "Epoch 516/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2811 - accuracy: 0.8771 - val_loss: 0.5294 - val_accuracy: 0.8353\n",
      "Epoch 517/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2814 - accuracy: 0.8768 - val_loss: 0.5199 - val_accuracy: 0.8328\n",
      "Epoch 518/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2806 - accuracy: 0.8774 - val_loss: 0.5291 - val_accuracy: 0.8360\n",
      "Epoch 519/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2799 - accuracy: 0.8779 - val_loss: 0.5226 - val_accuracy: 0.8346\n",
      "Epoch 520/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2816 - accuracy: 0.8768 - val_loss: 0.5281 - val_accuracy: 0.8360\n",
      "Epoch 521/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2800 - accuracy: 0.8776 - val_loss: 0.5344 - val_accuracy: 0.8336\n",
      "Epoch 522/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2803 - accuracy: 0.8780 - val_loss: 0.5244 - val_accuracy: 0.8347\n",
      "Epoch 523/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2801 - accuracy: 0.8780 - val_loss: 0.5268 - val_accuracy: 0.8320\n",
      "Epoch 524/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2805 - accuracy: 0.8775 - val_loss: 0.5221 - val_accuracy: 0.8364\n",
      "Epoch 525/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2809 - accuracy: 0.8772 - val_loss: 0.5205 - val_accuracy: 0.8340\n",
      "Epoch 526/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2798 - accuracy: 0.8775 - val_loss: 0.5250 - val_accuracy: 0.8350\n",
      "Epoch 527/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2802 - accuracy: 0.8776 - val_loss: 0.5195 - val_accuracy: 0.8376\n",
      "Epoch 528/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2799 - accuracy: 0.8782 - val_loss: 0.5314 - val_accuracy: 0.8347\n",
      "Epoch 529/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2793 - accuracy: 0.8786 - val_loss: 0.5280 - val_accuracy: 0.8339\n",
      "Epoch 530/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2806 - accuracy: 0.8774 - val_loss: 0.5294 - val_accuracy: 0.8353\n",
      "Epoch 531/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2790 - accuracy: 0.8782 - val_loss: 0.5265 - val_accuracy: 0.8364\n",
      "Epoch 532/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2794 - accuracy: 0.8779 - val_loss: 0.5310 - val_accuracy: 0.8351\n",
      "Epoch 533/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2796 - accuracy: 0.8781 - val_loss: 0.5472 - val_accuracy: 0.8195\n",
      "Epoch 534/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2792 - accuracy: 0.8777 - val_loss: 0.5263 - val_accuracy: 0.8351\n",
      "Epoch 535/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2799 - accuracy: 0.8779 - val_loss: 0.5264 - val_accuracy: 0.8353\n",
      "Epoch 536/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2796 - accuracy: 0.8780 - val_loss: 0.5290 - val_accuracy: 0.8311\n",
      "Epoch 537/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2800 - accuracy: 0.8773 - val_loss: 0.5272 - val_accuracy: 0.8360\n",
      "Epoch 538/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2790 - accuracy: 0.8778 - val_loss: 0.5322 - val_accuracy: 0.8328\n",
      "Epoch 539/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2786 - accuracy: 0.8783 - val_loss: 0.5208 - val_accuracy: 0.8359\n",
      "Epoch 540/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2797 - accuracy: 0.8776 - val_loss: 0.5262 - val_accuracy: 0.8351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2802 - accuracy: 0.8772 - val_loss: 0.5278 - val_accuracy: 0.8342\n",
      "Epoch 542/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2789 - accuracy: 0.8780 - val_loss: 0.5404 - val_accuracy: 0.8330\n",
      "Epoch 543/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2794 - accuracy: 0.8784 - val_loss: 0.5247 - val_accuracy: 0.8355\n",
      "Epoch 544/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2789 - accuracy: 0.8786 - val_loss: 0.5246 - val_accuracy: 0.8340\n",
      "Epoch 545/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2793 - accuracy: 0.8782 - val_loss: 0.5231 - val_accuracy: 0.8335\n",
      "Epoch 546/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2790 - accuracy: 0.8783 - val_loss: 0.5255 - val_accuracy: 0.8325\n",
      "Epoch 547/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2799 - accuracy: 0.8778 - val_loss: 0.5266 - val_accuracy: 0.8338\n",
      "Epoch 548/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2803 - accuracy: 0.8775 - val_loss: 0.5372 - val_accuracy: 0.8327\n",
      "Epoch 549/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2796 - accuracy: 0.8781 - val_loss: 0.5261 - val_accuracy: 0.8342\n",
      "Epoch 550/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2783 - accuracy: 0.8782 - val_loss: 0.5201 - val_accuracy: 0.8353\n",
      "Epoch 551/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2797 - accuracy: 0.8776 - val_loss: 0.5221 - val_accuracy: 0.8347\n",
      "Epoch 552/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2792 - accuracy: 0.8778 - val_loss: 0.5249 - val_accuracy: 0.8361\n",
      "Epoch 553/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2789 - accuracy: 0.8779 - val_loss: 0.5236 - val_accuracy: 0.8373\n",
      "Epoch 554/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2787 - accuracy: 0.8781 - val_loss: 0.5198 - val_accuracy: 0.8355\n",
      "Epoch 555/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2785 - accuracy: 0.8787 - val_loss: 0.5213 - val_accuracy: 0.8349\n",
      "Epoch 556/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2789 - accuracy: 0.8778 - val_loss: 0.5248 - val_accuracy: 0.8375\n",
      "Epoch 557/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2793 - accuracy: 0.8777 - val_loss: 0.5278 - val_accuracy: 0.8359\n",
      "Epoch 558/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2788 - accuracy: 0.8782 - val_loss: 0.5353 - val_accuracy: 0.8276\n",
      "Epoch 559/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2780 - accuracy: 0.8782 - val_loss: 0.5315 - val_accuracy: 0.8281\n",
      "Epoch 560/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2794 - accuracy: 0.8776 - val_loss: 0.5257 - val_accuracy: 0.8361\n",
      "Epoch 561/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2794 - accuracy: 0.8777 - val_loss: 0.5246 - val_accuracy: 0.8362\n",
      "Epoch 562/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2782 - accuracy: 0.8788 - val_loss: 0.5207 - val_accuracy: 0.8363\n",
      "Epoch 563/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2788 - accuracy: 0.8782 - val_loss: 0.5312 - val_accuracy: 0.8318\n",
      "Epoch 564/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2793 - accuracy: 0.8779 - val_loss: 0.5253 - val_accuracy: 0.8353\n",
      "Epoch 565/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2781 - accuracy: 0.8785 - val_loss: 0.5245 - val_accuracy: 0.8352\n",
      "Epoch 566/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2778 - accuracy: 0.8785 - val_loss: 0.5256 - val_accuracy: 0.8378\n",
      "Epoch 567/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2789 - accuracy: 0.8788 - val_loss: 0.5287 - val_accuracy: 0.8362\n",
      "Epoch 568/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2787 - accuracy: 0.8783 - val_loss: 0.5322 - val_accuracy: 0.8349\n",
      "Epoch 569/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2799 - accuracy: 0.8779 - val_loss: 0.5295 - val_accuracy: 0.8371\n",
      "Epoch 570/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2781 - accuracy: 0.8789 - val_loss: 0.5372 - val_accuracy: 0.8331\n",
      "Epoch 571/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2779 - accuracy: 0.8789 - val_loss: 0.5433 - val_accuracy: 0.8293\n",
      "Epoch 572/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2782 - accuracy: 0.8786 - val_loss: 0.5280 - val_accuracy: 0.8362\n",
      "Epoch 573/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2773 - accuracy: 0.8789 - val_loss: 0.5334 - val_accuracy: 0.8265\n",
      "Epoch 574/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2777 - accuracy: 0.8787 - val_loss: 0.5267 - val_accuracy: 0.8329\n",
      "Epoch 575/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2792 - accuracy: 0.8779 - val_loss: 0.5179 - val_accuracy: 0.8332\n",
      "Epoch 576/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2789 - accuracy: 0.8787 - val_loss: 0.5265 - val_accuracy: 0.8334\n",
      "Epoch 577/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2783 - accuracy: 0.8785 - val_loss: 0.5221 - val_accuracy: 0.8371\n",
      "Epoch 578/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2773 - accuracy: 0.8787 - val_loss: 0.5237 - val_accuracy: 0.8373\n",
      "Epoch 579/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2783 - accuracy: 0.8783 - val_loss: 0.5233 - val_accuracy: 0.8369\n",
      "Epoch 580/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2784 - accuracy: 0.8786 - val_loss: 0.5337 - val_accuracy: 0.8332\n",
      "Epoch 581/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2787 - accuracy: 0.8785 - val_loss: 0.5442 - val_accuracy: 0.8164\n",
      "Epoch 582/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2782 - accuracy: 0.8783 - val_loss: 0.5279 - val_accuracy: 0.8367\n",
      "Epoch 583/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2783 - accuracy: 0.8786 - val_loss: 0.5246 - val_accuracy: 0.8374\n",
      "Epoch 584/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2772 - accuracy: 0.8789 - val_loss: 0.5235 - val_accuracy: 0.8374\n",
      "Epoch 585/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2779 - accuracy: 0.8786 - val_loss: 0.5392 - val_accuracy: 0.8319\n",
      "Epoch 586/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2770 - accuracy: 0.8793 - val_loss: 0.5223 - val_accuracy: 0.8345\n",
      "Epoch 587/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2772 - accuracy: 0.8786 - val_loss: 0.5249 - val_accuracy: 0.8352\n",
      "Epoch 588/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2781 - accuracy: 0.8785 - val_loss: 0.5262 - val_accuracy: 0.8349\n",
      "Epoch 589/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2783 - accuracy: 0.8786 - val_loss: 0.5243 - val_accuracy: 0.8368\n",
      "Epoch 590/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2787 - accuracy: 0.8786 - val_loss: 0.5385 - val_accuracy: 0.8318\n",
      "Epoch 591/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2799 - accuracy: 0.8784 - val_loss: 0.5320 - val_accuracy: 0.8356\n",
      "Epoch 592/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2768 - accuracy: 0.8790 - val_loss: 0.5240 - val_accuracy: 0.8365\n",
      "Epoch 593/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2769 - accuracy: 0.8790 - val_loss: 0.5250 - val_accuracy: 0.8372\n",
      "Epoch 594/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2778 - accuracy: 0.8787 - val_loss: 0.5264 - val_accuracy: 0.8325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 595/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2766 - accuracy: 0.8793 - val_loss: 0.5206 - val_accuracy: 0.8344\n",
      "Epoch 596/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2769 - accuracy: 0.8791 - val_loss: 0.5251 - val_accuracy: 0.8357\n",
      "Epoch 597/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2763 - accuracy: 0.8794 - val_loss: 0.5284 - val_accuracy: 0.8365\n",
      "Epoch 598/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2773 - accuracy: 0.8792 - val_loss: 0.5355 - val_accuracy: 0.8361\n",
      "Epoch 599/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2766 - accuracy: 0.8793 - val_loss: 0.5278 - val_accuracy: 0.8369\n",
      "Epoch 600/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2763 - accuracy: 0.8788 - val_loss: 0.5233 - val_accuracy: 0.8377\n",
      "Epoch 601/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2768 - accuracy: 0.8790 - val_loss: 0.5277 - val_accuracy: 0.8365\n",
      "Epoch 602/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2760 - accuracy: 0.8798 - val_loss: 0.5210 - val_accuracy: 0.8372\n",
      "Epoch 603/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2773 - accuracy: 0.8789 - val_loss: 0.5255 - val_accuracy: 0.8370\n",
      "Epoch 604/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2773 - accuracy: 0.8792 - val_loss: 0.5261 - val_accuracy: 0.8350\n",
      "Epoch 605/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2762 - accuracy: 0.8793 - val_loss: 0.5266 - val_accuracy: 0.8369\n",
      "Epoch 606/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2775 - accuracy: 0.8789 - val_loss: 0.5287 - val_accuracy: 0.8362\n",
      "Epoch 607/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2762 - accuracy: 0.8792 - val_loss: 0.5273 - val_accuracy: 0.8327\n",
      "Epoch 608/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2755 - accuracy: 0.8796 - val_loss: 0.5275 - val_accuracy: 0.8381\n",
      "Epoch 609/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2769 - accuracy: 0.8791 - val_loss: 0.5282 - val_accuracy: 0.8330\n",
      "Epoch 610/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2776 - accuracy: 0.8790 - val_loss: 0.5299 - val_accuracy: 0.8351\n",
      "Epoch 611/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2760 - accuracy: 0.8791 - val_loss: 0.5261 - val_accuracy: 0.8364\n",
      "Epoch 612/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2769 - accuracy: 0.8792 - val_loss: 0.5243 - val_accuracy: 0.8368\n",
      "Epoch 613/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2763 - accuracy: 0.8789 - val_loss: 0.5214 - val_accuracy: 0.8371\n",
      "Epoch 614/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2770 - accuracy: 0.8793 - val_loss: 0.5272 - val_accuracy: 0.8373\n",
      "Epoch 615/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2757 - accuracy: 0.8802 - val_loss: 0.5236 - val_accuracy: 0.8366\n",
      "Epoch 616/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2762 - accuracy: 0.8795 - val_loss: 0.5223 - val_accuracy: 0.8375\n",
      "Epoch 617/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2767 - accuracy: 0.8795 - val_loss: 0.5285 - val_accuracy: 0.8366\n",
      "Epoch 618/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2760 - accuracy: 0.8794 - val_loss: 0.5376 - val_accuracy: 0.8284\n",
      "Epoch 619/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2758 - accuracy: 0.8796 - val_loss: 0.5244 - val_accuracy: 0.8368\n",
      "Epoch 620/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2772 - accuracy: 0.8786 - val_loss: 0.5273 - val_accuracy: 0.8365\n",
      "Epoch 621/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2760 - accuracy: 0.8795 - val_loss: 0.5310 - val_accuracy: 0.8353\n",
      "Epoch 622/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2759 - accuracy: 0.8791 - val_loss: 0.5283 - val_accuracy: 0.8378\n",
      "Epoch 623/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2756 - accuracy: 0.8796 - val_loss: 0.5291 - val_accuracy: 0.8365\n",
      "Epoch 624/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2765 - accuracy: 0.8793 - val_loss: 0.5374 - val_accuracy: 0.8342\n",
      "Epoch 625/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2754 - accuracy: 0.8798 - val_loss: 0.5303 - val_accuracy: 0.8335\n",
      "Epoch 626/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2756 - accuracy: 0.8799 - val_loss: 0.5276 - val_accuracy: 0.8368\n",
      "Epoch 627/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2750 - accuracy: 0.8801 - val_loss: 0.5272 - val_accuracy: 0.8371\n",
      "Epoch 628/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2754 - accuracy: 0.8795 - val_loss: 0.5302 - val_accuracy: 0.8374\n",
      "Epoch 629/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2750 - accuracy: 0.8799 - val_loss: 0.5377 - val_accuracy: 0.8379\n",
      "Epoch 630/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2755 - accuracy: 0.8797 - val_loss: 0.5343 - val_accuracy: 0.8350\n",
      "Epoch 631/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2760 - accuracy: 0.8799 - val_loss: 0.5259 - val_accuracy: 0.8369\n",
      "Epoch 632/1000\n",
      "216750/216750 [==============================] - 1s 6us/step - loss: 0.2746 - accuracy: 0.8805 - val_loss: 0.5277 - val_accuracy: 0.8378\n",
      "Epoch 633/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2748 - accuracy: 0.8802 - val_loss: 0.5340 - val_accuracy: 0.8344\n",
      "Epoch 634/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2770 - accuracy: 0.8791 - val_loss: 0.5343 - val_accuracy: 0.8346\n",
      "Epoch 635/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2753 - accuracy: 0.8799 - val_loss: 0.5316 - val_accuracy: 0.8379\n",
      "Epoch 636/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2754 - accuracy: 0.8797 - val_loss: 0.5276 - val_accuracy: 0.8373\n",
      "Epoch 637/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2752 - accuracy: 0.8802 - val_loss: 0.5367 - val_accuracy: 0.8338\n",
      "Epoch 638/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2755 - accuracy: 0.8798 - val_loss: 0.5330 - val_accuracy: 0.8266\n",
      "Epoch 639/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2760 - accuracy: 0.8801 - val_loss: 0.5342 - val_accuracy: 0.8373\n",
      "Epoch 640/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2744 - accuracy: 0.8801 - val_loss: 0.5317 - val_accuracy: 0.8341\n",
      "Epoch 641/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2748 - accuracy: 0.8802 - val_loss: 0.5342 - val_accuracy: 0.8379\n",
      "Epoch 642/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2757 - accuracy: 0.8796 - val_loss: 0.5404 - val_accuracy: 0.8325\n",
      "Epoch 643/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2749 - accuracy: 0.8803 - val_loss: 0.5310 - val_accuracy: 0.8352\n",
      "Epoch 644/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2740 - accuracy: 0.8802 - val_loss: 0.5299 - val_accuracy: 0.8368\n",
      "Epoch 645/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2746 - accuracy: 0.8799 - val_loss: 0.5313 - val_accuracy: 0.8327\n",
      "Epoch 646/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2760 - accuracy: 0.8793 - val_loss: 0.5295 - val_accuracy: 0.8348\n",
      "Epoch 647/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2747 - accuracy: 0.8801 - val_loss: 0.5478 - val_accuracy: 0.8244\n",
      "Epoch 648/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2742 - accuracy: 0.8808 - val_loss: 0.5283 - val_accuracy: 0.8359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2749 - accuracy: 0.8800 - val_loss: 0.5346 - val_accuracy: 0.8384\n",
      "Epoch 650/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2760 - accuracy: 0.8798 - val_loss: 0.5334 - val_accuracy: 0.8374\n",
      "Epoch 651/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2753 - accuracy: 0.8806 - val_loss: 0.5384 - val_accuracy: 0.8361\n",
      "Epoch 652/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2752 - accuracy: 0.8799 - val_loss: 0.5312 - val_accuracy: 0.8356\n",
      "Epoch 653/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2751 - accuracy: 0.8799 - val_loss: 0.5313 - val_accuracy: 0.8373\n",
      "Epoch 654/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2741 - accuracy: 0.8807 - val_loss: 0.5381 - val_accuracy: 0.8288\n",
      "Epoch 655/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2742 - accuracy: 0.8803 - val_loss: 0.5269 - val_accuracy: 0.8370\n",
      "Epoch 656/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2749 - accuracy: 0.8799 - val_loss: 0.5340 - val_accuracy: 0.8361\n",
      "Epoch 657/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2745 - accuracy: 0.8799 - val_loss: 0.5323 - val_accuracy: 0.8341\n",
      "Epoch 658/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2744 - accuracy: 0.8803 - val_loss: 0.5310 - val_accuracy: 0.8345\n",
      "Epoch 659/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2749 - accuracy: 0.8801 - val_loss: 0.5364 - val_accuracy: 0.8364\n",
      "Epoch 660/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2749 - accuracy: 0.8799 - val_loss: 0.5326 - val_accuracy: 0.8370\n",
      "Epoch 661/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2740 - accuracy: 0.8799 - val_loss: 0.5301 - val_accuracy: 0.8380\n",
      "Epoch 662/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2742 - accuracy: 0.8806 - val_loss: 0.5244 - val_accuracy: 0.8380\n",
      "Epoch 663/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2734 - accuracy: 0.8805 - val_loss: 0.5307 - val_accuracy: 0.8384\n",
      "Epoch 664/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2738 - accuracy: 0.8808 - val_loss: 0.5267 - val_accuracy: 0.8387\n",
      "Epoch 665/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2726 - accuracy: 0.8807 - val_loss: 0.5441 - val_accuracy: 0.8247\n",
      "Epoch 666/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2737 - accuracy: 0.8804 - val_loss: 0.5420 - val_accuracy: 0.8343\n",
      "Epoch 667/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2732 - accuracy: 0.8807 - val_loss: 0.5318 - val_accuracy: 0.8345\n",
      "Epoch 668/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2739 - accuracy: 0.8804 - val_loss: 0.5514 - val_accuracy: 0.8333\n",
      "Epoch 669/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2750 - accuracy: 0.8799 - val_loss: 0.5389 - val_accuracy: 0.8367\n",
      "Epoch 670/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2739 - accuracy: 0.8806 - val_loss: 0.5500 - val_accuracy: 0.8326\n",
      "Epoch 671/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2740 - accuracy: 0.8808 - val_loss: 0.5382 - val_accuracy: 0.8355\n",
      "Epoch 672/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2742 - accuracy: 0.8802 - val_loss: 0.5342 - val_accuracy: 0.8379\n",
      "Epoch 673/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2739 - accuracy: 0.8804 - val_loss: 0.5388 - val_accuracy: 0.8356\n",
      "Epoch 674/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2747 - accuracy: 0.8800 - val_loss: 0.5524 - val_accuracy: 0.8281\n",
      "Epoch 675/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2736 - accuracy: 0.8806 - val_loss: 0.5395 - val_accuracy: 0.8367\n",
      "Epoch 676/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2754 - accuracy: 0.8799 - val_loss: 0.5356 - val_accuracy: 0.8339\n",
      "Epoch 677/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2733 - accuracy: 0.8808 - val_loss: 0.5348 - val_accuracy: 0.8369\n",
      "Epoch 678/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2736 - accuracy: 0.8809 - val_loss: 0.5384 - val_accuracy: 0.8341\n",
      "Epoch 679/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2748 - accuracy: 0.8799 - val_loss: 0.5417 - val_accuracy: 0.8356\n",
      "Epoch 680/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2738 - accuracy: 0.8807 - val_loss: 0.5358 - val_accuracy: 0.8376\n",
      "Epoch 681/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2731 - accuracy: 0.8807 - val_loss: 0.5423 - val_accuracy: 0.8338\n",
      "Epoch 682/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2732 - accuracy: 0.8808 - val_loss: 0.5341 - val_accuracy: 0.8347\n",
      "Epoch 683/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2732 - accuracy: 0.8811 - val_loss: 0.5328 - val_accuracy: 0.8370\n",
      "Epoch 684/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2737 - accuracy: 0.8804 - val_loss: 0.5356 - val_accuracy: 0.8332\n",
      "Epoch 685/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2743 - accuracy: 0.8805 - val_loss: 0.5401 - val_accuracy: 0.8356\n",
      "Epoch 686/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2745 - accuracy: 0.8804 - val_loss: 0.5422 - val_accuracy: 0.8343\n",
      "Epoch 687/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2736 - accuracy: 0.8808 - val_loss: 0.5397 - val_accuracy: 0.8374\n",
      "Epoch 688/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2752 - accuracy: 0.8800 - val_loss: 0.5377 - val_accuracy: 0.8344\n",
      "Epoch 689/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2734 - accuracy: 0.8809 - val_loss: 0.5343 - val_accuracy: 0.8387\n",
      "Epoch 690/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2740 - accuracy: 0.8806 - val_loss: 0.5411 - val_accuracy: 0.8363\n",
      "Epoch 691/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2741 - accuracy: 0.8809 - val_loss: 0.5365 - val_accuracy: 0.8336\n",
      "Epoch 692/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2740 - accuracy: 0.8808 - val_loss: 0.5387 - val_accuracy: 0.8319\n",
      "Epoch 693/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2752 - accuracy: 0.8800 - val_loss: 0.5511 - val_accuracy: 0.8335\n",
      "Epoch 694/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2746 - accuracy: 0.8804 - val_loss: 0.5473 - val_accuracy: 0.8338\n",
      "Epoch 695/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2737 - accuracy: 0.8808 - val_loss: 0.5414 - val_accuracy: 0.8347\n",
      "Epoch 696/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2740 - accuracy: 0.8805 - val_loss: 0.5378 - val_accuracy: 0.8363\n",
      "Epoch 697/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2744 - accuracy: 0.8803 - val_loss: 0.5306 - val_accuracy: 0.8358\n",
      "Epoch 698/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2735 - accuracy: 0.8809 - val_loss: 0.5492 - val_accuracy: 0.8307\n",
      "Epoch 699/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2731 - accuracy: 0.8807 - val_loss: 0.5330 - val_accuracy: 0.8370\n",
      "Epoch 700/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2730 - accuracy: 0.8809 - val_loss: 0.5332 - val_accuracy: 0.8375\n",
      "Epoch 701/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2733 - accuracy: 0.8808 - val_loss: 0.5340 - val_accuracy: 0.8374\n",
      "Epoch 702/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2729 - accuracy: 0.8809 - val_loss: 0.5498 - val_accuracy: 0.8252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 703/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2737 - accuracy: 0.8808 - val_loss: 0.5383 - val_accuracy: 0.8373\n",
      "Epoch 704/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2734 - accuracy: 0.8807 - val_loss: 0.5349 - val_accuracy: 0.8351\n",
      "Epoch 705/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2727 - accuracy: 0.8812 - val_loss: 0.5362 - val_accuracy: 0.8369\n",
      "Epoch 706/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2728 - accuracy: 0.8816 - val_loss: 0.5352 - val_accuracy: 0.8370\n",
      "Epoch 707/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2734 - accuracy: 0.8805 - val_loss: 0.5343 - val_accuracy: 0.8373\n",
      "Epoch 708/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2736 - accuracy: 0.8812 - val_loss: 0.5367 - val_accuracy: 0.8378\n",
      "Epoch 709/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2732 - accuracy: 0.8810 - val_loss: 0.5405 - val_accuracy: 0.8308\n",
      "Epoch 710/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2731 - accuracy: 0.8809 - val_loss: 0.5342 - val_accuracy: 0.8359\n",
      "Epoch 711/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2736 - accuracy: 0.8807 - val_loss: 0.5344 - val_accuracy: 0.8368\n",
      "Epoch 712/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2729 - accuracy: 0.8812 - val_loss: 0.5293 - val_accuracy: 0.8386\n",
      "Epoch 713/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2732 - accuracy: 0.8807 - val_loss: 0.5302 - val_accuracy: 0.8379\n",
      "Epoch 714/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2721 - accuracy: 0.8814 - val_loss: 0.5339 - val_accuracy: 0.8345\n",
      "Epoch 715/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2717 - accuracy: 0.8815 - val_loss: 0.5351 - val_accuracy: 0.8382\n",
      "Epoch 716/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2730 - accuracy: 0.8808 - val_loss: 0.5370 - val_accuracy: 0.8364\n",
      "Epoch 717/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2732 - accuracy: 0.8808 - val_loss: 0.5320 - val_accuracy: 0.8360\n",
      "Epoch 718/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2723 - accuracy: 0.8818 - val_loss: 0.5393 - val_accuracy: 0.8341\n",
      "Epoch 719/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2726 - accuracy: 0.8810 - val_loss: 0.5349 - val_accuracy: 0.8359\n",
      "Epoch 720/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2720 - accuracy: 0.8818 - val_loss: 0.5332 - val_accuracy: 0.8372\n",
      "Epoch 721/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2718 - accuracy: 0.8817 - val_loss: 0.5354 - val_accuracy: 0.8342\n",
      "Epoch 722/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2732 - accuracy: 0.8809 - val_loss: 0.5411 - val_accuracy: 0.8383\n",
      "Epoch 723/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2722 - accuracy: 0.8810 - val_loss: 0.5331 - val_accuracy: 0.8381\n",
      "Epoch 724/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2725 - accuracy: 0.8811 - val_loss: 0.5337 - val_accuracy: 0.8384\n",
      "Epoch 725/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2730 - accuracy: 0.8810 - val_loss: 0.5402 - val_accuracy: 0.8349\n",
      "Epoch 726/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2728 - accuracy: 0.8810 - val_loss: 0.5434 - val_accuracy: 0.8360\n",
      "Epoch 727/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2731 - accuracy: 0.8810 - val_loss: 0.5392 - val_accuracy: 0.8340\n",
      "Epoch 728/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2725 - accuracy: 0.8809 - val_loss: 0.5592 - val_accuracy: 0.8296\n",
      "Epoch 729/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2714 - accuracy: 0.8817 - val_loss: 0.5367 - val_accuracy: 0.8369\n",
      "Epoch 730/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2716 - accuracy: 0.8814 - val_loss: 0.5325 - val_accuracy: 0.8385\n",
      "Epoch 731/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2722 - accuracy: 0.8811 - val_loss: 0.5356 - val_accuracy: 0.8379\n",
      "Epoch 732/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2719 - accuracy: 0.8813 - val_loss: 0.5595 - val_accuracy: 0.8325\n",
      "Epoch 733/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2717 - accuracy: 0.8815 - val_loss: 0.5413 - val_accuracy: 0.8359\n",
      "Epoch 734/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2710 - accuracy: 0.8820 - val_loss: 0.5344 - val_accuracy: 0.8394\n",
      "Epoch 735/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2719 - accuracy: 0.8813 - val_loss: 0.5409 - val_accuracy: 0.8378\n",
      "Epoch 736/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2727 - accuracy: 0.8812 - val_loss: 0.5452 - val_accuracy: 0.8302\n",
      "Epoch 737/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2725 - accuracy: 0.8811 - val_loss: 0.5451 - val_accuracy: 0.8357\n",
      "Epoch 738/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2733 - accuracy: 0.8807 - val_loss: 0.5433 - val_accuracy: 0.8361\n",
      "Epoch 739/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2713 - accuracy: 0.8818 - val_loss: 0.5441 - val_accuracy: 0.8340\n",
      "Epoch 740/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2724 - accuracy: 0.8812 - val_loss: 0.5449 - val_accuracy: 0.8369\n",
      "Epoch 741/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2731 - accuracy: 0.8812 - val_loss: 0.5393 - val_accuracy: 0.8361\n",
      "Epoch 742/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2719 - accuracy: 0.8815 - val_loss: 0.5463 - val_accuracy: 0.8344\n",
      "Epoch 743/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2726 - accuracy: 0.8810 - val_loss: 0.5350 - val_accuracy: 0.8386\n",
      "Epoch 744/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2718 - accuracy: 0.8815 - val_loss: 0.5459 - val_accuracy: 0.8315\n",
      "Epoch 745/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2724 - accuracy: 0.8814 - val_loss: 0.5447 - val_accuracy: 0.8372\n",
      "Epoch 746/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2720 - accuracy: 0.8813 - val_loss: 0.5376 - val_accuracy: 0.8366\n",
      "Epoch 747/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2721 - accuracy: 0.8818 - val_loss: 0.5425 - val_accuracy: 0.8357\n",
      "Epoch 748/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2723 - accuracy: 0.8814 - val_loss: 0.5436 - val_accuracy: 0.8378\n",
      "Epoch 749/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2725 - accuracy: 0.8815 - val_loss: 0.5369 - val_accuracy: 0.8376\n",
      "Epoch 750/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2713 - accuracy: 0.8816 - val_loss: 0.5334 - val_accuracy: 0.8381\n",
      "Epoch 751/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2729 - accuracy: 0.8807 - val_loss: 0.5449 - val_accuracy: 0.8310\n",
      "Epoch 752/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2727 - accuracy: 0.8808 - val_loss: 0.5397 - val_accuracy: 0.8373\n",
      "Epoch 753/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2718 - accuracy: 0.8814 - val_loss: 0.5366 - val_accuracy: 0.8378\n",
      "Epoch 754/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2707 - accuracy: 0.8820 - val_loss: 0.5378 - val_accuracy: 0.8387\n",
      "Epoch 755/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2716 - accuracy: 0.8818 - val_loss: 0.5457 - val_accuracy: 0.8373\n",
      "Epoch 756/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2720 - accuracy: 0.8811 - val_loss: 0.5521 - val_accuracy: 0.8361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 757/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2709 - accuracy: 0.8820 - val_loss: 0.5354 - val_accuracy: 0.8367\n",
      "Epoch 758/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2708 - accuracy: 0.8819 - val_loss: 0.5427 - val_accuracy: 0.8364\n",
      "Epoch 759/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2701 - accuracy: 0.8819 - val_loss: 0.5396 - val_accuracy: 0.8375\n",
      "Epoch 760/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2703 - accuracy: 0.8820 - val_loss: 0.5503 - val_accuracy: 0.8296\n",
      "Epoch 761/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2706 - accuracy: 0.8821 - val_loss: 0.5440 - val_accuracy: 0.8363\n",
      "Epoch 762/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2708 - accuracy: 0.8820 - val_loss: 0.5426 - val_accuracy: 0.8378\n",
      "Epoch 763/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2705 - accuracy: 0.8821 - val_loss: 0.5410 - val_accuracy: 0.8385\n",
      "Epoch 764/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2712 - accuracy: 0.8816 - val_loss: 0.5397 - val_accuracy: 0.8379\n",
      "Epoch 765/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2715 - accuracy: 0.8818 - val_loss: 0.5423 - val_accuracy: 0.8360\n",
      "Epoch 766/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2718 - accuracy: 0.8817 - val_loss: 0.5528 - val_accuracy: 0.8363\n",
      "Epoch 767/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2702 - accuracy: 0.8828 - val_loss: 0.5492 - val_accuracy: 0.8352\n",
      "Epoch 768/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2707 - accuracy: 0.8818 - val_loss: 0.5419 - val_accuracy: 0.8369\n",
      "Epoch 769/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2711 - accuracy: 0.8818 - val_loss: 0.5461 - val_accuracy: 0.8368\n",
      "Epoch 770/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2702 - accuracy: 0.8826 - val_loss: 0.5352 - val_accuracy: 0.8385\n",
      "Epoch 771/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2705 - accuracy: 0.8821 - val_loss: 0.5439 - val_accuracy: 0.8368\n",
      "Epoch 772/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2714 - accuracy: 0.8818 - val_loss: 0.5513 - val_accuracy: 0.8364\n",
      "Epoch 773/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2705 - accuracy: 0.8821 - val_loss: 0.5485 - val_accuracy: 0.8369\n",
      "Epoch 774/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2707 - accuracy: 0.8819 - val_loss: 0.5408 - val_accuracy: 0.8366\n",
      "Epoch 775/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2716 - accuracy: 0.8815 - val_loss: 0.5426 - val_accuracy: 0.8359\n",
      "Epoch 776/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2711 - accuracy: 0.8818 - val_loss: 0.5400 - val_accuracy: 0.8335\n",
      "Epoch 777/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2715 - accuracy: 0.8815 - val_loss: 0.5434 - val_accuracy: 0.8374\n",
      "Epoch 778/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2709 - accuracy: 0.8819 - val_loss: 0.5432 - val_accuracy: 0.8373\n",
      "Epoch 779/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2709 - accuracy: 0.8821 - val_loss: 0.5529 - val_accuracy: 0.8359\n",
      "Epoch 780/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2704 - accuracy: 0.8816 - val_loss: 0.5539 - val_accuracy: 0.8286\n",
      "Epoch 781/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2705 - accuracy: 0.8824 - val_loss: 0.5432 - val_accuracy: 0.8341\n",
      "Epoch 782/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2708 - accuracy: 0.8822 - val_loss: 0.5461 - val_accuracy: 0.8309\n",
      "Epoch 783/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2699 - accuracy: 0.8827 - val_loss: 0.5476 - val_accuracy: 0.8368\n",
      "Epoch 784/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2706 - accuracy: 0.8821 - val_loss: 0.5481 - val_accuracy: 0.8284\n",
      "Epoch 785/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2722 - accuracy: 0.8817 - val_loss: 0.5525 - val_accuracy: 0.8381\n",
      "Epoch 786/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2707 - accuracy: 0.8817 - val_loss: 0.5463 - val_accuracy: 0.8353\n",
      "Epoch 787/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2709 - accuracy: 0.8823 - val_loss: 0.5417 - val_accuracy: 0.8371\n",
      "Epoch 788/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2703 - accuracy: 0.8822 - val_loss: 0.5380 - val_accuracy: 0.8377\n",
      "Epoch 789/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2705 - accuracy: 0.8820 - val_loss: 0.5487 - val_accuracy: 0.8338\n",
      "Epoch 790/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2699 - accuracy: 0.8825 - val_loss: 0.5461 - val_accuracy: 0.8361\n",
      "Epoch 791/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2701 - accuracy: 0.8822 - val_loss: 0.5427 - val_accuracy: 0.8379\n",
      "Epoch 792/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2695 - accuracy: 0.8830 - val_loss: 0.5451 - val_accuracy: 0.8375\n",
      "Epoch 793/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2698 - accuracy: 0.8824 - val_loss: 0.5429 - val_accuracy: 0.8375\n",
      "Epoch 794/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2705 - accuracy: 0.8820 - val_loss: 0.5391 - val_accuracy: 0.8390\n",
      "Epoch 795/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2696 - accuracy: 0.8826 - val_loss: 0.5421 - val_accuracy: 0.8338\n",
      "Epoch 796/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2704 - accuracy: 0.8824 - val_loss: 0.5423 - val_accuracy: 0.8387\n",
      "Epoch 797/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2691 - accuracy: 0.8824 - val_loss: 0.5416 - val_accuracy: 0.8388\n",
      "Epoch 798/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2699 - accuracy: 0.8824 - val_loss: 0.5469 - val_accuracy: 0.8389\n",
      "Epoch 799/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2697 - accuracy: 0.8823 - val_loss: 0.5423 - val_accuracy: 0.8371\n",
      "Epoch 800/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2708 - accuracy: 0.8821 - val_loss: 0.5423 - val_accuracy: 0.8340\n",
      "Epoch 801/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2694 - accuracy: 0.8827 - val_loss: 0.5626 - val_accuracy: 0.8356\n",
      "Epoch 802/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2701 - accuracy: 0.8822 - val_loss: 0.5494 - val_accuracy: 0.8313\n",
      "Epoch 803/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2706 - accuracy: 0.8822 - val_loss: 0.5454 - val_accuracy: 0.8364\n",
      "Epoch 804/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2700 - accuracy: 0.8824 - val_loss: 0.5464 - val_accuracy: 0.8381\n",
      "Epoch 805/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2700 - accuracy: 0.8825 - val_loss: 0.5390 - val_accuracy: 0.8390\n",
      "Epoch 806/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2698 - accuracy: 0.8822 - val_loss: 0.5484 - val_accuracy: 0.8348\n",
      "Epoch 807/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2699 - accuracy: 0.8824 - val_loss: 0.5507 - val_accuracy: 0.8378\n",
      "Epoch 808/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2703 - accuracy: 0.8823 - val_loss: 0.5454 - val_accuracy: 0.8386\n",
      "Epoch 809/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2703 - accuracy: 0.8823 - val_loss: 0.5576 - val_accuracy: 0.8341\n",
      "Epoch 810/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2699 - accuracy: 0.8824 - val_loss: 0.5444 - val_accuracy: 0.8377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 811/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2696 - accuracy: 0.8828 - val_loss: 0.5414 - val_accuracy: 0.8387\n",
      "Epoch 812/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2698 - accuracy: 0.8824 - val_loss: 0.5555 - val_accuracy: 0.8356\n",
      "Epoch 813/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2693 - accuracy: 0.8829 - val_loss: 0.5541 - val_accuracy: 0.8338\n",
      "Epoch 814/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2702 - accuracy: 0.8825 - val_loss: 0.5459 - val_accuracy: 0.8380\n",
      "Epoch 815/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2693 - accuracy: 0.8827 - val_loss: 0.5526 - val_accuracy: 0.8379\n",
      "Epoch 816/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2699 - accuracy: 0.8825 - val_loss: 0.5445 - val_accuracy: 0.8391\n",
      "Epoch 817/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2698 - accuracy: 0.8821 - val_loss: 0.5485 - val_accuracy: 0.8366\n",
      "Epoch 818/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2703 - accuracy: 0.8822 - val_loss: 0.5527 - val_accuracy: 0.8307\n",
      "Epoch 819/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2686 - accuracy: 0.8831 - val_loss: 0.5451 - val_accuracy: 0.8375\n",
      "Epoch 820/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2699 - accuracy: 0.8823 - val_loss: 0.5473 - val_accuracy: 0.8373\n",
      "Epoch 821/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2691 - accuracy: 0.8826 - val_loss: 0.5503 - val_accuracy: 0.8379\n",
      "Epoch 822/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2691 - accuracy: 0.8828 - val_loss: 0.5526 - val_accuracy: 0.8381\n",
      "Epoch 823/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2680 - accuracy: 0.8834 - val_loss: 0.5487 - val_accuracy: 0.8386\n",
      "Epoch 824/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2699 - accuracy: 0.8823 - val_loss: 0.5527 - val_accuracy: 0.8357\n",
      "Epoch 825/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2694 - accuracy: 0.8828 - val_loss: 0.5499 - val_accuracy: 0.8359\n",
      "Epoch 826/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2702 - accuracy: 0.8823 - val_loss: 0.5564 - val_accuracy: 0.8377\n",
      "Epoch 827/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2709 - accuracy: 0.8824 - val_loss: 0.5576 - val_accuracy: 0.8356\n",
      "Epoch 828/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2709 - accuracy: 0.8824 - val_loss: 0.5593 - val_accuracy: 0.8364\n",
      "Epoch 829/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2701 - accuracy: 0.8821 - val_loss: 0.5513 - val_accuracy: 0.8328\n",
      "Epoch 830/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2695 - accuracy: 0.8825 - val_loss: 0.5565 - val_accuracy: 0.8381\n",
      "Epoch 831/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2698 - accuracy: 0.8825 - val_loss: 0.5516 - val_accuracy: 0.8385\n",
      "Epoch 832/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2696 - accuracy: 0.8824 - val_loss: 0.5498 - val_accuracy: 0.8357\n",
      "Epoch 833/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2711 - accuracy: 0.8818 - val_loss: 0.5508 - val_accuracy: 0.8375\n",
      "Epoch 834/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2698 - accuracy: 0.8826 - val_loss: 0.5455 - val_accuracy: 0.8374\n",
      "Epoch 835/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2703 - accuracy: 0.8820 - val_loss: 0.5511 - val_accuracy: 0.8357\n",
      "Epoch 836/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2703 - accuracy: 0.8821 - val_loss: 0.5521 - val_accuracy: 0.8378\n",
      "Epoch 837/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2696 - accuracy: 0.8824 - val_loss: 0.5516 - val_accuracy: 0.8380\n",
      "Epoch 838/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2696 - accuracy: 0.8827 - val_loss: 0.5549 - val_accuracy: 0.8374\n",
      "Epoch 839/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2690 - accuracy: 0.8829 - val_loss: 0.5526 - val_accuracy: 0.8338\n",
      "Epoch 840/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2692 - accuracy: 0.8828 - val_loss: 0.5548 - val_accuracy: 0.8378\n",
      "Epoch 841/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2679 - accuracy: 0.8833 - val_loss: 0.5451 - val_accuracy: 0.8388\n",
      "Epoch 842/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2689 - accuracy: 0.8830 - val_loss: 0.5504 - val_accuracy: 0.8375\n",
      "Epoch 843/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2691 - accuracy: 0.8828 - val_loss: 0.5531 - val_accuracy: 0.8352\n",
      "Epoch 844/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2695 - accuracy: 0.8824 - val_loss: 0.5611 - val_accuracy: 0.8323\n",
      "Epoch 845/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8831 - val_loss: 0.5615 - val_accuracy: 0.8349\n",
      "Epoch 846/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2695 - accuracy: 0.8821 - val_loss: 0.5475 - val_accuracy: 0.8361\n",
      "Epoch 847/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2685 - accuracy: 0.8831 - val_loss: 0.5502 - val_accuracy: 0.8385\n",
      "Epoch 848/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2696 - accuracy: 0.8826 - val_loss: 0.5475 - val_accuracy: 0.8387\n",
      "Epoch 849/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2688 - accuracy: 0.8830 - val_loss: 0.5521 - val_accuracy: 0.8358\n",
      "Epoch 850/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2703 - accuracy: 0.8823 - val_loss: 0.5484 - val_accuracy: 0.8379\n",
      "Epoch 851/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8833 - val_loss: 0.5543 - val_accuracy: 0.8336\n",
      "Epoch 852/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2690 - accuracy: 0.8828 - val_loss: 0.5547 - val_accuracy: 0.8353\n",
      "Epoch 853/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2690 - accuracy: 0.8831 - val_loss: 0.5518 - val_accuracy: 0.8384\n",
      "Epoch 854/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2692 - accuracy: 0.8827 - val_loss: 0.5532 - val_accuracy: 0.8382\n",
      "Epoch 855/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2698 - accuracy: 0.8822 - val_loss: 0.5541 - val_accuracy: 0.8358\n",
      "Epoch 856/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2685 - accuracy: 0.8829 - val_loss: 0.5462 - val_accuracy: 0.8390\n",
      "Epoch 857/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2696 - accuracy: 0.8825 - val_loss: 0.5523 - val_accuracy: 0.8383\n",
      "Epoch 858/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2686 - accuracy: 0.8827 - val_loss: 0.5518 - val_accuracy: 0.8393\n",
      "Epoch 859/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2687 - accuracy: 0.8831 - val_loss: 0.5610 - val_accuracy: 0.8323\n",
      "Epoch 860/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2706 - accuracy: 0.8818 - val_loss: 0.5576 - val_accuracy: 0.8367\n",
      "Epoch 861/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2690 - accuracy: 0.8827 - val_loss: 0.5622 - val_accuracy: 0.8345\n",
      "Epoch 862/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2683 - accuracy: 0.8831 - val_loss: 0.5506 - val_accuracy: 0.8380\n",
      "Epoch 863/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2691 - accuracy: 0.8831 - val_loss: 0.5558 - val_accuracy: 0.8328\n",
      "Epoch 864/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8835 - val_loss: 0.5572 - val_accuracy: 0.8294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 865/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2694 - accuracy: 0.8828 - val_loss: 0.5517 - val_accuracy: 0.8385\n",
      "Epoch 866/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2687 - accuracy: 0.8831 - val_loss: 0.5506 - val_accuracy: 0.8382\n",
      "Epoch 867/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2675 - accuracy: 0.8835 - val_loss: 0.5545 - val_accuracy: 0.8372\n",
      "Epoch 868/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8833 - val_loss: 0.5513 - val_accuracy: 0.8364\n",
      "Epoch 869/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2685 - accuracy: 0.8831 - val_loss: 0.5555 - val_accuracy: 0.8367\n",
      "Epoch 870/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2685 - accuracy: 0.8834 - val_loss: 0.5546 - val_accuracy: 0.8295\n",
      "Epoch 871/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2690 - accuracy: 0.8825 - val_loss: 0.5509 - val_accuracy: 0.8372\n",
      "Epoch 872/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2686 - accuracy: 0.8830 - val_loss: 0.5540 - val_accuracy: 0.8375\n",
      "Epoch 873/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2681 - accuracy: 0.8833 - val_loss: 0.5599 - val_accuracy: 0.8371\n",
      "Epoch 874/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8836 - val_loss: 0.5549 - val_accuracy: 0.8381\n",
      "Epoch 875/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2678 - accuracy: 0.8831 - val_loss: 0.5576 - val_accuracy: 0.8380\n",
      "Epoch 876/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2681 - accuracy: 0.8834 - val_loss: 0.5518 - val_accuracy: 0.8374\n",
      "Epoch 877/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8831 - val_loss: 0.5541 - val_accuracy: 0.8377\n",
      "Epoch 878/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2692 - accuracy: 0.8829 - val_loss: 0.5595 - val_accuracy: 0.8391\n",
      "Epoch 879/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2689 - accuracy: 0.8828 - val_loss: 0.5653 - val_accuracy: 0.8348\n",
      "Epoch 880/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8833 - val_loss: 0.5575 - val_accuracy: 0.8380\n",
      "Epoch 881/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2691 - accuracy: 0.8824 - val_loss: 0.5581 - val_accuracy: 0.8394\n",
      "Epoch 882/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8833 - val_loss: 0.5551 - val_accuracy: 0.8391\n",
      "Epoch 883/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2688 - accuracy: 0.8831 - val_loss: 0.5564 - val_accuracy: 0.8350\n",
      "Epoch 884/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2678 - accuracy: 0.8831 - val_loss: 0.5564 - val_accuracy: 0.8382\n",
      "Epoch 885/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2670 - accuracy: 0.8836 - val_loss: 0.5565 - val_accuracy: 0.8391\n",
      "Epoch 886/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2675 - accuracy: 0.8835 - val_loss: 0.5536 - val_accuracy: 0.8363\n",
      "Epoch 887/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2680 - accuracy: 0.8834 - val_loss: 0.5582 - val_accuracy: 0.8369\n",
      "Epoch 888/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2680 - accuracy: 0.8835 - val_loss: 0.5511 - val_accuracy: 0.8395\n",
      "Epoch 889/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8833 - val_loss: 0.5538 - val_accuracy: 0.8387\n",
      "Epoch 890/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2695 - accuracy: 0.8829 - val_loss: 0.5566 - val_accuracy: 0.8366\n",
      "Epoch 891/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2681 - accuracy: 0.8832 - val_loss: 0.5471 - val_accuracy: 0.8393\n",
      "Epoch 892/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2685 - accuracy: 0.8831 - val_loss: 0.5558 - val_accuracy: 0.8385\n",
      "Epoch 893/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8836 - val_loss: 0.5609 - val_accuracy: 0.8328\n",
      "Epoch 894/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2685 - accuracy: 0.8832 - val_loss: 0.5486 - val_accuracy: 0.8391\n",
      "Epoch 895/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8831 - val_loss: 0.5596 - val_accuracy: 0.8379\n",
      "Epoch 896/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2681 - accuracy: 0.8831 - val_loss: 0.5538 - val_accuracy: 0.8388\n",
      "Epoch 897/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2681 - accuracy: 0.8830 - val_loss: 0.5502 - val_accuracy: 0.8384\n",
      "Epoch 898/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2689 - accuracy: 0.8828 - val_loss: 0.5517 - val_accuracy: 0.8397\n",
      "Epoch 899/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2675 - accuracy: 0.8834 - val_loss: 0.5514 - val_accuracy: 0.8401\n",
      "Epoch 900/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2670 - accuracy: 0.8836 - val_loss: 0.5505 - val_accuracy: 0.8397\n",
      "Epoch 901/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2678 - accuracy: 0.8829 - val_loss: 0.5520 - val_accuracy: 0.8377\n",
      "Epoch 902/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2673 - accuracy: 0.8835 - val_loss: 0.5609 - val_accuracy: 0.8379\n",
      "Epoch 903/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2672 - accuracy: 0.8838 - val_loss: 0.5581 - val_accuracy: 0.8365\n",
      "Epoch 904/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2682 - accuracy: 0.8831 - val_loss: 0.5595 - val_accuracy: 0.8387\n",
      "Epoch 905/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8833 - val_loss: 0.5526 - val_accuracy: 0.8396\n",
      "Epoch 906/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2668 - accuracy: 0.8841 - val_loss: 0.5551 - val_accuracy: 0.8397\n",
      "Epoch 907/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2683 - accuracy: 0.8832 - val_loss: 0.5569 - val_accuracy: 0.8386\n",
      "Epoch 908/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2684 - accuracy: 0.8834 - val_loss: 0.5612 - val_accuracy: 0.8393\n",
      "Epoch 909/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2689 - accuracy: 0.8831 - val_loss: 0.5551 - val_accuracy: 0.8399\n",
      "Epoch 910/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2683 - accuracy: 0.8832 - val_loss: 0.5529 - val_accuracy: 0.8388\n",
      "Epoch 911/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2680 - accuracy: 0.8831 - val_loss: 0.5563 - val_accuracy: 0.8385\n",
      "Epoch 912/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2676 - accuracy: 0.8835 - val_loss: 0.5561 - val_accuracy: 0.8350\n",
      "Epoch 913/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2681 - accuracy: 0.8827 - val_loss: 0.5573 - val_accuracy: 0.8383\n",
      "Epoch 914/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2679 - accuracy: 0.8834 - val_loss: 0.5511 - val_accuracy: 0.8395\n",
      "Epoch 915/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2685 - accuracy: 0.8831 - val_loss: 0.5554 - val_accuracy: 0.8380\n",
      "Epoch 916/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2675 - accuracy: 0.8833 - val_loss: 0.5499 - val_accuracy: 0.8385\n",
      "Epoch 917/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2665 - accuracy: 0.8838 - val_loss: 0.5517 - val_accuracy: 0.8396\n",
      "Epoch 918/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2679 - accuracy: 0.8823 - val_loss: 0.5562 - val_accuracy: 0.8380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 919/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2673 - accuracy: 0.8834 - val_loss: 0.5715 - val_accuracy: 0.8330\n",
      "Epoch 920/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2665 - accuracy: 0.8837 - val_loss: 0.5480 - val_accuracy: 0.8386\n",
      "Epoch 921/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2675 - accuracy: 0.8836 - val_loss: 0.5557 - val_accuracy: 0.8366\n",
      "Epoch 922/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2675 - accuracy: 0.8835 - val_loss: 0.5557 - val_accuracy: 0.8387\n",
      "Epoch 923/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2670 - accuracy: 0.8834 - val_loss: 0.5561 - val_accuracy: 0.8393\n",
      "Epoch 924/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2668 - accuracy: 0.8837 - val_loss: 0.5534 - val_accuracy: 0.8387\n",
      "Epoch 925/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2662 - accuracy: 0.8843 - val_loss: 0.5626 - val_accuracy: 0.8322\n",
      "Epoch 926/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2679 - accuracy: 0.8830 - val_loss: 0.5658 - val_accuracy: 0.8362\n",
      "Epoch 927/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8833 - val_loss: 0.5589 - val_accuracy: 0.8390\n",
      "Epoch 928/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2668 - accuracy: 0.8841 - val_loss: 0.5506 - val_accuracy: 0.8370\n",
      "Epoch 929/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2669 - accuracy: 0.8835 - val_loss: 0.5641 - val_accuracy: 0.8380\n",
      "Epoch 930/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2685 - accuracy: 0.8828 - val_loss: 0.5560 - val_accuracy: 0.8378\n",
      "Epoch 931/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2672 - accuracy: 0.8841 - val_loss: 0.5633 - val_accuracy: 0.8376\n",
      "Epoch 932/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2669 - accuracy: 0.8839 - val_loss: 0.5608 - val_accuracy: 0.8370\n",
      "Epoch 933/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8834 - val_loss: 0.5596 - val_accuracy: 0.8385\n",
      "Epoch 934/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2681 - accuracy: 0.8832 - val_loss: 0.5590 - val_accuracy: 0.8380\n",
      "Epoch 935/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2672 - accuracy: 0.8834 - val_loss: 0.5596 - val_accuracy: 0.8380\n",
      "Epoch 936/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2667 - accuracy: 0.8837 - val_loss: 0.5610 - val_accuracy: 0.8388\n",
      "Epoch 937/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2666 - accuracy: 0.8839 - val_loss: 0.5636 - val_accuracy: 0.8389\n",
      "Epoch 938/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8833 - val_loss: 0.5537 - val_accuracy: 0.8390\n",
      "Epoch 939/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2662 - accuracy: 0.8838 - val_loss: 0.5624 - val_accuracy: 0.8378\n",
      "Epoch 940/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2672 - accuracy: 0.8838 - val_loss: 0.5619 - val_accuracy: 0.8376\n",
      "Epoch 941/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2669 - accuracy: 0.8841 - val_loss: 0.5625 - val_accuracy: 0.8378\n",
      "Epoch 942/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8833 - val_loss: 0.5656 - val_accuracy: 0.8328\n",
      "Epoch 943/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2680 - accuracy: 0.8833 - val_loss: 0.5694 - val_accuracy: 0.8385\n",
      "Epoch 944/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2668 - accuracy: 0.8836 - val_loss: 0.5563 - val_accuracy: 0.8396\n",
      "Epoch 945/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2665 - accuracy: 0.8842 - val_loss: 0.5572 - val_accuracy: 0.8387\n",
      "Epoch 946/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2655 - accuracy: 0.8848 - val_loss: 0.5523 - val_accuracy: 0.8384\n",
      "Epoch 947/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2674 - accuracy: 0.8832 - val_loss: 0.5558 - val_accuracy: 0.8364\n",
      "Epoch 948/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2664 - accuracy: 0.8840 - val_loss: 0.5513 - val_accuracy: 0.8382\n",
      "Epoch 949/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2664 - accuracy: 0.8840 - val_loss: 0.5555 - val_accuracy: 0.8394\n",
      "Epoch 950/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2656 - accuracy: 0.8842 - val_loss: 0.5543 - val_accuracy: 0.8390\n",
      "Epoch 951/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2676 - accuracy: 0.8838 - val_loss: 0.5569 - val_accuracy: 0.8399\n",
      "Epoch 952/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2686 - accuracy: 0.8831 - val_loss: 0.5698 - val_accuracy: 0.8372\n",
      "Epoch 953/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2674 - accuracy: 0.8836 - val_loss: 0.5618 - val_accuracy: 0.8363\n",
      "Epoch 954/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2671 - accuracy: 0.8836 - val_loss: 0.5552 - val_accuracy: 0.8387\n",
      "Epoch 955/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2662 - accuracy: 0.8841 - val_loss: 0.5556 - val_accuracy: 0.8385\n",
      "Epoch 956/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2672 - accuracy: 0.8834 - val_loss: 0.5692 - val_accuracy: 0.8364\n",
      "Epoch 957/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2675 - accuracy: 0.8835 - val_loss: 0.5635 - val_accuracy: 0.8379\n",
      "Epoch 958/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2671 - accuracy: 0.8834 - val_loss: 0.5810 - val_accuracy: 0.8327\n",
      "Epoch 959/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2666 - accuracy: 0.8836 - val_loss: 0.5549 - val_accuracy: 0.8387\n",
      "Epoch 960/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2665 - accuracy: 0.8837 - val_loss: 0.5567 - val_accuracy: 0.8401\n",
      "Epoch 961/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2664 - accuracy: 0.8843 - val_loss: 0.5576 - val_accuracy: 0.8384\n",
      "Epoch 962/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2666 - accuracy: 0.8837 - val_loss: 0.5553 - val_accuracy: 0.8385\n",
      "Epoch 963/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2669 - accuracy: 0.8837 - val_loss: 0.5584 - val_accuracy: 0.8387\n",
      "Epoch 964/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8832 - val_loss: 0.5647 - val_accuracy: 0.8345\n",
      "Epoch 965/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8832 - val_loss: 0.5601 - val_accuracy: 0.8395\n",
      "Epoch 966/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2676 - accuracy: 0.8840 - val_loss: 0.5561 - val_accuracy: 0.8378\n",
      "Epoch 967/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2655 - accuracy: 0.8845 - val_loss: 0.5638 - val_accuracy: 0.8365\n",
      "Epoch 968/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2654 - accuracy: 0.8845 - val_loss: 0.5634 - val_accuracy: 0.8369\n",
      "Epoch 969/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2663 - accuracy: 0.8840 - val_loss: 0.5549 - val_accuracy: 0.8378\n",
      "Epoch 970/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2655 - accuracy: 0.8845 - val_loss: 0.5628 - val_accuracy: 0.8382\n",
      "Epoch 971/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2657 - accuracy: 0.8840 - val_loss: 0.5609 - val_accuracy: 0.8352\n",
      "Epoch 972/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2666 - accuracy: 0.8839 - val_loss: 0.5607 - val_accuracy: 0.8387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 973/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2659 - accuracy: 0.8843 - val_loss: 0.5575 - val_accuracy: 0.8395\n",
      "Epoch 974/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2653 - accuracy: 0.8840 - val_loss: 0.5626 - val_accuracy: 0.8383\n",
      "Epoch 975/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2677 - accuracy: 0.8830 - val_loss: 0.5681 - val_accuracy: 0.8314\n",
      "Epoch 976/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2659 - accuracy: 0.8841 - val_loss: 0.5635 - val_accuracy: 0.8355\n",
      "Epoch 977/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2668 - accuracy: 0.8840 - val_loss: 0.5553 - val_accuracy: 0.8390\n",
      "Epoch 978/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2660 - accuracy: 0.8839 - val_loss: 0.5615 - val_accuracy: 0.8364\n",
      "Epoch 979/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2657 - accuracy: 0.8841 - val_loss: 0.5567 - val_accuracy: 0.8394\n",
      "Epoch 980/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2668 - accuracy: 0.8834 - val_loss: 0.5652 - val_accuracy: 0.8383\n",
      "Epoch 981/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2658 - accuracy: 0.8842 - val_loss: 0.5685 - val_accuracy: 0.8371\n",
      "Epoch 982/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2662 - accuracy: 0.8839 - val_loss: 0.5627 - val_accuracy: 0.8395\n",
      "Epoch 983/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2665 - accuracy: 0.8837 - val_loss: 0.5588 - val_accuracy: 0.8402\n",
      "Epoch 984/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2665 - accuracy: 0.8840 - val_loss: 0.5651 - val_accuracy: 0.8388\n",
      "Epoch 985/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2675 - accuracy: 0.8832 - val_loss: 0.5604 - val_accuracy: 0.8360\n",
      "Epoch 986/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2670 - accuracy: 0.8836 - val_loss: 0.5655 - val_accuracy: 0.8368\n",
      "Epoch 987/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2653 - accuracy: 0.8845 - val_loss: 0.5577 - val_accuracy: 0.8382\n",
      "Epoch 988/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2666 - accuracy: 0.8839 - val_loss: 0.5572 - val_accuracy: 0.8390\n",
      "Epoch 989/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2670 - accuracy: 0.8837 - val_loss: 0.5618 - val_accuracy: 0.8391\n",
      "Epoch 990/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2661 - accuracy: 0.8842 - val_loss: 0.5574 - val_accuracy: 0.8389\n",
      "Epoch 991/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2663 - accuracy: 0.8839 - val_loss: 0.5595 - val_accuracy: 0.8390\n",
      "Epoch 992/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2655 - accuracy: 0.8845 - val_loss: 0.5547 - val_accuracy: 0.8398\n",
      "Epoch 993/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2663 - accuracy: 0.8838 - val_loss: 0.5653 - val_accuracy: 0.8384\n",
      "Epoch 994/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2646 - accuracy: 0.8847 - val_loss: 0.5556 - val_accuracy: 0.8386\n",
      "Epoch 995/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2661 - accuracy: 0.8840 - val_loss: 0.5683 - val_accuracy: 0.8362\n",
      "Epoch 996/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2661 - accuracy: 0.8840 - val_loss: 0.5595 - val_accuracy: 0.8375\n",
      "Epoch 997/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2661 - accuracy: 0.8842 - val_loss: 0.5536 - val_accuracy: 0.8390\n",
      "Epoch 998/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2661 - accuracy: 0.8842 - val_loss: 0.5570 - val_accuracy: 0.8378\n",
      "Epoch 999/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2667 - accuracy: 0.8840 - val_loss: 0.5565 - val_accuracy: 0.8399\n",
      "Epoch 1000/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2652 - accuracy: 0.8843 - val_loss: 0.5558 - val_accuracy: 0.8388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f88b09df88>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = \"adagrad\", loss = \"binary_crossentropy\", metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 1000, callbacks=callbacks, batch_size = 4096, validation_split= 0.15, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.3902724e-06, 2.2750646e-06, 3.8516291e-06, ..., 7.5785232e-01,\n",
       "       7.3724592e-01, 9.9930811e-01], dtype=float32)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final = np.array([i > 0.5 for i in predictions.flatten()], dtype = np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.95      0.81     16791\n",
      "           1       0.96      0.77      0.85     28209\n",
      "\n",
      "    accuracy                           0.84     45000\n",
      "   macro avg       0.84      0.86      0.83     45000\n",
      "weighted avg       0.87      0.84      0.84     45000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions_final, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
