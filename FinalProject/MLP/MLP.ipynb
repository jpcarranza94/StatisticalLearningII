{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron for classification\n",
    "\n",
    "## Health Insurance Cross Sell Prediction \n",
    "\n",
    "\n",
    "For this final project I am going to implement a classical feed-forward neural network to classify if customers who currently own a car insurance are interested in acquiring a health insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40454.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38294.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>28619.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>27496.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n",
       "0   1    Male   44                1         28.0                   0   \n",
       "1   2    Male   76                1          3.0                   0   \n",
       "2   3    Male   47                1         28.0                   0   \n",
       "3   4    Male   21                1         11.0                   1   \n",
       "4   5  Female   29                1         41.0                   1   \n",
       "\n",
       "  Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Vintage  \\\n",
       "0   > 2 Years            Yes         40454.0                  26.0      217   \n",
       "1    1-2 Year             No         33536.0                  26.0      183   \n",
       "2   > 2 Years            Yes         38294.0                  26.0       27   \n",
       "3    < 1 Year             No         28619.0                 152.0      203   \n",
       "4    < 1 Year             No         27496.0                 152.0       39   \n",
       "\n",
       "   Response  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>190555.000000</td>\n",
       "      <td>38.822584</td>\n",
       "      <td>0.997869</td>\n",
       "      <td>26.388807</td>\n",
       "      <td>0.458210</td>\n",
       "      <td>30564.389581</td>\n",
       "      <td>112.034295</td>\n",
       "      <td>154.347397</td>\n",
       "      <td>0.122563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>110016.836208</td>\n",
       "      <td>15.511611</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>13.229888</td>\n",
       "      <td>0.498251</td>\n",
       "      <td>17213.155057</td>\n",
       "      <td>54.203995</td>\n",
       "      <td>83.671304</td>\n",
       "      <td>0.327936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2630.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>95278.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24405.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>190555.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31669.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>285832.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39400.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>381109.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>540165.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id            Age  Driving_License    Region_Code  \\\n",
       "count  381109.000000  381109.000000    381109.000000  381109.000000   \n",
       "mean   190555.000000      38.822584         0.997869      26.388807   \n",
       "std    110016.836208      15.511611         0.046110      13.229888   \n",
       "min         1.000000      20.000000         0.000000       0.000000   \n",
       "25%     95278.000000      25.000000         1.000000      15.000000   \n",
       "50%    190555.000000      36.000000         1.000000      28.000000   \n",
       "75%    285832.000000      49.000000         1.000000      35.000000   \n",
       "max    381109.000000      85.000000         1.000000      52.000000   \n",
       "\n",
       "       Previously_Insured  Annual_Premium  Policy_Sales_Channel  \\\n",
       "count       381109.000000   381109.000000         381109.000000   \n",
       "mean             0.458210    30564.389581            112.034295   \n",
       "std              0.498251    17213.155057             54.203995   \n",
       "min              0.000000     2630.000000              1.000000   \n",
       "25%              0.000000    24405.000000             29.000000   \n",
       "50%              0.000000    31669.000000            133.000000   \n",
       "75%              1.000000    39400.000000            152.000000   \n",
       "max              1.000000   540165.000000            163.000000   \n",
       "\n",
       "             Vintage       Response  \n",
       "count  381109.000000  381109.000000  \n",
       "mean      154.347397       0.122563  \n",
       "std        83.671304       0.327936  \n",
       "min        10.000000       0.000000  \n",
       "25%        82.000000       0.000000  \n",
       "50%       154.000000       0.000000  \n",
       "75%       227.000000       0.000000  \n",
       "max       299.000000       1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the dataset contains a total of 381109 records, which is quite a good amount to train an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain various categorical variables with name different categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0    106415\n",
       "8.0      33877\n",
       "46.0     19749\n",
       "41.0     18263\n",
       "15.0     13308\n",
       "30.0     12191\n",
       "29.0     11065\n",
       "50.0     10243\n",
       "3.0       9251\n",
       "11.0      9232\n",
       "36.0      8797\n",
       "33.0      7654\n",
       "47.0      7436\n",
       "35.0      6942\n",
       "6.0       6280\n",
       "45.0      5605\n",
       "37.0      5501\n",
       "18.0      5153\n",
       "48.0      4681\n",
       "14.0      4678\n",
       "39.0      4644\n",
       "10.0      4374\n",
       "21.0      4266\n",
       "2.0       4038\n",
       "13.0      4036\n",
       "7.0       3279\n",
       "12.0      3198\n",
       "9.0       3101\n",
       "27.0      2823\n",
       "32.0      2787\n",
       "43.0      2639\n",
       "17.0      2617\n",
       "26.0      2587\n",
       "25.0      2503\n",
       "24.0      2415\n",
       "38.0      2026\n",
       "0.0       2021\n",
       "16.0      2007\n",
       "31.0      1960\n",
       "23.0      1960\n",
       "20.0      1935\n",
       "49.0      1832\n",
       "4.0       1801\n",
       "34.0      1664\n",
       "19.0      1535\n",
       "22.0      1309\n",
       "40.0      1295\n",
       "5.0       1279\n",
       "1.0       1008\n",
       "44.0       808\n",
       "42.0       591\n",
       "52.0       267\n",
       "51.0       183\n",
       "Name: Region_Code, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Region_Code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26., 152., 160., 124.,  14.,  13.,  30., 156., 163., 157., 122.,\n",
       "        19.,  22.,  15., 154.,  16.,  52., 155.,  11., 151., 125.,  25.,\n",
       "        61.,   1.,  86.,  31., 150.,  23.,  60.,  21., 121.,   3., 139.,\n",
       "        12.,  29.,  55.,   7.,  47., 127., 153.,  78., 158.,  89.,  32.,\n",
       "         8.,  10., 120.,  65.,   4.,  42.,  83., 136.,  24.,  18.,  56.,\n",
       "        48., 106.,  54.,  93., 116.,  91.,  45.,   9., 145., 147.,  44.,\n",
       "       109.,  37., 140., 107., 128., 131., 114., 118., 159., 119., 105.,\n",
       "       135.,  62., 138., 129.,  88.,  92., 111., 113.,  73.,  36.,  28.,\n",
       "        35.,  59.,  53., 148., 133., 108.,  64.,  39.,  94., 132.,  46.,\n",
       "        81., 103.,  90.,  51.,  27., 146.,  63.,  96.,  40.,  66., 100.,\n",
       "        95., 123.,  98.,  75.,  69., 130., 134.,  49.,  97.,  38.,  17.,\n",
       "       110.,  80.,  71., 117.,  58.,  20.,  76., 104.,  87.,  84., 137.,\n",
       "       126.,  68.,  67., 101., 115.,  57.,  82.,  79., 112.,  99.,  70.,\n",
       "         2.,  34.,  33.,  74., 102., 149.,  43.,   6.,  50., 144., 143.,\n",
       "        41.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Policy_Sales_Channel.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-2 Year     200316\n",
       "< 1 Year     164786\n",
       "> 2 Years     16007\n",
       "Name: Vehicle_Age, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Vehicle_Age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    334399\n",
       "1     46710\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Response.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a very important result which shows that the dataset in question has very unbalanced classes for prediction, which means that further processing will be needed to create a robust classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    380297\n",
       "0       812\n",
       "Name: Driving_License, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Driving_License.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    206481\n",
       "1    174628\n",
       "Name: Previously_Insured, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Previously_Insured.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((train_df.Vintage))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I apply a log transformation to make the data distribution more gaussian\n",
    "plt.hist(np.log(train_df.Age))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#I apply a log transformation to make the data distribution more gaussian\n",
    "\n",
    "plt.hist(np.log(train_df.Annual_Premium))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40454.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38294.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>28619.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>27496.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381104</th>\n",
       "      <td>381105</td>\n",
       "      <td>Male</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>30170.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381105</th>\n",
       "      <td>381106</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>40016.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381106</th>\n",
       "      <td>381107</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>35118.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381107</th>\n",
       "      <td>381108</td>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>44617.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381108</th>\n",
       "      <td>381109</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>41777.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381109 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n",
       "0            1    Male   44                1         28.0                   0   \n",
       "1            2    Male   76                1          3.0                   0   \n",
       "2            3    Male   47                1         28.0                   0   \n",
       "3            4    Male   21                1         11.0                   1   \n",
       "4            5  Female   29                1         41.0                   1   \n",
       "...        ...     ...  ...              ...          ...                 ...   \n",
       "381104  381105    Male   74                1         26.0                   1   \n",
       "381105  381106    Male   30                1         37.0                   1   \n",
       "381106  381107    Male   21                1         30.0                   1   \n",
       "381107  381108  Female   68                1         14.0                   0   \n",
       "381108  381109    Male   46                1         29.0                   0   \n",
       "\n",
       "       Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  \\\n",
       "0        > 2 Years            Yes         40454.0                  26.0   \n",
       "1         1-2 Year             No         33536.0                  26.0   \n",
       "2        > 2 Years            Yes         38294.0                  26.0   \n",
       "3         < 1 Year             No         28619.0                 152.0   \n",
       "4         < 1 Year             No         27496.0                 152.0   \n",
       "...            ...            ...             ...                   ...   \n",
       "381104    1-2 Year             No         30170.0                  26.0   \n",
       "381105    < 1 Year             No         40016.0                 152.0   \n",
       "381106    < 1 Year             No         35118.0                 160.0   \n",
       "381107   > 2 Years            Yes         44617.0                 124.0   \n",
       "381108    1-2 Year             No         41777.0                  26.0   \n",
       "\n",
       "        Vintage  Response  \n",
       "0           217         1  \n",
       "1           183         0  \n",
       "2            27         1  \n",
       "3           203         0  \n",
       "4            39         0  \n",
       "...         ...       ...  \n",
       "381104       88         0  \n",
       "381105      131         0  \n",
       "381106      161         0  \n",
       "381107       74         0  \n",
       "381108      237         0  \n",
       "\n",
       "[381109 rows x 12 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         44\n",
       "1         76\n",
       "2         47\n",
       "3         21\n",
       "4         29\n",
       "          ..\n",
       "381104    74\n",
       "381105    30\n",
       "381106    21\n",
       "381107    68\n",
       "381108    46\n",
       "Name: Age, Length: 381109, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation of variables to one hot vectors\n",
    "vehicle_age_onehot = onehot.fit_transform(np.array(train_df.Vehicle_Age).reshape(-1,1))\n",
    "region_code_onehot = onehot.fit_transform(np.array(train_df.Region_Code).reshape(-1,1))\n",
    "policy_sales_onehot = onehot.fit_transform(np.array(train_df.Policy_Sales_Channel).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_onehot = np.array([i=='Male' for i in train_df.Gender ],dtype=np.int).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vehicle_Damage_onehot = np.array([i=='Yes' for i in train_df.Vehicle_Damage],dtype=np.int).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale of values to 0-1\n",
    "age_scaled = minmax.fit_transform(np.array(train_df.Age).reshape(-1,1))\n",
    "annual_premium_scaled = minmax.fit_transform(np.log(np.array(train_df.Annual_Premium)).reshape(-1,1))\n",
    "vintage_scaled = minmax.fit_transform(np.array(train_df.Vintage).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations necessary to create final dataframe\n",
    "\n",
    "driving_license = np.array(train_df.Driving_License).reshape(-1,1)\n",
    "previously_insured = np.array(train_df.Previously_Insured).reshape(-1,1)\n",
    "y_train = np.array(train_df.Response).reshape(-1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = np.concatenate((vehicle_age_onehot, region_code_onehot, policy_sales_onehot, Gender_onehot, Vehicle_Damage_onehot, age_scaled, annual_premium_scaled, vintage_scaled, driving_license, previously_insured, y_train),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalancing of prediction classes\n",
    "\n",
    "The approach we are going to take to create a dataset that will hopefully make the classifier more robust towards the classification of the positive class is to upsample (sample with replacement) the class which has less data points. On the contrary, to balance this out we will downsample (sample without replacement) the class which has more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_1 = clean_df[clean_df[:,218]==1]\n",
    "class_0 = clean_df[clean_df[:,218]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = np.array(pd.DataFrame(class_1).sample(n =  150000, replace = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = np.array(pd.DataFrame(class_0).sample(n =  150000, replace = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframe is created\n",
    "final_df = np.concatenate((class_1, class_0), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if numbers correspond to the correct amount of classes\n",
    "final_df[:,218].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_df[:,:218], final_df[:,218], test_size = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP architecture using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation = \"relu\", input_shape = (1,218)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 1, 512)            112128    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1, 1024)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1, 256)            131328    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1, 64)             16448     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1, 32)             2080      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1, 1)              33        \n",
      "=================================================================\n",
      "Total params: 1,312,129\n",
      "Trainable params: 1,312,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind this first architecture to be tested is to create a model with a moderate amount of trainable parameters, and check whether this has proper learning or if it will lead to overfitting of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adagrad\", loss = \"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad is used because it is more useful for very sparse features, which is the case of the one hot encoded categorical variables in our dataset. This is the case because it gives more importance (in terms of the update to weights) to less frequent features inside the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train,axis = 2)\n",
    "X_test = np.expand_dims(X_test, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1,218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1,1,218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229500 samples, validate on 25500 samples\n",
      "Epoch 1/50\n",
      "229500/229500 [==============================] - 23s 100us/step - loss: 0.4329 - accuracy: 0.7977 - val_loss: 0.4208 - val_accuracy: 0.8025\n",
      "Epoch 2/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4234 - accuracy: 0.8006 - val_loss: 0.4184 - val_accuracy: 0.8030\n",
      "Epoch 3/50\n",
      "229500/229500 [==============================] - 22s 97us/step - loss: 0.4196 - accuracy: 0.8023 - val_loss: 0.4153 - val_accuracy: 0.8026\n",
      "Epoch 4/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4166 - accuracy: 0.8041 - val_loss: 0.4135 - val_accuracy: 0.8056\n",
      "Epoch 5/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.4137 - accuracy: 0.8059 - val_loss: 0.4124 - val_accuracy: 0.8045\n",
      "Epoch 6/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4115 - accuracy: 0.8067 - val_loss: 0.4127 - val_accuracy: 0.8056\n",
      "Epoch 7/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4093 - accuracy: 0.8073 - val_loss: 0.4110 - val_accuracy: 0.8057\n",
      "Epoch 8/50\n",
      "229500/229500 [==============================] - 23s 101us/step - loss: 0.4077 - accuracy: 0.8085 - val_loss: 0.4102 - val_accuracy: 0.8064\n",
      "Epoch 9/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4052 - accuracy: 0.8093 - val_loss: 0.4094 - val_accuracy: 0.8077\n",
      "Epoch 10/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4039 - accuracy: 0.8104 - val_loss: 0.4081 - val_accuracy: 0.8078\n",
      "Epoch 11/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.4014 - accuracy: 0.8121 - val_loss: 0.4069 - val_accuracy: 0.8090\n",
      "Epoch 12/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.4004 - accuracy: 0.8119 - val_loss: 0.4068 - val_accuracy: 0.8087\n",
      "Epoch 13/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3989 - accuracy: 0.8131 - val_loss: 0.4067 - val_accuracy: 0.8095\n",
      "Epoch 14/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.3969 - accuracy: 0.8136 - val_loss: 0.4056 - val_accuracy: 0.8096\n",
      "Epoch 15/50\n",
      "229500/229500 [==============================] - 22s 98us/step - loss: 0.3957 - accuracy: 0.8148 - val_loss: 0.4052 - val_accuracy: 0.8101\n",
      "Epoch 16/50\n",
      "229500/229500 [==============================] - 23s 101us/step - loss: 0.3943 - accuracy: 0.8156 - val_loss: 0.4053 - val_accuracy: 0.8107\n",
      "Epoch 17/50\n",
      "229500/229500 [==============================] - 22s 97us/step - loss: 0.3926 - accuracy: 0.8160 - val_loss: 0.4035 - val_accuracy: 0.8109\n",
      "Epoch 18/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3914 - accuracy: 0.8165 - val_loss: 0.4036 - val_accuracy: 0.8109\n",
      "Epoch 19/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3904 - accuracy: 0.8173 - val_loss: 0.4045 - val_accuracy: 0.8110\n",
      "Epoch 20/50\n",
      "229500/229500 [==============================] - 22s 96us/step - loss: 0.3894 - accuracy: 0.8183 - val_loss: 0.4033 - val_accuracy: 0.8122\n",
      "Epoch 21/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3882 - accuracy: 0.8188 - val_loss: 0.4030 - val_accuracy: 0.8107\n",
      "Epoch 22/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3869 - accuracy: 0.8197 - val_loss: 0.4022 - val_accuracy: 0.8132\n",
      "Epoch 23/50\n",
      "229500/229500 [==============================] - 22s 95us/step - loss: 0.3857 - accuracy: 0.8203 - val_loss: 0.4023 - val_accuracy: 0.8139\n",
      "Epoch 24/50\n",
      "229500/229500 [==============================] - 22s 94us/step - loss: 0.3852 - accuracy: 0.8207 - val_loss: 0.4017 - val_accuracy: 0.8142\n",
      "Epoch 25/50\n",
      "145312/229500 [=================>............] - ETA: 7s - loss: 0.3836 - accuracy: 0.8207"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-246-fd271a585c4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 50, callbacks=callbacks, batch_size = 32, validation_split= 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is stopped to modify strcuture as learning is ocurring quite slow, this is why we add batch normalization in the upper two layers. The structure is modified to create an inverted triangle of neurons per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation = \"relu\", input_shape = (1,218)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 1, 1024)           224256    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1, 1024)           1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1, 256)            131328    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1, 128)            32896     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1, 64)             8256      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1, 1)              65        \n",
      "=================================================================\n",
      "Total params: 1,979,393\n",
      "Trainable params: 1,975,297\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = adagrad , loss = \"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229500 samples, validate on 25500 samples\n",
      "Epoch 1/50\n",
      "229500/229500 [==============================] - 14s 60us/step - loss: 0.4327 - accuracy: 0.7950 - val_loss: 0.4255 - val_accuracy: 0.7982\n",
      "Epoch 2/50\n",
      "  2880/229500 [..............................] - ETA: 12s - loss: 0.4406 - accuracy: 0.7878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpcar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4283 - accuracy: 0.7973 - val_loss: 0.4208 - val_accuracy: 0.8006\n",
      "Epoch 3/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4253 - accuracy: 0.7982 - val_loss: 0.4178 - val_accuracy: 0.8030\n",
      "Epoch 4/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4237 - accuracy: 0.7996 - val_loss: 0.4206 - val_accuracy: 0.7998\n",
      "Epoch 5/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4223 - accuracy: 0.7995 - val_loss: 0.4155 - val_accuracy: 0.8038\n",
      "Epoch 6/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4208 - accuracy: 0.8004 - val_loss: 0.4158 - val_accuracy: 0.8025\n",
      "Epoch 7/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4202 - accuracy: 0.8008 - val_loss: 0.4162 - val_accuracy: 0.8016\n",
      "Epoch 8/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4185 - accuracy: 0.8012 - val_loss: 0.4162 - val_accuracy: 0.8028\n",
      "Epoch 9/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4174 - accuracy: 0.8020 - val_loss: 0.4145 - val_accuracy: 0.8050\n",
      "Epoch 10/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4165 - accuracy: 0.8023 - val_loss: 0.4126 - val_accuracy: 0.8055\n",
      "Epoch 11/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4157 - accuracy: 0.8025 - val_loss: 0.4138 - val_accuracy: 0.8052\n",
      "Epoch 12/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4146 - accuracy: 0.8036 - val_loss: 0.4148 - val_accuracy: 0.8029\n",
      "Epoch 13/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4137 - accuracy: 0.8039 - val_loss: 0.4124 - val_accuracy: 0.8045\n",
      "Epoch 14/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4127 - accuracy: 0.8049 - val_loss: 0.4121 - val_accuracy: 0.8052\n",
      "Epoch 15/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4119 - accuracy: 0.8053 - val_loss: 0.4123 - val_accuracy: 0.8054\n",
      "Epoch 16/50\n",
      "229500/229500 [==============================] - 13s 59us/step - loss: 0.4110 - accuracy: 0.8061 - val_loss: 0.4101 - val_accuracy: 0.8077\n",
      "Epoch 17/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4101 - accuracy: 0.8061 - val_loss: 0.4106 - val_accuracy: 0.8054\n",
      "Epoch 18/50\n",
      "229500/229500 [==============================] - 13s 59us/step - loss: 0.4091 - accuracy: 0.8069 - val_loss: 0.4098 - val_accuracy: 0.8070\n",
      "Epoch 19/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4083 - accuracy: 0.8072 - val_loss: 0.4095 - val_accuracy: 0.8078\n",
      "Epoch 20/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4070 - accuracy: 0.8079 - val_loss: 0.4071 - val_accuracy: 0.8098\n",
      "Epoch 21/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4058 - accuracy: 0.8080 - val_loss: 0.4090 - val_accuracy: 0.8078\n",
      "Epoch 22/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4047 - accuracy: 0.8091 - val_loss: 0.4070 - val_accuracy: 0.8096\n",
      "Epoch 23/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4039 - accuracy: 0.8093 - val_loss: 0.4084 - val_accuracy: 0.8070\n",
      "Epoch 24/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4031 - accuracy: 0.8098 - val_loss: 0.4069 - val_accuracy: 0.8080\n",
      "Epoch 25/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4021 - accuracy: 0.8102 - val_loss: 0.4061 - val_accuracy: 0.8100\n",
      "Epoch 26/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.4007 - accuracy: 0.8112 - val_loss: 0.4057 - val_accuracy: 0.8085\n",
      "Epoch 27/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.4000 - accuracy: 0.8110 - val_loss: 0.4052 - val_accuracy: 0.8095\n",
      "Epoch 28/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3998 - accuracy: 0.8113 - val_loss: 0.4077 - val_accuracy: 0.8096\n",
      "Epoch 29/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3990 - accuracy: 0.8116 - val_loss: 0.4060 - val_accuracy: 0.8107\n",
      "Epoch 30/50\n",
      "229500/229500 [==============================] - 13s 59us/step - loss: 0.3979 - accuracy: 0.8123 - val_loss: 0.4060 - val_accuracy: 0.8091\n",
      "Epoch 31/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3967 - accuracy: 0.8128 - val_loss: 0.4047 - val_accuracy: 0.8109\n",
      "Epoch 32/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3956 - accuracy: 0.8135 - val_loss: 0.4047 - val_accuracy: 0.8108\n",
      "Epoch 33/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3951 - accuracy: 0.8141 - val_loss: 0.4054 - val_accuracy: 0.8108\n",
      "Epoch 34/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3946 - accuracy: 0.8143 - val_loss: 0.4044 - val_accuracy: 0.8116\n",
      "Epoch 35/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3929 - accuracy: 0.8149 - val_loss: 0.4040 - val_accuracy: 0.8110\n",
      "Epoch 36/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3923 - accuracy: 0.8155 - val_loss: 0.4036 - val_accuracy: 0.8122\n",
      "Epoch 37/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3914 - accuracy: 0.8158 - val_loss: 0.4047 - val_accuracy: 0.8118\n",
      "Epoch 38/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.3901 - accuracy: 0.8159 - val_loss: 0.4040 - val_accuracy: 0.8127\n",
      "Epoch 39/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3899 - accuracy: 0.8166 - val_loss: 0.4046 - val_accuracy: 0.8115\n",
      "Epoch 40/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3890 - accuracy: 0.8173 - val_loss: 0.4046 - val_accuracy: 0.8136\n",
      "Epoch 41/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3883 - accuracy: 0.8177 - val_loss: 0.4035 - val_accuracy: 0.8136\n",
      "Epoch 42/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3868 - accuracy: 0.8188 - val_loss: 0.4041 - val_accuracy: 0.8131\n",
      "Epoch 43/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3867 - accuracy: 0.8182 - val_loss: 0.4033 - val_accuracy: 0.8139\n",
      "Epoch 44/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3856 - accuracy: 0.8184 - val_loss: 0.4034 - val_accuracy: 0.8134\n",
      "Epoch 45/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3840 - accuracy: 0.8197 - val_loss: 0.4044 - val_accuracy: 0.8120\n",
      "Epoch 46/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3842 - accuracy: 0.8195 - val_loss: 0.4040 - val_accuracy: 0.8141\n",
      "Epoch 47/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3830 - accuracy: 0.8204 - val_loss: 0.4018 - val_accuracy: 0.8148\n",
      "Epoch 48/50\n",
      "229500/229500 [==============================] - 13s 57us/step - loss: 0.3821 - accuracy: 0.8206 - val_loss: 0.4040 - val_accuracy: 0.8144\n",
      "Epoch 49/50\n",
      "229500/229500 [==============================] - 13s 58us/step - loss: 0.3811 - accuracy: 0.8217 - val_loss: 0.4022 - val_accuracy: 0.8154\n",
      "Epoch 50/50\n",
      "229500/229500 [==============================] - 13s 59us/step - loss: 0.3805 - accuracy: 0.8214 - val_loss: 0.4037 - val_accuracy: 0.8160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f4c6fd3b48>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 50, callbacks=callbacks, batch_size = 64, validation_split= 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model stops training because of the lack of improvement in val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229500 samples, validate on 25500 samples\n",
      "Epoch 1/50\n",
      "229500/229500 [==============================] - 4s 16us/step - loss: 0.3762 - accuracy: 0.8238 - val_loss: 0.4029 - val_accuracy: 0.8164\n",
      "Epoch 2/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3758 - accuracy: 0.8250 - val_loss: 0.4051 - val_accuracy: 0.8173\n",
      "Epoch 3/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3741 - accuracy: 0.8249 - val_loss: 0.4014 - val_accuracy: 0.8178\n",
      "Epoch 4/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3746 - accuracy: 0.8255 - val_loss: 0.3982 - val_accuracy: 0.8187\n",
      "Epoch 5/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3723 - accuracy: 0.8255 - val_loss: 0.3985 - val_accuracy: 0.8181\n",
      "Epoch 6/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3715 - accuracy: 0.8263 - val_loss: 0.4001 - val_accuracy: 0.8174\n",
      "Epoch 7/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3711 - accuracy: 0.8266 - val_loss: 0.4017 - val_accuracy: 0.8182\n",
      "Epoch 8/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3709 - accuracy: 0.8265 - val_loss: 0.4040 - val_accuracy: 0.8177\n",
      "Epoch 9/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3699 - accuracy: 0.8266 - val_loss: 0.4004 - val_accuracy: 0.8199\n",
      "Epoch 10/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3692 - accuracy: 0.8275 - val_loss: 0.4055 - val_accuracy: 0.8200\n",
      "Epoch 11/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3697 - accuracy: 0.8278 - val_loss: 0.4095 - val_accuracy: 0.8185\n",
      "Epoch 12/50\n",
      "229500/229500 [==============================] - 3s 13us/step - loss: 0.3690 - accuracy: 0.8279 - val_loss: 0.4006 - val_accuracy: 0.8190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f4c9d94888>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 50, callbacks=callbacks, batch_size = 512, validation_split= 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification of batch size and optimizer to ADAM to see if this helps in stabilizing training and leading to better results in terms of val_loss, but results end up the same. This may be caused by overfitting to training data, due to the large number of trainable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation = \"relu\", input_shape = (1,218), activity_regularizer= tf.keras.regularizers.l1(0.0000025), bias_regularizer = tf.keras.regularizers.l1(0.0000025)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation = \"relu\", activity_regularizer= tf.keras.regularizers.l1(0.0000025), bias_regularizer= tf.keras.regularizers.l1(0.0000025)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1, 1024)           224256    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 512)            2048      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1, 256)            131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1, 256)            1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1, 128)            32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1, 128)            512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1, 64)             8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 64)             256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1, 32)             2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1, 32)             128       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1, 16)             528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1, 16)             64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1, 1)              17        \n",
      "=================================================================\n",
      "Total params: 932,289\n",
      "Trainable params: 928,225\n",
      "Non-trainable params: 4,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler model is created, which batch normalization along all its layers, top layer is removed and the decrease of layer size is continued all the way up to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is very sparse and the number of features is limited, a greater batch_size along with a greater number of epochs are used to train the final model. The main reason is to stabilize training which will lead to the desired 0.85 accuracy in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216750 samples, validate on 38250 samples\n",
      "Epoch 1/1000\n",
      "216750/216750 [==============================] - 3s 16us/step - loss: 2.8260 - accuracy: 0.6348 - val_loss: 1.2190 - val_accuracy: 0.4958\n",
      "Epoch 2/1000\n",
      " 28672/216750 [==>...........................] - ETA: 0s - loss: 2.6347 - accuracy: 0.6902"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpcar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216750/216750 [==============================] - 1s 5us/step - loss: 2.5520 - accuracy: 0.6999 - val_loss: 1.1918 - val_accuracy: 0.4958\n",
      "Epoch 3/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 2.3915 - accuracy: 0.7191 - val_loss: 1.1868 - val_accuracy: 0.4958\n",
      "Epoch 4/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 2.2690 - accuracy: 0.7295 - val_loss: 1.1945 - val_accuracy: 0.4958\n",
      "Epoch 5/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 2.1625 - accuracy: 0.7365 - val_loss: 1.2087 - val_accuracy: 0.4980\n",
      "Epoch 6/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 2.0715 - accuracy: 0.7402 - val_loss: 1.2303 - val_accuracy: 0.5259\n",
      "Epoch 7/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.9901 - accuracy: 0.7446 - val_loss: 1.2565 - val_accuracy: 0.5532\n",
      "Epoch 8/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.9204 - accuracy: 0.7482 - val_loss: 1.2857 - val_accuracy: 0.5910\n",
      "Epoch 9/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.8554 - accuracy: 0.7505 - val_loss: 1.3172 - val_accuracy: 0.6472\n",
      "Epoch 10/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.7982 - accuracy: 0.7524 - val_loss: 1.3444 - val_accuracy: 0.6840\n",
      "Epoch 11/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.7433 - accuracy: 0.7552 - val_loss: 1.3656 - val_accuracy: 0.7230\n",
      "Epoch 12/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.6946 - accuracy: 0.7568 - val_loss: 1.3846 - val_accuracy: 0.7527\n",
      "Epoch 13/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.6510 - accuracy: 0.7580 - val_loss: 1.3978 - val_accuracy: 0.7703\n",
      "Epoch 14/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.6100 - accuracy: 0.7587 - val_loss: 1.4046 - val_accuracy: 0.7786\n",
      "Epoch 15/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.5713 - accuracy: 0.7614 - val_loss: 1.4018 - val_accuracy: 0.7833\n",
      "Epoch 16/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.5342 - accuracy: 0.7627 - val_loss: 1.3916 - val_accuracy: 0.7851\n",
      "Epoch 17/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.5013 - accuracy: 0.7629 - val_loss: 1.3747 - val_accuracy: 0.7871\n",
      "Epoch 18/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.4680 - accuracy: 0.7646 - val_loss: 1.3560 - val_accuracy: 0.7868\n",
      "Epoch 19/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.4367 - accuracy: 0.7662 - val_loss: 1.3341 - val_accuracy: 0.7882\n",
      "Epoch 20/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.4073 - accuracy: 0.7669 - val_loss: 1.3110 - val_accuracy: 0.7894\n",
      "Epoch 21/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.3818 - accuracy: 0.7673 - val_loss: 1.2888 - val_accuracy: 0.7899\n",
      "Epoch 22/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.3537 - accuracy: 0.7696 - val_loss: 1.2658 - val_accuracy: 0.7901\n",
      "Epoch 23/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.3297 - accuracy: 0.7688 - val_loss: 1.2454 - val_accuracy: 0.7894\n",
      "Epoch 24/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.3059 - accuracy: 0.7695 - val_loss: 1.2249 - val_accuracy: 0.7906\n",
      "Epoch 25/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.2828 - accuracy: 0.7709 - val_loss: 1.2036 - val_accuracy: 0.7911\n",
      "Epoch 26/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.2613 - accuracy: 0.7710 - val_loss: 1.1837 - val_accuracy: 0.7920\n",
      "Epoch 27/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.2398 - accuracy: 0.7726 - val_loss: 1.1652 - val_accuracy: 0.7917\n",
      "Epoch 28/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.2197 - accuracy: 0.7732 - val_loss: 1.1474 - val_accuracy: 0.7922\n",
      "Epoch 29/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.2000 - accuracy: 0.7743 - val_loss: 1.1292 - val_accuracy: 0.7930\n",
      "Epoch 30/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.1803 - accuracy: 0.7738 - val_loss: 1.1121 - val_accuracy: 0.7928\n",
      "Epoch 31/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.1636 - accuracy: 0.7745 - val_loss: 1.0965 - val_accuracy: 0.7927\n",
      "Epoch 32/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.1462 - accuracy: 0.7749 - val_loss: 1.0803 - val_accuracy: 0.7931\n",
      "Epoch 33/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.1308 - accuracy: 0.7762 - val_loss: 1.0665 - val_accuracy: 0.7935\n",
      "Epoch 34/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.1132 - accuracy: 0.7768 - val_loss: 1.0503 - val_accuracy: 0.7935\n",
      "Epoch 35/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.0974 - accuracy: 0.7767 - val_loss: 1.0349 - val_accuracy: 0.7938\n",
      "Epoch 36/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.0806 - accuracy: 0.7786 - val_loss: 1.0205 - val_accuracy: 0.7937\n",
      "Epoch 37/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.0656 - accuracy: 0.7796 - val_loss: 1.0077 - val_accuracy: 0.7940\n",
      "Epoch 38/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.0492 - accuracy: 0.7792 - val_loss: 0.9946 - val_accuracy: 0.7951\n",
      "Epoch 39/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.0354 - accuracy: 0.7809 - val_loss: 0.9805 - val_accuracy: 0.7950\n",
      "Epoch 40/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.0230 - accuracy: 0.7797 - val_loss: 0.9685 - val_accuracy: 0.7953\n",
      "Epoch 41/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 1.0081 - accuracy: 0.7806 - val_loss: 0.9557 - val_accuracy: 0.7964\n",
      "Epoch 42/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9956 - accuracy: 0.7810 - val_loss: 0.9441 - val_accuracy: 0.7962\n",
      "Epoch 43/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9827 - accuracy: 0.7816 - val_loss: 0.9327 - val_accuracy: 0.7958\n",
      "Epoch 44/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9705 - accuracy: 0.7825 - val_loss: 0.9210 - val_accuracy: 0.7970\n",
      "Epoch 45/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9574 - accuracy: 0.7830 - val_loss: 0.9101 - val_accuracy: 0.7970\n",
      "Epoch 46/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9458 - accuracy: 0.7826 - val_loss: 0.8994 - val_accuracy: 0.7961\n",
      "Epoch 47/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9336 - accuracy: 0.7834 - val_loss: 0.8886 - val_accuracy: 0.7967\n",
      "Epoch 48/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9231 - accuracy: 0.7841 - val_loss: 0.8787 - val_accuracy: 0.7967\n",
      "Epoch 49/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9126 - accuracy: 0.7850 - val_loss: 0.8685 - val_accuracy: 0.7973\n",
      "Epoch 50/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.9015 - accuracy: 0.7857 - val_loss: 0.8596 - val_accuracy: 0.7976\n",
      "Epoch 51/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8909 - accuracy: 0.7858 - val_loss: 0.8505 - val_accuracy: 0.7983\n",
      "Epoch 52/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8811 - accuracy: 0.7856 - val_loss: 0.8417 - val_accuracy: 0.7984\n",
      "Epoch 53/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8705 - accuracy: 0.7873 - val_loss: 0.8323 - val_accuracy: 0.7985\n",
      "Epoch 54/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8613 - accuracy: 0.7869 - val_loss: 0.8234 - val_accuracy: 0.7990\n",
      "Epoch 55/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8511 - accuracy: 0.7883 - val_loss: 0.8145 - val_accuracy: 0.7986\n",
      "Epoch 56/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8414 - accuracy: 0.7884 - val_loss: 0.8053 - val_accuracy: 0.7994\n",
      "Epoch 57/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8334 - accuracy: 0.7886 - val_loss: 0.7979 - val_accuracy: 0.8000\n",
      "Epoch 58/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8251 - accuracy: 0.7900 - val_loss: 0.7895 - val_accuracy: 0.8000\n",
      "Epoch 59/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8159 - accuracy: 0.7903 - val_loss: 0.7811 - val_accuracy: 0.8005\n",
      "Epoch 60/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.8085 - accuracy: 0.7893 - val_loss: 0.7738 - val_accuracy: 0.8010\n",
      "Epoch 61/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7990 - accuracy: 0.7904 - val_loss: 0.7674 - val_accuracy: 0.8004\n",
      "Epoch 62/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7924 - accuracy: 0.7909 - val_loss: 0.7602 - val_accuracy: 0.8010\n",
      "Epoch 63/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7840 - accuracy: 0.7924 - val_loss: 0.7528 - val_accuracy: 0.8004\n",
      "Epoch 64/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7762 - accuracy: 0.7915 - val_loss: 0.7464 - val_accuracy: 0.8007\n",
      "Epoch 65/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7688 - accuracy: 0.7928 - val_loss: 0.7403 - val_accuracy: 0.8001\n",
      "Epoch 66/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7620 - accuracy: 0.7928 - val_loss: 0.7344 - val_accuracy: 0.8007\n",
      "Epoch 67/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7547 - accuracy: 0.7937 - val_loss: 0.7279 - val_accuracy: 0.8009\n",
      "Epoch 68/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7490 - accuracy: 0.7935 - val_loss: 0.7219 - val_accuracy: 0.8012\n",
      "Epoch 69/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7422 - accuracy: 0.7941 - val_loss: 0.7161 - val_accuracy: 0.8018\n",
      "Epoch 70/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7343 - accuracy: 0.7951 - val_loss: 0.7110 - val_accuracy: 0.8016\n",
      "Epoch 71/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7292 - accuracy: 0.7942 - val_loss: 0.7055 - val_accuracy: 0.8018\n",
      "Epoch 72/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7225 - accuracy: 0.7960 - val_loss: 0.6998 - val_accuracy: 0.8023\n",
      "Epoch 73/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7165 - accuracy: 0.7967 - val_loss: 0.6955 - val_accuracy: 0.8023\n",
      "Epoch 74/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7111 - accuracy: 0.7969 - val_loss: 0.6906 - val_accuracy: 0.8018\n",
      "Epoch 75/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.7054 - accuracy: 0.7967 - val_loss: 0.6854 - val_accuracy: 0.8024\n",
      "Epoch 76/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6993 - accuracy: 0.7979 - val_loss: 0.6807 - val_accuracy: 0.8026\n",
      "Epoch 77/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6956 - accuracy: 0.7984 - val_loss: 0.6759 - val_accuracy: 0.8031\n",
      "Epoch 78/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6903 - accuracy: 0.7975 - val_loss: 0.6717 - val_accuracy: 0.8031\n",
      "Epoch 79/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6859 - accuracy: 0.7978 - val_loss: 0.6679 - val_accuracy: 0.8022\n",
      "Epoch 80/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6808 - accuracy: 0.7980 - val_loss: 0.6638 - val_accuracy: 0.8015\n",
      "Epoch 81/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6763 - accuracy: 0.7993 - val_loss: 0.6600 - val_accuracy: 0.8029\n",
      "Epoch 82/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6705 - accuracy: 0.8008 - val_loss: 0.6557 - val_accuracy: 0.8031\n",
      "Epoch 83/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6665 - accuracy: 0.8011 - val_loss: 0.6523 - val_accuracy: 0.8034\n",
      "Epoch 84/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6620 - accuracy: 0.8012 - val_loss: 0.6484 - val_accuracy: 0.8038\n",
      "Epoch 85/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6576 - accuracy: 0.8012 - val_loss: 0.6444 - val_accuracy: 0.8038\n",
      "Epoch 86/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6538 - accuracy: 0.8017 - val_loss: 0.6408 - val_accuracy: 0.8038\n",
      "Epoch 87/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6489 - accuracy: 0.8026 - val_loss: 0.6378 - val_accuracy: 0.8043\n",
      "Epoch 88/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6456 - accuracy: 0.8020 - val_loss: 0.6348 - val_accuracy: 0.8045\n",
      "Epoch 89/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6408 - accuracy: 0.8032 - val_loss: 0.6314 - val_accuracy: 0.8047\n",
      "Epoch 90/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6384 - accuracy: 0.8029 - val_loss: 0.6283 - val_accuracy: 0.8050\n",
      "Epoch 91/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6343 - accuracy: 0.8036 - val_loss: 0.6252 - val_accuracy: 0.8056\n",
      "Epoch 92/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6306 - accuracy: 0.8034 - val_loss: 0.6224 - val_accuracy: 0.8060\n",
      "Epoch 93/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6272 - accuracy: 0.8041 - val_loss: 0.6196 - val_accuracy: 0.8058\n",
      "Epoch 94/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6245 - accuracy: 0.8048 - val_loss: 0.6166 - val_accuracy: 0.8056\n",
      "Epoch 95/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6216 - accuracy: 0.8055 - val_loss: 0.6142 - val_accuracy: 0.8059\n",
      "Epoch 96/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6176 - accuracy: 0.8060 - val_loss: 0.6114 - val_accuracy: 0.8062\n",
      "Epoch 97/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6143 - accuracy: 0.8062 - val_loss: 0.6086 - val_accuracy: 0.8071\n",
      "Epoch 98/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6111 - accuracy: 0.8072 - val_loss: 0.6058 - val_accuracy: 0.8070\n",
      "Epoch 99/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6072 - accuracy: 0.8073 - val_loss: 0.6035 - val_accuracy: 0.8072\n",
      "Epoch 100/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6042 - accuracy: 0.8072 - val_loss: 0.6010 - val_accuracy: 0.8075\n",
      "Epoch 101/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.6025 - accuracy: 0.8080 - val_loss: 0.5989 - val_accuracy: 0.8072\n",
      "Epoch 102/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5986 - accuracy: 0.8089 - val_loss: 0.5963 - val_accuracy: 0.8077\n",
      "Epoch 103/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5962 - accuracy: 0.8084 - val_loss: 0.5944 - val_accuracy: 0.8083\n",
      "Epoch 104/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5940 - accuracy: 0.8094 - val_loss: 0.5919 - val_accuracy: 0.8083\n",
      "Epoch 105/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5914 - accuracy: 0.8090 - val_loss: 0.5903 - val_accuracy: 0.8083\n",
      "Epoch 106/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5893 - accuracy: 0.8097 - val_loss: 0.5881 - val_accuracy: 0.8076\n",
      "Epoch 107/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5863 - accuracy: 0.8097 - val_loss: 0.5864 - val_accuracy: 0.8082\n",
      "Epoch 108/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5837 - accuracy: 0.8107 - val_loss: 0.5838 - val_accuracy: 0.8077\n",
      "Epoch 109/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5813 - accuracy: 0.8108 - val_loss: 0.5820 - val_accuracy: 0.8084\n",
      "Epoch 110/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5796 - accuracy: 0.8114 - val_loss: 0.5798 - val_accuracy: 0.8090\n",
      "Epoch 111/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5765 - accuracy: 0.8113 - val_loss: 0.5783 - val_accuracy: 0.8088\n",
      "Epoch 112/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5754 - accuracy: 0.8109 - val_loss: 0.5764 - val_accuracy: 0.8089\n",
      "Epoch 113/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5729 - accuracy: 0.8119 - val_loss: 0.5751 - val_accuracy: 0.8088\n",
      "Epoch 114/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5703 - accuracy: 0.8120 - val_loss: 0.5731 - val_accuracy: 0.8089\n",
      "Epoch 115/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5687 - accuracy: 0.8133 - val_loss: 0.5718 - val_accuracy: 0.8096\n",
      "Epoch 116/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5656 - accuracy: 0.8132 - val_loss: 0.5701 - val_accuracy: 0.8101\n",
      "Epoch 117/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5643 - accuracy: 0.8142 - val_loss: 0.5685 - val_accuracy: 0.8097\n",
      "Epoch 118/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5623 - accuracy: 0.8141 - val_loss: 0.5676 - val_accuracy: 0.8091\n",
      "Epoch 119/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5605 - accuracy: 0.8142 - val_loss: 0.5658 - val_accuracy: 0.8100\n",
      "Epoch 120/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5581 - accuracy: 0.8152 - val_loss: 0.5643 - val_accuracy: 0.8105\n",
      "Epoch 121/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5574 - accuracy: 0.8137 - val_loss: 0.5629 - val_accuracy: 0.8105\n",
      "Epoch 122/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5550 - accuracy: 0.8159 - val_loss: 0.5616 - val_accuracy: 0.8102\n",
      "Epoch 123/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5527 - accuracy: 0.8162 - val_loss: 0.5605 - val_accuracy: 0.8115\n",
      "Epoch 124/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5513 - accuracy: 0.8164 - val_loss: 0.5590 - val_accuracy: 0.8116\n",
      "Epoch 125/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5490 - accuracy: 0.8163 - val_loss: 0.5572 - val_accuracy: 0.8111\n",
      "Epoch 126/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5484 - accuracy: 0.8161 - val_loss: 0.5561 - val_accuracy: 0.8113\n",
      "Epoch 127/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5459 - accuracy: 0.8175 - val_loss: 0.5553 - val_accuracy: 0.8100\n",
      "Epoch 128/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5451 - accuracy: 0.8177 - val_loss: 0.5540 - val_accuracy: 0.8110\n",
      "Epoch 129/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5424 - accuracy: 0.8179 - val_loss: 0.5530 - val_accuracy: 0.8116\n",
      "Epoch 130/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5417 - accuracy: 0.8186 - val_loss: 0.5517 - val_accuracy: 0.8112\n",
      "Epoch 131/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5388 - accuracy: 0.8193 - val_loss: 0.5504 - val_accuracy: 0.8119\n",
      "Epoch 132/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5386 - accuracy: 0.8190 - val_loss: 0.5495 - val_accuracy: 0.8118\n",
      "Epoch 133/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5352 - accuracy: 0.8203 - val_loss: 0.5484 - val_accuracy: 0.8125\n",
      "Epoch 134/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5350 - accuracy: 0.8204 - val_loss: 0.5477 - val_accuracy: 0.8122\n",
      "Epoch 135/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5346 - accuracy: 0.8200 - val_loss: 0.5468 - val_accuracy: 0.8118\n",
      "Epoch 136/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5315 - accuracy: 0.8205 - val_loss: 0.5455 - val_accuracy: 0.8124\n",
      "Epoch 137/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5302 - accuracy: 0.8222 - val_loss: 0.5445 - val_accuracy: 0.8125\n",
      "Epoch 138/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5292 - accuracy: 0.8212 - val_loss: 0.5438 - val_accuracy: 0.8125\n",
      "Epoch 139/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5277 - accuracy: 0.8217 - val_loss: 0.5424 - val_accuracy: 0.8129\n",
      "Epoch 140/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5266 - accuracy: 0.8218 - val_loss: 0.5418 - val_accuracy: 0.8126\n",
      "Epoch 141/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5251 - accuracy: 0.8225 - val_loss: 0.5407 - val_accuracy: 0.8124\n",
      "Epoch 142/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5241 - accuracy: 0.8227 - val_loss: 0.5400 - val_accuracy: 0.8134\n",
      "Epoch 143/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5227 - accuracy: 0.8229 - val_loss: 0.5389 - val_accuracy: 0.8139\n",
      "Epoch 144/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5211 - accuracy: 0.8236 - val_loss: 0.5383 - val_accuracy: 0.8134\n",
      "Epoch 145/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5201 - accuracy: 0.8243 - val_loss: 0.5374 - val_accuracy: 0.8141\n",
      "Epoch 146/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5196 - accuracy: 0.8238 - val_loss: 0.5370 - val_accuracy: 0.8138\n",
      "Epoch 147/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5181 - accuracy: 0.8243 - val_loss: 0.5359 - val_accuracy: 0.8147\n",
      "Epoch 148/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5159 - accuracy: 0.8248 - val_loss: 0.5348 - val_accuracy: 0.8146\n",
      "Epoch 149/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5154 - accuracy: 0.8249 - val_loss: 0.5342 - val_accuracy: 0.8137\n",
      "Epoch 150/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5129 - accuracy: 0.8261 - val_loss: 0.5340 - val_accuracy: 0.8144\n",
      "Epoch 151/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5132 - accuracy: 0.8258 - val_loss: 0.5328 - val_accuracy: 0.8147\n",
      "Epoch 152/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5119 - accuracy: 0.8264 - val_loss: 0.5321 - val_accuracy: 0.8152\n",
      "Epoch 153/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5113 - accuracy: 0.8258 - val_loss: 0.5317 - val_accuracy: 0.8150\n",
      "Epoch 154/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5085 - accuracy: 0.8273 - val_loss: 0.5308 - val_accuracy: 0.8155\n",
      "Epoch 155/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5088 - accuracy: 0.8269 - val_loss: 0.5302 - val_accuracy: 0.8161\n",
      "Epoch 156/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5070 - accuracy: 0.8271 - val_loss: 0.5293 - val_accuracy: 0.8149\n",
      "Epoch 157/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5059 - accuracy: 0.8279 - val_loss: 0.5290 - val_accuracy: 0.8158\n",
      "Epoch 158/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5054 - accuracy: 0.8275 - val_loss: 0.5278 - val_accuracy: 0.8159\n",
      "Epoch 159/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5048 - accuracy: 0.8278 - val_loss: 0.5277 - val_accuracy: 0.8163\n",
      "Epoch 160/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5034 - accuracy: 0.8286 - val_loss: 0.5266 - val_accuracy: 0.8170\n",
      "Epoch 161/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5017 - accuracy: 0.8293 - val_loss: 0.5260 - val_accuracy: 0.8178\n",
      "Epoch 162/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5006 - accuracy: 0.8292 - val_loss: 0.5251 - val_accuracy: 0.8176\n",
      "Epoch 163/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.5001 - accuracy: 0.8294 - val_loss: 0.5246 - val_accuracy: 0.8172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4993 - accuracy: 0.8298 - val_loss: 0.5242 - val_accuracy: 0.8178\n",
      "Epoch 165/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4972 - accuracy: 0.8305 - val_loss: 0.5236 - val_accuracy: 0.8172\n",
      "Epoch 166/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4972 - accuracy: 0.8298 - val_loss: 0.5230 - val_accuracy: 0.8171\n",
      "Epoch 167/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4967 - accuracy: 0.8299 - val_loss: 0.5223 - val_accuracy: 0.8178\n",
      "Epoch 168/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4945 - accuracy: 0.8302 - val_loss: 0.5217 - val_accuracy: 0.8174\n",
      "Epoch 169/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4943 - accuracy: 0.8309 - val_loss: 0.5211 - val_accuracy: 0.8182\n",
      "Epoch 170/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4941 - accuracy: 0.8309 - val_loss: 0.5206 - val_accuracy: 0.8182\n",
      "Epoch 171/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4928 - accuracy: 0.8321 - val_loss: 0.5201 - val_accuracy: 0.8184\n",
      "Epoch 172/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4907 - accuracy: 0.8321 - val_loss: 0.5197 - val_accuracy: 0.8181\n",
      "Epoch 173/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4905 - accuracy: 0.8327 - val_loss: 0.5190 - val_accuracy: 0.8190\n",
      "Epoch 174/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4901 - accuracy: 0.8323 - val_loss: 0.5182 - val_accuracy: 0.8191\n",
      "Epoch 175/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4883 - accuracy: 0.8334 - val_loss: 0.5180 - val_accuracy: 0.8185\n",
      "Epoch 176/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4875 - accuracy: 0.8336 - val_loss: 0.5172 - val_accuracy: 0.8189\n",
      "Epoch 177/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4861 - accuracy: 0.8338 - val_loss: 0.5168 - val_accuracy: 0.8190\n",
      "Epoch 178/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4854 - accuracy: 0.8348 - val_loss: 0.5166 - val_accuracy: 0.8189\n",
      "Epoch 179/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4845 - accuracy: 0.8340 - val_loss: 0.5161 - val_accuracy: 0.8188\n",
      "Epoch 180/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4837 - accuracy: 0.8346 - val_loss: 0.5154 - val_accuracy: 0.8190\n",
      "Epoch 181/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4840 - accuracy: 0.8345 - val_loss: 0.5154 - val_accuracy: 0.8195\n",
      "Epoch 182/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4823 - accuracy: 0.8349 - val_loss: 0.5151 - val_accuracy: 0.8201\n",
      "Epoch 183/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4819 - accuracy: 0.8353 - val_loss: 0.5139 - val_accuracy: 0.8197\n",
      "Epoch 184/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4811 - accuracy: 0.8357 - val_loss: 0.5135 - val_accuracy: 0.8199\n",
      "Epoch 185/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4788 - accuracy: 0.8360 - val_loss: 0.5130 - val_accuracy: 0.8198\n",
      "Epoch 186/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4794 - accuracy: 0.8363 - val_loss: 0.5133 - val_accuracy: 0.8198\n",
      "Epoch 187/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4778 - accuracy: 0.8364 - val_loss: 0.5124 - val_accuracy: 0.8197\n",
      "Epoch 188/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4775 - accuracy: 0.8363 - val_loss: 0.5118 - val_accuracy: 0.8201\n",
      "Epoch 189/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4764 - accuracy: 0.8371 - val_loss: 0.5120 - val_accuracy: 0.8196\n",
      "Epoch 190/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4758 - accuracy: 0.8375 - val_loss: 0.5112 - val_accuracy: 0.8205\n",
      "Epoch 191/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4746 - accuracy: 0.8388 - val_loss: 0.5106 - val_accuracy: 0.8202\n",
      "Epoch 192/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4738 - accuracy: 0.8371 - val_loss: 0.5102 - val_accuracy: 0.8204\n",
      "Epoch 193/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4734 - accuracy: 0.8381 - val_loss: 0.5100 - val_accuracy: 0.8207\n",
      "Epoch 194/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4732 - accuracy: 0.8379 - val_loss: 0.5094 - val_accuracy: 0.8217\n",
      "Epoch 195/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4715 - accuracy: 0.8389 - val_loss: 0.5093 - val_accuracy: 0.8213\n",
      "Epoch 196/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4720 - accuracy: 0.8385 - val_loss: 0.5086 - val_accuracy: 0.8208\n",
      "Epoch 197/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4706 - accuracy: 0.8385 - val_loss: 0.5083 - val_accuracy: 0.8210\n",
      "Epoch 198/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4705 - accuracy: 0.8387 - val_loss: 0.5080 - val_accuracy: 0.8226\n",
      "Epoch 199/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4697 - accuracy: 0.8391 - val_loss: 0.5075 - val_accuracy: 0.8223\n",
      "Epoch 200/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4687 - accuracy: 0.8398 - val_loss: 0.5074 - val_accuracy: 0.8218\n",
      "Epoch 201/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4681 - accuracy: 0.8403 - val_loss: 0.5065 - val_accuracy: 0.8224\n",
      "Epoch 202/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4668 - accuracy: 0.8406 - val_loss: 0.5065 - val_accuracy: 0.8216\n",
      "Epoch 203/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4668 - accuracy: 0.8403 - val_loss: 0.5057 - val_accuracy: 0.8221\n",
      "Epoch 204/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4662 - accuracy: 0.8406 - val_loss: 0.5056 - val_accuracy: 0.8225\n",
      "Epoch 205/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4647 - accuracy: 0.8413 - val_loss: 0.5051 - val_accuracy: 0.8226\n",
      "Epoch 206/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4632 - accuracy: 0.8421 - val_loss: 0.5052 - val_accuracy: 0.8228\n",
      "Epoch 207/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4639 - accuracy: 0.8409 - val_loss: 0.5046 - val_accuracy: 0.8226\n",
      "Epoch 208/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4637 - accuracy: 0.8409 - val_loss: 0.5046 - val_accuracy: 0.8231\n",
      "Epoch 209/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4621 - accuracy: 0.8417 - val_loss: 0.5041 - val_accuracy: 0.8233\n",
      "Epoch 210/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4610 - accuracy: 0.8430 - val_loss: 0.5038 - val_accuracy: 0.8238\n",
      "Epoch 211/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4612 - accuracy: 0.8423 - val_loss: 0.5032 - val_accuracy: 0.8238\n",
      "Epoch 212/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4602 - accuracy: 0.8424 - val_loss: 0.5028 - val_accuracy: 0.8242\n",
      "Epoch 213/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4596 - accuracy: 0.8430 - val_loss: 0.5027 - val_accuracy: 0.8232\n",
      "Epoch 214/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4588 - accuracy: 0.8431 - val_loss: 0.5025 - val_accuracy: 0.8240\n",
      "Epoch 215/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4588 - accuracy: 0.8430 - val_loss: 0.5022 - val_accuracy: 0.8241\n",
      "Epoch 216/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4575 - accuracy: 0.8435 - val_loss: 0.5023 - val_accuracy: 0.8237\n",
      "Epoch 217/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4572 - accuracy: 0.8436 - val_loss: 0.5015 - val_accuracy: 0.8241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4562 - accuracy: 0.8446 - val_loss: 0.5012 - val_accuracy: 0.8243\n",
      "Epoch 219/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4560 - accuracy: 0.8444 - val_loss: 0.5008 - val_accuracy: 0.8254\n",
      "Epoch 220/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4547 - accuracy: 0.8443 - val_loss: 0.5005 - val_accuracy: 0.8251\n",
      "Epoch 221/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4543 - accuracy: 0.8447 - val_loss: 0.5007 - val_accuracy: 0.8253\n",
      "Epoch 222/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4540 - accuracy: 0.8452 - val_loss: 0.5004 - val_accuracy: 0.8255\n",
      "Epoch 223/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4524 - accuracy: 0.8460 - val_loss: 0.4999 - val_accuracy: 0.8258\n",
      "Epoch 224/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4535 - accuracy: 0.8448 - val_loss: 0.4995 - val_accuracy: 0.8252\n",
      "Epoch 225/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4520 - accuracy: 0.8457 - val_loss: 0.4998 - val_accuracy: 0.8259\n",
      "Epoch 226/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4517 - accuracy: 0.8455 - val_loss: 0.4994 - val_accuracy: 0.8259\n",
      "Epoch 227/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4510 - accuracy: 0.8464 - val_loss: 0.4992 - val_accuracy: 0.8262\n",
      "Epoch 228/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4497 - accuracy: 0.8467 - val_loss: 0.4989 - val_accuracy: 0.8261\n",
      "Epoch 229/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4497 - accuracy: 0.8467 - val_loss: 0.4990 - val_accuracy: 0.8260\n",
      "Epoch 230/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4493 - accuracy: 0.8468 - val_loss: 0.4986 - val_accuracy: 0.8253\n",
      "Epoch 231/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4489 - accuracy: 0.8469 - val_loss: 0.4982 - val_accuracy: 0.8261\n",
      "Epoch 232/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4476 - accuracy: 0.8472 - val_loss: 0.4979 - val_accuracy: 0.8265\n",
      "Epoch 233/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4484 - accuracy: 0.8468 - val_loss: 0.4979 - val_accuracy: 0.8265\n",
      "Epoch 234/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4463 - accuracy: 0.8475 - val_loss: 0.4980 - val_accuracy: 0.8264\n",
      "Epoch 235/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4457 - accuracy: 0.8480 - val_loss: 0.4972 - val_accuracy: 0.8267\n",
      "Epoch 236/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4448 - accuracy: 0.8489 - val_loss: 0.4970 - val_accuracy: 0.8266\n",
      "Epoch 237/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4450 - accuracy: 0.8484 - val_loss: 0.4973 - val_accuracy: 0.8266\n",
      "Epoch 238/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4446 - accuracy: 0.8483 - val_loss: 0.4968 - val_accuracy: 0.8277\n",
      "Epoch 239/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4439 - accuracy: 0.8483 - val_loss: 0.4963 - val_accuracy: 0.8270\n",
      "Epoch 240/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4434 - accuracy: 0.8492 - val_loss: 0.4962 - val_accuracy: 0.8273\n",
      "Epoch 241/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4435 - accuracy: 0.8488 - val_loss: 0.4960 - val_accuracy: 0.8278\n",
      "Epoch 242/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4425 - accuracy: 0.8491 - val_loss: 0.4959 - val_accuracy: 0.8270\n",
      "Epoch 243/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4424 - accuracy: 0.8499 - val_loss: 0.4952 - val_accuracy: 0.8284\n",
      "Epoch 244/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4415 - accuracy: 0.8492 - val_loss: 0.4952 - val_accuracy: 0.8271\n",
      "Epoch 245/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4403 - accuracy: 0.8501 - val_loss: 0.4951 - val_accuracy: 0.8276\n",
      "Epoch 246/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4396 - accuracy: 0.8503 - val_loss: 0.4951 - val_accuracy: 0.8269\n",
      "Epoch 247/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4393 - accuracy: 0.8507 - val_loss: 0.4947 - val_accuracy: 0.8284\n",
      "Epoch 248/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4388 - accuracy: 0.8507 - val_loss: 0.4945 - val_accuracy: 0.8278\n",
      "Epoch 249/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4389 - accuracy: 0.8504 - val_loss: 0.4941 - val_accuracy: 0.8270\n",
      "Epoch 250/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4378 - accuracy: 0.8506 - val_loss: 0.4942 - val_accuracy: 0.8283\n",
      "Epoch 251/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4377 - accuracy: 0.8515 - val_loss: 0.4938 - val_accuracy: 0.8288\n",
      "Epoch 252/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4367 - accuracy: 0.8516 - val_loss: 0.4936 - val_accuracy: 0.8284\n",
      "Epoch 253/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4356 - accuracy: 0.8520 - val_loss: 0.4935 - val_accuracy: 0.8287\n",
      "Epoch 254/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4352 - accuracy: 0.8518 - val_loss: 0.4930 - val_accuracy: 0.8279\n",
      "Epoch 255/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4351 - accuracy: 0.8526 - val_loss: 0.4932 - val_accuracy: 0.8285\n",
      "Epoch 256/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4362 - accuracy: 0.8516 - val_loss: 0.4928 - val_accuracy: 0.8292\n",
      "Epoch 257/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4341 - accuracy: 0.8528 - val_loss: 0.4925 - val_accuracy: 0.8289\n",
      "Epoch 258/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4336 - accuracy: 0.8522 - val_loss: 0.4925 - val_accuracy: 0.8289\n",
      "Epoch 259/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4330 - accuracy: 0.8531 - val_loss: 0.4923 - val_accuracy: 0.8283\n",
      "Epoch 260/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4330 - accuracy: 0.8525 - val_loss: 0.4922 - val_accuracy: 0.8295\n",
      "Epoch 261/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4317 - accuracy: 0.8532 - val_loss: 0.4920 - val_accuracy: 0.8289\n",
      "Epoch 262/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4312 - accuracy: 0.8535 - val_loss: 0.4917 - val_accuracy: 0.8285\n",
      "Epoch 263/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4310 - accuracy: 0.8534 - val_loss: 0.4914 - val_accuracy: 0.8288\n",
      "Epoch 264/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4304 - accuracy: 0.8545 - val_loss: 0.4912 - val_accuracy: 0.8294\n",
      "Epoch 265/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4296 - accuracy: 0.8545 - val_loss: 0.4907 - val_accuracy: 0.8305\n",
      "Epoch 266/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4292 - accuracy: 0.8549 - val_loss: 0.4914 - val_accuracy: 0.8297\n",
      "Epoch 267/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4289 - accuracy: 0.8548 - val_loss: 0.4913 - val_accuracy: 0.8294\n",
      "Epoch 268/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4288 - accuracy: 0.8540 - val_loss: 0.4905 - val_accuracy: 0.8302\n",
      "Epoch 269/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4281 - accuracy: 0.8547 - val_loss: 0.4907 - val_accuracy: 0.8301\n",
      "Epoch 270/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4281 - accuracy: 0.8546 - val_loss: 0.4912 - val_accuracy: 0.8293\n",
      "Epoch 271/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4268 - accuracy: 0.8556 - val_loss: 0.4905 - val_accuracy: 0.8295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4269 - accuracy: 0.8558 - val_loss: 0.4904 - val_accuracy: 0.8304\n",
      "Epoch 273/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4258 - accuracy: 0.8562 - val_loss: 0.4913 - val_accuracy: 0.8295\n",
      "Epoch 274/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4256 - accuracy: 0.8557 - val_loss: 0.4906 - val_accuracy: 0.8302\n",
      "Epoch 275/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4253 - accuracy: 0.8557 - val_loss: 0.4906 - val_accuracy: 0.8299\n",
      "Epoch 276/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4252 - accuracy: 0.8557 - val_loss: 0.4901 - val_accuracy: 0.8300\n",
      "Epoch 277/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4236 - accuracy: 0.8574 - val_loss: 0.4897 - val_accuracy: 0.8308\n",
      "Epoch 278/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4238 - accuracy: 0.8570 - val_loss: 0.4895 - val_accuracy: 0.8305\n",
      "Epoch 279/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4236 - accuracy: 0.8567 - val_loss: 0.4897 - val_accuracy: 0.8310\n",
      "Epoch 280/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4232 - accuracy: 0.8571 - val_loss: 0.4893 - val_accuracy: 0.8315\n",
      "Epoch 281/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4226 - accuracy: 0.8569 - val_loss: 0.4897 - val_accuracy: 0.8308\n",
      "Epoch 282/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4213 - accuracy: 0.8580 - val_loss: 0.4894 - val_accuracy: 0.8298\n",
      "Epoch 283/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4223 - accuracy: 0.8571 - val_loss: 0.4891 - val_accuracy: 0.8316\n",
      "Epoch 284/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4208 - accuracy: 0.8575 - val_loss: 0.4889 - val_accuracy: 0.8310\n",
      "Epoch 285/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4209 - accuracy: 0.8584 - val_loss: 0.4883 - val_accuracy: 0.8313\n",
      "Epoch 286/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4208 - accuracy: 0.8576 - val_loss: 0.4886 - val_accuracy: 0.8310\n",
      "Epoch 287/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4200 - accuracy: 0.8576 - val_loss: 0.4883 - val_accuracy: 0.8315\n",
      "Epoch 288/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4196 - accuracy: 0.8582 - val_loss: 0.4884 - val_accuracy: 0.8317\n",
      "Epoch 289/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4187 - accuracy: 0.8587 - val_loss: 0.4884 - val_accuracy: 0.8323\n",
      "Epoch 290/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4188 - accuracy: 0.8583 - val_loss: 0.4887 - val_accuracy: 0.8322\n",
      "Epoch 291/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4176 - accuracy: 0.8592 - val_loss: 0.4887 - val_accuracy: 0.8316\n",
      "Epoch 292/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4185 - accuracy: 0.8589 - val_loss: 0.4888 - val_accuracy: 0.8325\n",
      "Epoch 293/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4171 - accuracy: 0.8593 - val_loss: 0.4882 - val_accuracy: 0.8313\n",
      "Epoch 294/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4169 - accuracy: 0.8602 - val_loss: 0.4884 - val_accuracy: 0.8319\n",
      "Epoch 295/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4154 - accuracy: 0.8603 - val_loss: 0.4879 - val_accuracy: 0.8320\n",
      "Epoch 296/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4169 - accuracy: 0.8592 - val_loss: 0.4882 - val_accuracy: 0.8322\n",
      "Epoch 297/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4151 - accuracy: 0.8605 - val_loss: 0.4873 - val_accuracy: 0.8324\n",
      "Epoch 298/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4153 - accuracy: 0.8604 - val_loss: 0.4875 - val_accuracy: 0.8325\n",
      "Epoch 299/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4150 - accuracy: 0.8606 - val_loss: 0.4876 - val_accuracy: 0.8327\n",
      "Epoch 300/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4129 - accuracy: 0.8607 - val_loss: 0.4875 - val_accuracy: 0.8321\n",
      "Epoch 301/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4141 - accuracy: 0.8600 - val_loss: 0.4875 - val_accuracy: 0.8328\n",
      "Epoch 302/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4134 - accuracy: 0.8605 - val_loss: 0.4874 - val_accuracy: 0.8324\n",
      "Epoch 303/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4131 - accuracy: 0.8608 - val_loss: 0.4873 - val_accuracy: 0.8322\n",
      "Epoch 304/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4136 - accuracy: 0.8606 - val_loss: 0.4869 - val_accuracy: 0.8326\n",
      "Epoch 305/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4120 - accuracy: 0.8616 - val_loss: 0.4872 - val_accuracy: 0.8324\n",
      "Epoch 306/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4113 - accuracy: 0.8617 - val_loss: 0.4869 - val_accuracy: 0.8323\n",
      "Epoch 307/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4117 - accuracy: 0.8611 - val_loss: 0.4868 - val_accuracy: 0.8332\n",
      "Epoch 308/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4112 - accuracy: 0.8621 - val_loss: 0.4868 - val_accuracy: 0.8327\n",
      "Epoch 309/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4093 - accuracy: 0.8625 - val_loss: 0.4864 - val_accuracy: 0.8328\n",
      "Epoch 310/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4110 - accuracy: 0.8624 - val_loss: 0.4872 - val_accuracy: 0.8325\n",
      "Epoch 311/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4097 - accuracy: 0.8627 - val_loss: 0.4862 - val_accuracy: 0.8334\n",
      "Epoch 312/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4094 - accuracy: 0.8628 - val_loss: 0.4866 - val_accuracy: 0.8328\n",
      "Epoch 313/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4097 - accuracy: 0.8623 - val_loss: 0.4870 - val_accuracy: 0.8329\n",
      "Epoch 314/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4096 - accuracy: 0.8622 - val_loss: 0.4864 - val_accuracy: 0.8331\n",
      "Epoch 315/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4083 - accuracy: 0.8628 - val_loss: 0.4861 - val_accuracy: 0.8335\n",
      "Epoch 316/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4081 - accuracy: 0.8641 - val_loss: 0.4863 - val_accuracy: 0.8331\n",
      "Epoch 317/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4075 - accuracy: 0.8626 - val_loss: 0.4859 - val_accuracy: 0.8329\n",
      "Epoch 318/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4067 - accuracy: 0.8638 - val_loss: 0.4863 - val_accuracy: 0.8329\n",
      "Epoch 319/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4069 - accuracy: 0.8628 - val_loss: 0.4862 - val_accuracy: 0.8336\n",
      "Epoch 320/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4056 - accuracy: 0.8634 - val_loss: 0.4865 - val_accuracy: 0.8339\n",
      "Epoch 321/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4063 - accuracy: 0.8633 - val_loss: 0.4864 - val_accuracy: 0.8333\n",
      "Epoch 322/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4059 - accuracy: 0.8636 - val_loss: 0.4863 - val_accuracy: 0.8328\n",
      "Epoch 323/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4043 - accuracy: 0.8644 - val_loss: 0.4858 - val_accuracy: 0.8339\n",
      "Epoch 324/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4039 - accuracy: 0.8646 - val_loss: 0.4864 - val_accuracy: 0.8339\n",
      "Epoch 325/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4047 - accuracy: 0.8642 - val_loss: 0.4860 - val_accuracy: 0.8337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 326/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4037 - accuracy: 0.8648 - val_loss: 0.4863 - val_accuracy: 0.8336\n",
      "Epoch 327/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4041 - accuracy: 0.8645 - val_loss: 0.4858 - val_accuracy: 0.8339\n",
      "Epoch 328/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4034 - accuracy: 0.8651 - val_loss: 0.4857 - val_accuracy: 0.8341\n",
      "Epoch 329/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4020 - accuracy: 0.8656 - val_loss: 0.4857 - val_accuracy: 0.8348\n",
      "Epoch 330/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4030 - accuracy: 0.8655 - val_loss: 0.4858 - val_accuracy: 0.8337\n",
      "Epoch 331/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4025 - accuracy: 0.8649 - val_loss: 0.4860 - val_accuracy: 0.8353\n",
      "Epoch 332/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4018 - accuracy: 0.8655 - val_loss: 0.4861 - val_accuracy: 0.8344\n",
      "Epoch 333/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4002 - accuracy: 0.8663 - val_loss: 0.4850 - val_accuracy: 0.8344\n",
      "Epoch 334/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.4009 - accuracy: 0.8658 - val_loss: 0.4851 - val_accuracy: 0.8346\n",
      "Epoch 335/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3997 - accuracy: 0.8668 - val_loss: 0.4855 - val_accuracy: 0.8341\n",
      "Epoch 336/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3996 - accuracy: 0.8666 - val_loss: 0.4853 - val_accuracy: 0.8348\n",
      "Epoch 337/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3996 - accuracy: 0.8666 - val_loss: 0.4857 - val_accuracy: 0.8339\n",
      "Epoch 338/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3996 - accuracy: 0.8665 - val_loss: 0.4851 - val_accuracy: 0.8350\n",
      "Epoch 339/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3993 - accuracy: 0.8665 - val_loss: 0.4850 - val_accuracy: 0.8350\n",
      "Epoch 340/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3988 - accuracy: 0.8658 - val_loss: 0.4842 - val_accuracy: 0.8359\n",
      "Epoch 341/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3984 - accuracy: 0.8670 - val_loss: 0.4848 - val_accuracy: 0.8357\n",
      "Epoch 342/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3978 - accuracy: 0.8669 - val_loss: 0.4851 - val_accuracy: 0.8354\n",
      "Epoch 343/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3980 - accuracy: 0.8671 - val_loss: 0.4851 - val_accuracy: 0.8353\n",
      "Epoch 344/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3972 - accuracy: 0.8673 - val_loss: 0.4855 - val_accuracy: 0.8355\n",
      "Epoch 345/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3967 - accuracy: 0.8676 - val_loss: 0.4853 - val_accuracy: 0.8360\n",
      "Epoch 346/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3961 - accuracy: 0.8682 - val_loss: 0.4854 - val_accuracy: 0.8353\n",
      "Epoch 347/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3961 - accuracy: 0.8673 - val_loss: 0.4852 - val_accuracy: 0.8354\n",
      "Epoch 348/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3954 - accuracy: 0.8683 - val_loss: 0.4853 - val_accuracy: 0.8355\n",
      "Epoch 349/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3952 - accuracy: 0.8687 - val_loss: 0.4852 - val_accuracy: 0.8363\n",
      "Epoch 350/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3943 - accuracy: 0.8684 - val_loss: 0.4857 - val_accuracy: 0.8359\n",
      "Epoch 351/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3952 - accuracy: 0.8686 - val_loss: 0.4857 - val_accuracy: 0.8356\n",
      "Epoch 352/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3943 - accuracy: 0.8690 - val_loss: 0.4850 - val_accuracy: 0.8358\n",
      "Epoch 353/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3942 - accuracy: 0.8693 - val_loss: 0.4853 - val_accuracy: 0.8361\n",
      "Epoch 354/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3933 - accuracy: 0.8689 - val_loss: 0.4848 - val_accuracy: 0.8361\n",
      "Epoch 355/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3930 - accuracy: 0.8692 - val_loss: 0.4849 - val_accuracy: 0.8361\n",
      "Epoch 356/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3925 - accuracy: 0.8704 - val_loss: 0.4856 - val_accuracy: 0.8359\n",
      "Epoch 357/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3924 - accuracy: 0.8696 - val_loss: 0.4850 - val_accuracy: 0.8362\n",
      "Epoch 358/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3928 - accuracy: 0.8696 - val_loss: 0.4846 - val_accuracy: 0.8370\n",
      "Epoch 359/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3921 - accuracy: 0.8697 - val_loss: 0.4849 - val_accuracy: 0.8356\n",
      "Epoch 360/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3914 - accuracy: 0.8695 - val_loss: 0.4842 - val_accuracy: 0.8365\n",
      "Epoch 361/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3911 - accuracy: 0.8706 - val_loss: 0.4845 - val_accuracy: 0.8366\n",
      "Epoch 362/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3913 - accuracy: 0.8702 - val_loss: 0.4851 - val_accuracy: 0.8370\n",
      "Epoch 363/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3916 - accuracy: 0.8696 - val_loss: 0.4848 - val_accuracy: 0.8366\n",
      "Epoch 364/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3893 - accuracy: 0.8709 - val_loss: 0.4843 - val_accuracy: 0.8358\n",
      "Epoch 365/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3887 - accuracy: 0.8710 - val_loss: 0.4855 - val_accuracy: 0.8358\n",
      "Epoch 366/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3904 - accuracy: 0.8701 - val_loss: 0.4851 - val_accuracy: 0.8372\n",
      "Epoch 367/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3888 - accuracy: 0.8713 - val_loss: 0.4851 - val_accuracy: 0.8374\n",
      "Epoch 368/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3892 - accuracy: 0.8705 - val_loss: 0.4847 - val_accuracy: 0.8370\n",
      "Epoch 369/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3884 - accuracy: 0.8720 - val_loss: 0.4849 - val_accuracy: 0.8371\n",
      "Epoch 370/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3883 - accuracy: 0.8714 - val_loss: 0.4843 - val_accuracy: 0.8380\n",
      "Epoch 371/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3879 - accuracy: 0.8713 - val_loss: 0.4850 - val_accuracy: 0.8365\n",
      "Epoch 372/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3884 - accuracy: 0.8711 - val_loss: 0.4853 - val_accuracy: 0.8375\n",
      "Epoch 373/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3867 - accuracy: 0.8720 - val_loss: 0.4848 - val_accuracy: 0.8376\n",
      "Epoch 374/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3868 - accuracy: 0.8722 - val_loss: 0.4849 - val_accuracy: 0.8367\n",
      "Epoch 375/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3861 - accuracy: 0.8723 - val_loss: 0.4848 - val_accuracy: 0.8375\n",
      "Epoch 376/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3873 - accuracy: 0.8719 - val_loss: 0.4852 - val_accuracy: 0.8375\n",
      "Epoch 377/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3862 - accuracy: 0.8719 - val_loss: 0.4854 - val_accuracy: 0.8369\n",
      "Epoch 378/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3858 - accuracy: 0.8725 - val_loss: 0.4852 - val_accuracy: 0.8366\n",
      "Epoch 379/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3852 - accuracy: 0.8722 - val_loss: 0.4853 - val_accuracy: 0.8374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3852 - accuracy: 0.8717 - val_loss: 0.4850 - val_accuracy: 0.8373\n",
      "Epoch 381/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3843 - accuracy: 0.8731 - val_loss: 0.4855 - val_accuracy: 0.8365\n",
      "Epoch 382/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3841 - accuracy: 0.8725 - val_loss: 0.4848 - val_accuracy: 0.8376\n",
      "Epoch 383/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3844 - accuracy: 0.8726 - val_loss: 0.4854 - val_accuracy: 0.8374\n",
      "Epoch 384/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3841 - accuracy: 0.8726 - val_loss: 0.4851 - val_accuracy: 0.8375\n",
      "Epoch 385/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3841 - accuracy: 0.8722 - val_loss: 0.4854 - val_accuracy: 0.8379\n",
      "Epoch 386/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3839 - accuracy: 0.8729 - val_loss: 0.4845 - val_accuracy: 0.8379\n",
      "Epoch 387/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3832 - accuracy: 0.8733 - val_loss: 0.4847 - val_accuracy: 0.8375\n",
      "Epoch 388/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3828 - accuracy: 0.8736 - val_loss: 0.4852 - val_accuracy: 0.8378\n",
      "Epoch 389/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3817 - accuracy: 0.8742 - val_loss: 0.4854 - val_accuracy: 0.8378\n",
      "Epoch 390/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3824 - accuracy: 0.8730 - val_loss: 0.4847 - val_accuracy: 0.8376\n",
      "Epoch 391/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3806 - accuracy: 0.8748 - val_loss: 0.4848 - val_accuracy: 0.8379\n",
      "Epoch 392/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3806 - accuracy: 0.8749 - val_loss: 0.4851 - val_accuracy: 0.8380\n",
      "Epoch 393/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3816 - accuracy: 0.8740 - val_loss: 0.4845 - val_accuracy: 0.8389\n",
      "Epoch 394/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3802 - accuracy: 0.8751 - val_loss: 0.4850 - val_accuracy: 0.8378\n",
      "Epoch 395/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3808 - accuracy: 0.8740 - val_loss: 0.4857 - val_accuracy: 0.8383\n",
      "Epoch 396/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3805 - accuracy: 0.8743 - val_loss: 0.4847 - val_accuracy: 0.8386\n",
      "Epoch 397/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3818 - accuracy: 0.8740 - val_loss: 0.4852 - val_accuracy: 0.8388\n",
      "Epoch 398/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3798 - accuracy: 0.8746 - val_loss: 0.4853 - val_accuracy: 0.8392\n",
      "Epoch 399/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3788 - accuracy: 0.8754 - val_loss: 0.4856 - val_accuracy: 0.8386\n",
      "Epoch 400/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3787 - accuracy: 0.8753 - val_loss: 0.4856 - val_accuracy: 0.8395\n",
      "Epoch 401/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3781 - accuracy: 0.8749 - val_loss: 0.4858 - val_accuracy: 0.8386\n",
      "Epoch 402/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3774 - accuracy: 0.8750 - val_loss: 0.4843 - val_accuracy: 0.8395\n",
      "Epoch 403/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3774 - accuracy: 0.8762 - val_loss: 0.4854 - val_accuracy: 0.8385\n",
      "Epoch 404/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3780 - accuracy: 0.8752 - val_loss: 0.4850 - val_accuracy: 0.8385\n",
      "Epoch 405/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3772 - accuracy: 0.8753 - val_loss: 0.4857 - val_accuracy: 0.8392\n",
      "Epoch 406/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3764 - accuracy: 0.8766 - val_loss: 0.4849 - val_accuracy: 0.8397\n",
      "Epoch 407/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3773 - accuracy: 0.8760 - val_loss: 0.4849 - val_accuracy: 0.8390\n",
      "Epoch 408/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3764 - accuracy: 0.8759 - val_loss: 0.4849 - val_accuracy: 0.8391\n",
      "Epoch 409/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3755 - accuracy: 0.8762 - val_loss: 0.4846 - val_accuracy: 0.8401\n",
      "Epoch 410/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3760 - accuracy: 0.8766 - val_loss: 0.4846 - val_accuracy: 0.8388\n",
      "Epoch 411/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3756 - accuracy: 0.8764 - val_loss: 0.4862 - val_accuracy: 0.8390\n",
      "Epoch 412/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3745 - accuracy: 0.8766 - val_loss: 0.4852 - val_accuracy: 0.8387\n",
      "Epoch 413/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3739 - accuracy: 0.8773 - val_loss: 0.4849 - val_accuracy: 0.8400\n",
      "Epoch 414/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3744 - accuracy: 0.8767 - val_loss: 0.4858 - val_accuracy: 0.8397\n",
      "Epoch 415/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3746 - accuracy: 0.8771 - val_loss: 0.4860 - val_accuracy: 0.8407\n",
      "Epoch 416/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3738 - accuracy: 0.8771 - val_loss: 0.4854 - val_accuracy: 0.8404\n",
      "Epoch 417/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3741 - accuracy: 0.8767 - val_loss: 0.4858 - val_accuracy: 0.8397\n",
      "Epoch 418/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3746 - accuracy: 0.8767 - val_loss: 0.4854 - val_accuracy: 0.8401\n",
      "Epoch 419/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3735 - accuracy: 0.8772 - val_loss: 0.4851 - val_accuracy: 0.8403\n",
      "Epoch 420/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3728 - accuracy: 0.8772 - val_loss: 0.4858 - val_accuracy: 0.8402\n",
      "Epoch 421/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3725 - accuracy: 0.8776 - val_loss: 0.4854 - val_accuracy: 0.8399\n",
      "Epoch 422/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3718 - accuracy: 0.8786 - val_loss: 0.4851 - val_accuracy: 0.8402\n",
      "Epoch 423/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3720 - accuracy: 0.8777 - val_loss: 0.4848 - val_accuracy: 0.8393\n",
      "Epoch 424/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3715 - accuracy: 0.8781 - val_loss: 0.4861 - val_accuracy: 0.8406\n",
      "Epoch 425/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3710 - accuracy: 0.8786 - val_loss: 0.4864 - val_accuracy: 0.8394\n",
      "Epoch 426/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3717 - accuracy: 0.8775 - val_loss: 0.4869 - val_accuracy: 0.8405\n",
      "Epoch 427/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3706 - accuracy: 0.8784 - val_loss: 0.4855 - val_accuracy: 0.8403\n",
      "Epoch 428/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3721 - accuracy: 0.8770 - val_loss: 0.4857 - val_accuracy: 0.8400\n",
      "Epoch 429/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3709 - accuracy: 0.8780 - val_loss: 0.4864 - val_accuracy: 0.8401\n",
      "Epoch 430/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3707 - accuracy: 0.8783 - val_loss: 0.4862 - val_accuracy: 0.8412\n",
      "Epoch 431/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3701 - accuracy: 0.8788 - val_loss: 0.4868 - val_accuracy: 0.8407\n",
      "Epoch 432/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3693 - accuracy: 0.8789 - val_loss: 0.4860 - val_accuracy: 0.8404\n",
      "Epoch 433/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3691 - accuracy: 0.8791 - val_loss: 0.4854 - val_accuracy: 0.8409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 434/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3688 - accuracy: 0.8794 - val_loss: 0.4855 - val_accuracy: 0.8411\n",
      "Epoch 435/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3683 - accuracy: 0.8805 - val_loss: 0.4860 - val_accuracy: 0.8413\n",
      "Epoch 436/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3689 - accuracy: 0.8786 - val_loss: 0.4865 - val_accuracy: 0.8410\n",
      "Epoch 437/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3685 - accuracy: 0.8785 - val_loss: 0.4865 - val_accuracy: 0.8404\n",
      "Epoch 438/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3675 - accuracy: 0.8796 - val_loss: 0.4863 - val_accuracy: 0.8403\n",
      "Epoch 439/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3688 - accuracy: 0.8793 - val_loss: 0.4875 - val_accuracy: 0.8401\n",
      "Epoch 440/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3673 - accuracy: 0.8801 - val_loss: 0.4859 - val_accuracy: 0.8418\n",
      "Epoch 441/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3667 - accuracy: 0.8792 - val_loss: 0.4852 - val_accuracy: 0.8407\n",
      "Epoch 442/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3671 - accuracy: 0.8796 - val_loss: 0.4867 - val_accuracy: 0.8414\n",
      "Epoch 443/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3675 - accuracy: 0.8796 - val_loss: 0.4877 - val_accuracy: 0.8401\n",
      "Epoch 444/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3673 - accuracy: 0.8795 - val_loss: 0.4871 - val_accuracy: 0.8411\n",
      "Epoch 445/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3663 - accuracy: 0.8798 - val_loss: 0.4862 - val_accuracy: 0.8412\n",
      "Epoch 446/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3660 - accuracy: 0.8802 - val_loss: 0.4874 - val_accuracy: 0.8419\n",
      "Epoch 447/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3657 - accuracy: 0.8806 - val_loss: 0.4866 - val_accuracy: 0.8417\n",
      "Epoch 448/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3658 - accuracy: 0.8797 - val_loss: 0.4865 - val_accuracy: 0.8413\n",
      "Epoch 449/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3648 - accuracy: 0.8812 - val_loss: 0.4863 - val_accuracy: 0.8424\n",
      "Epoch 450/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3650 - accuracy: 0.8812 - val_loss: 0.4863 - val_accuracy: 0.8421\n",
      "Epoch 451/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3650 - accuracy: 0.8806 - val_loss: 0.4871 - val_accuracy: 0.8411\n",
      "Epoch 452/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3639 - accuracy: 0.8811 - val_loss: 0.4866 - val_accuracy: 0.8412\n",
      "Epoch 453/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3647 - accuracy: 0.8806 - val_loss: 0.4868 - val_accuracy: 0.8415\n",
      "Epoch 454/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3642 - accuracy: 0.8816 - val_loss: 0.4874 - val_accuracy: 0.8422\n",
      "Epoch 455/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3633 - accuracy: 0.8815 - val_loss: 0.4871 - val_accuracy: 0.8415\n",
      "Epoch 456/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3621 - accuracy: 0.8818 - val_loss: 0.4871 - val_accuracy: 0.8411\n",
      "Epoch 457/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3627 - accuracy: 0.8815 - val_loss: 0.4871 - val_accuracy: 0.8413\n",
      "Epoch 458/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3631 - accuracy: 0.8815 - val_loss: 0.4864 - val_accuracy: 0.8420\n",
      "Epoch 459/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3625 - accuracy: 0.8815 - val_loss: 0.4868 - val_accuracy: 0.8424\n",
      "Epoch 460/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3632 - accuracy: 0.8808 - val_loss: 0.4868 - val_accuracy: 0.8415\n",
      "Epoch 461/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3621 - accuracy: 0.8819 - val_loss: 0.4874 - val_accuracy: 0.8419\n",
      "Epoch 462/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3626 - accuracy: 0.8816 - val_loss: 0.4860 - val_accuracy: 0.8424\n",
      "Epoch 463/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3622 - accuracy: 0.8821 - val_loss: 0.4868 - val_accuracy: 0.8419\n",
      "Epoch 464/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3608 - accuracy: 0.8828 - val_loss: 0.4866 - val_accuracy: 0.8420\n",
      "Epoch 465/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3617 - accuracy: 0.8818 - val_loss: 0.4877 - val_accuracy: 0.8418\n",
      "Epoch 466/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3606 - accuracy: 0.8827 - val_loss: 0.4866 - val_accuracy: 0.8427\n",
      "Epoch 467/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3609 - accuracy: 0.8818 - val_loss: 0.4861 - val_accuracy: 0.8428\n",
      "Epoch 468/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3602 - accuracy: 0.8825 - val_loss: 0.4864 - val_accuracy: 0.8417\n",
      "Epoch 469/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3600 - accuracy: 0.8828 - val_loss: 0.4874 - val_accuracy: 0.8421\n",
      "Epoch 470/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3601 - accuracy: 0.8829 - val_loss: 0.4873 - val_accuracy: 0.8433\n",
      "Epoch 471/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3598 - accuracy: 0.8826 - val_loss: 0.4881 - val_accuracy: 0.8421\n",
      "Epoch 472/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3582 - accuracy: 0.8834 - val_loss: 0.4872 - val_accuracy: 0.8424\n",
      "Epoch 473/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3590 - accuracy: 0.8837 - val_loss: 0.4875 - val_accuracy: 0.8419\n",
      "Epoch 474/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3598 - accuracy: 0.8826 - val_loss: 0.4874 - val_accuracy: 0.8426\n",
      "Epoch 475/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3596 - accuracy: 0.8827 - val_loss: 0.4875 - val_accuracy: 0.8426\n",
      "Epoch 476/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3587 - accuracy: 0.8829 - val_loss: 0.4875 - val_accuracy: 0.8428\n",
      "Epoch 477/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3580 - accuracy: 0.8834 - val_loss: 0.4867 - val_accuracy: 0.8438\n",
      "Epoch 478/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3577 - accuracy: 0.8835 - val_loss: 0.4868 - val_accuracy: 0.8426\n",
      "Epoch 479/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3588 - accuracy: 0.8831 - val_loss: 0.4869 - val_accuracy: 0.8424\n",
      "Epoch 480/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3571 - accuracy: 0.8840 - val_loss: 0.4871 - val_accuracy: 0.8424\n",
      "Epoch 481/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3570 - accuracy: 0.8834 - val_loss: 0.4879 - val_accuracy: 0.8433\n",
      "Epoch 482/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3574 - accuracy: 0.8833 - val_loss: 0.4861 - val_accuracy: 0.8430\n",
      "Epoch 483/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3571 - accuracy: 0.8841 - val_loss: 0.4865 - val_accuracy: 0.8418\n",
      "Epoch 484/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3579 - accuracy: 0.8834 - val_loss: 0.4866 - val_accuracy: 0.8426\n",
      "Epoch 485/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3566 - accuracy: 0.8847 - val_loss: 0.4874 - val_accuracy: 0.8436\n",
      "Epoch 486/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3560 - accuracy: 0.8842 - val_loss: 0.4876 - val_accuracy: 0.8421\n",
      "Epoch 487/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3560 - accuracy: 0.8843 - val_loss: 0.4880 - val_accuracy: 0.8435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 488/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3544 - accuracy: 0.8853 - val_loss: 0.4878 - val_accuracy: 0.8427\n",
      "Epoch 489/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3537 - accuracy: 0.8850 - val_loss: 0.4870 - val_accuracy: 0.8440\n",
      "Epoch 490/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3563 - accuracy: 0.8843 - val_loss: 0.4875 - val_accuracy: 0.8430\n",
      "Epoch 491/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3552 - accuracy: 0.8845 - val_loss: 0.4878 - val_accuracy: 0.8431\n",
      "Epoch 492/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3546 - accuracy: 0.8847 - val_loss: 0.4881 - val_accuracy: 0.8436\n",
      "Epoch 493/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3533 - accuracy: 0.8855 - val_loss: 0.4882 - val_accuracy: 0.8437\n",
      "Epoch 494/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3536 - accuracy: 0.8854 - val_loss: 0.4879 - val_accuracy: 0.8433\n",
      "Epoch 495/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3540 - accuracy: 0.8855 - val_loss: 0.4879 - val_accuracy: 0.8452\n",
      "Epoch 496/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3537 - accuracy: 0.8853 - val_loss: 0.4877 - val_accuracy: 0.8441\n",
      "Epoch 497/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3533 - accuracy: 0.8855 - val_loss: 0.4889 - val_accuracy: 0.8441\n",
      "Epoch 498/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3533 - accuracy: 0.8854 - val_loss: 0.4890 - val_accuracy: 0.8438\n",
      "Epoch 499/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3531 - accuracy: 0.8853 - val_loss: 0.4889 - val_accuracy: 0.8441\n",
      "Epoch 500/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3522 - accuracy: 0.8858 - val_loss: 0.4887 - val_accuracy: 0.8442\n",
      "Epoch 501/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3527 - accuracy: 0.8857 - val_loss: 0.4899 - val_accuracy: 0.8442\n",
      "Epoch 502/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3521 - accuracy: 0.8856 - val_loss: 0.4891 - val_accuracy: 0.8443\n",
      "Epoch 503/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3528 - accuracy: 0.8857 - val_loss: 0.4881 - val_accuracy: 0.8444\n",
      "Epoch 504/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3513 - accuracy: 0.8864 - val_loss: 0.4888 - val_accuracy: 0.8449\n",
      "Epoch 505/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3520 - accuracy: 0.8858 - val_loss: 0.4886 - val_accuracy: 0.8443\n",
      "Epoch 506/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3520 - accuracy: 0.8858 - val_loss: 0.4892 - val_accuracy: 0.8443\n",
      "Epoch 507/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3516 - accuracy: 0.8856 - val_loss: 0.4885 - val_accuracy: 0.8441\n",
      "Epoch 508/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3499 - accuracy: 0.8866 - val_loss: 0.4894 - val_accuracy: 0.8440\n",
      "Epoch 509/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3510 - accuracy: 0.8870 - val_loss: 0.4888 - val_accuracy: 0.8443\n",
      "Epoch 510/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3513 - accuracy: 0.8863 - val_loss: 0.4893 - val_accuracy: 0.8445\n",
      "Epoch 511/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3502 - accuracy: 0.8861 - val_loss: 0.4892 - val_accuracy: 0.8446\n",
      "Epoch 512/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3503 - accuracy: 0.8862 - val_loss: 0.4896 - val_accuracy: 0.8443\n",
      "Epoch 513/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3499 - accuracy: 0.8866 - val_loss: 0.4900 - val_accuracy: 0.8447\n",
      "Epoch 514/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3501 - accuracy: 0.8871 - val_loss: 0.4897 - val_accuracy: 0.8453\n",
      "Epoch 515/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3495 - accuracy: 0.8873 - val_loss: 0.4890 - val_accuracy: 0.8443\n",
      "Epoch 516/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3489 - accuracy: 0.8869 - val_loss: 0.4889 - val_accuracy: 0.8454\n",
      "Epoch 517/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3499 - accuracy: 0.8873 - val_loss: 0.4893 - val_accuracy: 0.8443\n",
      "Epoch 518/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3493 - accuracy: 0.8869 - val_loss: 0.4909 - val_accuracy: 0.8441\n",
      "Epoch 519/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3484 - accuracy: 0.8876 - val_loss: 0.4895 - val_accuracy: 0.8444\n",
      "Epoch 520/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3486 - accuracy: 0.8879 - val_loss: 0.4902 - val_accuracy: 0.8456\n",
      "Epoch 521/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3477 - accuracy: 0.8880 - val_loss: 0.4901 - val_accuracy: 0.8444\n",
      "Epoch 522/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3482 - accuracy: 0.8876 - val_loss: 0.4899 - val_accuracy: 0.8451\n",
      "Epoch 523/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3482 - accuracy: 0.8879 - val_loss: 0.4903 - val_accuracy: 0.8453\n",
      "Epoch 524/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3472 - accuracy: 0.8877 - val_loss: 0.4902 - val_accuracy: 0.8455\n",
      "Epoch 525/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3476 - accuracy: 0.8870 - val_loss: 0.4902 - val_accuracy: 0.8447\n",
      "Epoch 526/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3470 - accuracy: 0.8885 - val_loss: 0.4903 - val_accuracy: 0.8458\n",
      "Epoch 527/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3464 - accuracy: 0.8888 - val_loss: 0.4895 - val_accuracy: 0.8456\n",
      "Epoch 528/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3460 - accuracy: 0.8876 - val_loss: 0.4894 - val_accuracy: 0.8454\n",
      "Epoch 529/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3460 - accuracy: 0.8879 - val_loss: 0.4897 - val_accuracy: 0.8461\n",
      "Epoch 530/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3468 - accuracy: 0.8876 - val_loss: 0.4890 - val_accuracy: 0.8465\n",
      "Epoch 531/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3468 - accuracy: 0.8879 - val_loss: 0.4902 - val_accuracy: 0.8454\n",
      "Epoch 532/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3452 - accuracy: 0.8884 - val_loss: 0.4911 - val_accuracy: 0.8461\n",
      "Epoch 533/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3449 - accuracy: 0.8892 - val_loss: 0.4905 - val_accuracy: 0.8452\n",
      "Epoch 534/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3463 - accuracy: 0.8884 - val_loss: 0.4893 - val_accuracy: 0.8462\n",
      "Epoch 535/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3457 - accuracy: 0.8878 - val_loss: 0.4909 - val_accuracy: 0.8462\n",
      "Epoch 536/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3456 - accuracy: 0.8883 - val_loss: 0.4902 - val_accuracy: 0.8457\n",
      "Epoch 537/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3449 - accuracy: 0.8886 - val_loss: 0.4913 - val_accuracy: 0.8457\n",
      "Epoch 538/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3448 - accuracy: 0.8887 - val_loss: 0.4903 - val_accuracy: 0.8456\n",
      "Epoch 539/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3449 - accuracy: 0.8885 - val_loss: 0.4912 - val_accuracy: 0.8459\n",
      "Epoch 540/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3450 - accuracy: 0.8880 - val_loss: 0.4913 - val_accuracy: 0.8456\n",
      "Epoch 541/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3443 - accuracy: 0.8893 - val_loss: 0.4901 - val_accuracy: 0.8464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 542/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3437 - accuracy: 0.8896 - val_loss: 0.4903 - val_accuracy: 0.8466\n",
      "Epoch 543/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3436 - accuracy: 0.8892 - val_loss: 0.4904 - val_accuracy: 0.8467\n",
      "Epoch 544/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3440 - accuracy: 0.8893 - val_loss: 0.4916 - val_accuracy: 0.8460\n",
      "Epoch 545/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3433 - accuracy: 0.8897 - val_loss: 0.4903 - val_accuracy: 0.8468\n",
      "Epoch 546/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3427 - accuracy: 0.8897 - val_loss: 0.4912 - val_accuracy: 0.8465\n",
      "Epoch 547/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3432 - accuracy: 0.8894 - val_loss: 0.4914 - val_accuracy: 0.8459\n",
      "Epoch 548/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3427 - accuracy: 0.8900 - val_loss: 0.4909 - val_accuracy: 0.8464\n",
      "Epoch 549/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3424 - accuracy: 0.8896 - val_loss: 0.4915 - val_accuracy: 0.8476\n",
      "Epoch 550/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3427 - accuracy: 0.8899 - val_loss: 0.4908 - val_accuracy: 0.8468\n",
      "Epoch 551/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3418 - accuracy: 0.8901 - val_loss: 0.4914 - val_accuracy: 0.8465\n",
      "Epoch 552/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3405 - accuracy: 0.8911 - val_loss: 0.4921 - val_accuracy: 0.8474\n",
      "Epoch 553/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3421 - accuracy: 0.8897 - val_loss: 0.4919 - val_accuracy: 0.8461\n",
      "Epoch 554/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3409 - accuracy: 0.8901 - val_loss: 0.4925 - val_accuracy: 0.8466\n",
      "Epoch 555/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3407 - accuracy: 0.8903 - val_loss: 0.4919 - val_accuracy: 0.8470\n",
      "Epoch 556/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3411 - accuracy: 0.8902 - val_loss: 0.4935 - val_accuracy: 0.8461\n",
      "Epoch 557/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3413 - accuracy: 0.8902 - val_loss: 0.4921 - val_accuracy: 0.8459\n",
      "Epoch 558/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3406 - accuracy: 0.8911 - val_loss: 0.4922 - val_accuracy: 0.8468\n",
      "Epoch 559/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3394 - accuracy: 0.8910 - val_loss: 0.4924 - val_accuracy: 0.8462\n",
      "Epoch 560/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3406 - accuracy: 0.8903 - val_loss: 0.4932 - val_accuracy: 0.8471\n",
      "Epoch 561/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3391 - accuracy: 0.8913 - val_loss: 0.4919 - val_accuracy: 0.8468\n",
      "Epoch 562/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3397 - accuracy: 0.8907 - val_loss: 0.4924 - val_accuracy: 0.8467\n",
      "Epoch 563/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3408 - accuracy: 0.8906 - val_loss: 0.4938 - val_accuracy: 0.8467\n",
      "Epoch 564/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3396 - accuracy: 0.8908 - val_loss: 0.4925 - val_accuracy: 0.8468\n",
      "Epoch 565/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3401 - accuracy: 0.8912 - val_loss: 0.4929 - val_accuracy: 0.8475\n",
      "Epoch 566/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3388 - accuracy: 0.8912 - val_loss: 0.4933 - val_accuracy: 0.8464\n",
      "Epoch 567/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3396 - accuracy: 0.8909 - val_loss: 0.4931 - val_accuracy: 0.8473\n",
      "Epoch 568/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3380 - accuracy: 0.8914 - val_loss: 0.4927 - val_accuracy: 0.8470\n",
      "Epoch 569/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3395 - accuracy: 0.8911 - val_loss: 0.4937 - val_accuracy: 0.8470\n",
      "Epoch 570/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3380 - accuracy: 0.8918 - val_loss: 0.4951 - val_accuracy: 0.8473\n",
      "Epoch 571/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3380 - accuracy: 0.8922 - val_loss: 0.4924 - val_accuracy: 0.8476\n",
      "Epoch 572/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3386 - accuracy: 0.8911 - val_loss: 0.4942 - val_accuracy: 0.8470\n",
      "Epoch 573/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3372 - accuracy: 0.8921 - val_loss: 0.4944 - val_accuracy: 0.8469\n",
      "Epoch 574/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3376 - accuracy: 0.8914 - val_loss: 0.4935 - val_accuracy: 0.8477\n",
      "Epoch 575/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3376 - accuracy: 0.8922 - val_loss: 0.4936 - val_accuracy: 0.8473\n",
      "Epoch 576/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3372 - accuracy: 0.8917 - val_loss: 0.4938 - val_accuracy: 0.8474\n",
      "Epoch 577/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3370 - accuracy: 0.8918 - val_loss: 0.4928 - val_accuracy: 0.8470\n",
      "Epoch 578/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3365 - accuracy: 0.8921 - val_loss: 0.4939 - val_accuracy: 0.8475\n",
      "Epoch 579/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3356 - accuracy: 0.8930 - val_loss: 0.4949 - val_accuracy: 0.8483\n",
      "Epoch 580/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3365 - accuracy: 0.8926 - val_loss: 0.4941 - val_accuracy: 0.8473\n",
      "Epoch 581/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3374 - accuracy: 0.8916 - val_loss: 0.4938 - val_accuracy: 0.8469\n",
      "Epoch 582/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3360 - accuracy: 0.8926 - val_loss: 0.4961 - val_accuracy: 0.8468\n",
      "Epoch 583/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3364 - accuracy: 0.8916 - val_loss: 0.4961 - val_accuracy: 0.8470\n",
      "Epoch 584/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3349 - accuracy: 0.8927 - val_loss: 0.4960 - val_accuracy: 0.8476\n",
      "Epoch 585/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3351 - accuracy: 0.8923 - val_loss: 0.4951 - val_accuracy: 0.8477\n",
      "Epoch 586/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3346 - accuracy: 0.8932 - val_loss: 0.4935 - val_accuracy: 0.8474\n",
      "Epoch 587/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3351 - accuracy: 0.8924 - val_loss: 0.4949 - val_accuracy: 0.8470\n",
      "Epoch 588/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3360 - accuracy: 0.8921 - val_loss: 0.4941 - val_accuracy: 0.8474\n",
      "Epoch 589/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3345 - accuracy: 0.8934 - val_loss: 0.4944 - val_accuracy: 0.8475\n",
      "Epoch 590/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3342 - accuracy: 0.8936 - val_loss: 0.4959 - val_accuracy: 0.8476\n",
      "Epoch 591/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3337 - accuracy: 0.8928 - val_loss: 0.4955 - val_accuracy: 0.8473\n",
      "Epoch 592/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3344 - accuracy: 0.8925 - val_loss: 0.4964 - val_accuracy: 0.8470\n",
      "Epoch 593/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3326 - accuracy: 0.8941 - val_loss: 0.4952 - val_accuracy: 0.8477\n",
      "Epoch 594/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3331 - accuracy: 0.8938 - val_loss: 0.4964 - val_accuracy: 0.8476\n",
      "Epoch 595/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3340 - accuracy: 0.8927 - val_loss: 0.4965 - val_accuracy: 0.8474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 596/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3345 - accuracy: 0.8924 - val_loss: 0.4967 - val_accuracy: 0.8468\n",
      "Epoch 597/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3336 - accuracy: 0.8933 - val_loss: 0.4966 - val_accuracy: 0.8475\n",
      "Epoch 598/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3321 - accuracy: 0.8936 - val_loss: 0.4963 - val_accuracy: 0.8472\n",
      "Epoch 599/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3332 - accuracy: 0.8934 - val_loss: 0.4964 - val_accuracy: 0.8481\n",
      "Epoch 600/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3322 - accuracy: 0.8946 - val_loss: 0.4964 - val_accuracy: 0.8479\n",
      "Epoch 601/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3322 - accuracy: 0.8942 - val_loss: 0.4948 - val_accuracy: 0.8486\n",
      "Epoch 602/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3308 - accuracy: 0.8948 - val_loss: 0.4951 - val_accuracy: 0.8489\n",
      "Epoch 603/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3325 - accuracy: 0.8927 - val_loss: 0.4952 - val_accuracy: 0.8484\n",
      "Epoch 604/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3316 - accuracy: 0.8947 - val_loss: 0.4970 - val_accuracy: 0.8487\n",
      "Epoch 605/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3324 - accuracy: 0.8935 - val_loss: 0.4983 - val_accuracy: 0.8476\n",
      "Epoch 606/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3328 - accuracy: 0.8939 - val_loss: 0.4974 - val_accuracy: 0.8487\n",
      "Epoch 607/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3329 - accuracy: 0.8928 - val_loss: 0.4970 - val_accuracy: 0.8487\n",
      "Epoch 608/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3314 - accuracy: 0.8941 - val_loss: 0.4967 - val_accuracy: 0.8482\n",
      "Epoch 609/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3312 - accuracy: 0.8946 - val_loss: 0.4960 - val_accuracy: 0.8481\n",
      "Epoch 610/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3307 - accuracy: 0.8946 - val_loss: 0.4965 - val_accuracy: 0.8483\n",
      "Epoch 611/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3308 - accuracy: 0.8944 - val_loss: 0.4970 - val_accuracy: 0.8485\n",
      "Epoch 612/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3313 - accuracy: 0.8940 - val_loss: 0.4974 - val_accuracy: 0.8479\n",
      "Epoch 613/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3301 - accuracy: 0.8950 - val_loss: 0.4965 - val_accuracy: 0.8479\n",
      "Epoch 614/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3297 - accuracy: 0.8946 - val_loss: 0.4977 - val_accuracy: 0.8483\n",
      "Epoch 615/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3307 - accuracy: 0.8947 - val_loss: 0.4979 - val_accuracy: 0.8484\n",
      "Epoch 616/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3301 - accuracy: 0.8950 - val_loss: 0.4976 - val_accuracy: 0.8483\n",
      "Epoch 617/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3297 - accuracy: 0.8949 - val_loss: 0.4969 - val_accuracy: 0.8486\n",
      "Epoch 618/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3294 - accuracy: 0.8954 - val_loss: 0.4979 - val_accuracy: 0.8477\n",
      "Epoch 619/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3293 - accuracy: 0.8948 - val_loss: 0.4982 - val_accuracy: 0.8482\n",
      "Epoch 620/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3287 - accuracy: 0.8952 - val_loss: 0.4985 - val_accuracy: 0.8485\n",
      "Epoch 621/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3296 - accuracy: 0.8952 - val_loss: 0.4970 - val_accuracy: 0.8486\n",
      "Epoch 622/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3294 - accuracy: 0.8956 - val_loss: 0.4973 - val_accuracy: 0.8493\n",
      "Epoch 623/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3291 - accuracy: 0.8953 - val_loss: 0.4984 - val_accuracy: 0.8476\n",
      "Epoch 624/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3281 - accuracy: 0.8955 - val_loss: 0.4977 - val_accuracy: 0.8491\n",
      "Epoch 625/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3284 - accuracy: 0.8949 - val_loss: 0.4971 - val_accuracy: 0.8498\n",
      "Epoch 626/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3287 - accuracy: 0.8950 - val_loss: 0.4972 - val_accuracy: 0.8487\n",
      "Epoch 627/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3278 - accuracy: 0.8958 - val_loss: 0.4991 - val_accuracy: 0.8491\n",
      "Epoch 628/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3273 - accuracy: 0.8965 - val_loss: 0.4993 - val_accuracy: 0.8474\n",
      "Epoch 629/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3275 - accuracy: 0.8957 - val_loss: 0.4988 - val_accuracy: 0.8486\n",
      "Epoch 630/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3269 - accuracy: 0.8958 - val_loss: 0.4987 - val_accuracy: 0.8487\n",
      "Epoch 631/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3277 - accuracy: 0.8950 - val_loss: 0.4982 - val_accuracy: 0.8487\n",
      "Epoch 632/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3268 - accuracy: 0.8959 - val_loss: 0.4984 - val_accuracy: 0.8490\n",
      "Epoch 633/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3281 - accuracy: 0.8954 - val_loss: 0.4991 - val_accuracy: 0.8485\n",
      "Epoch 634/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3276 - accuracy: 0.8957 - val_loss: 0.4998 - val_accuracy: 0.8484\n",
      "Epoch 635/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3259 - accuracy: 0.8962 - val_loss: 0.5005 - val_accuracy: 0.8485\n",
      "Epoch 636/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3265 - accuracy: 0.8958 - val_loss: 0.4983 - val_accuracy: 0.8494\n",
      "Epoch 637/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3270 - accuracy: 0.8957 - val_loss: 0.4979 - val_accuracy: 0.8490\n",
      "Epoch 638/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3262 - accuracy: 0.8965 - val_loss: 0.4983 - val_accuracy: 0.8493\n",
      "Epoch 639/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3273 - accuracy: 0.8955 - val_loss: 0.4993 - val_accuracy: 0.8491\n",
      "Epoch 640/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3258 - accuracy: 0.8965 - val_loss: 0.4996 - val_accuracy: 0.8499\n",
      "Epoch 641/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3254 - accuracy: 0.8966 - val_loss: 0.4993 - val_accuracy: 0.8497\n",
      "Epoch 642/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3253 - accuracy: 0.8970 - val_loss: 0.4990 - val_accuracy: 0.8491\n",
      "Epoch 643/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3245 - accuracy: 0.8970 - val_loss: 0.4983 - val_accuracy: 0.8499\n",
      "Epoch 644/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3246 - accuracy: 0.8963 - val_loss: 0.4995 - val_accuracy: 0.8496\n",
      "Epoch 645/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3254 - accuracy: 0.8970 - val_loss: 0.4986 - val_accuracy: 0.8501\n",
      "Epoch 646/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3257 - accuracy: 0.8960 - val_loss: 0.5001 - val_accuracy: 0.8491\n",
      "Epoch 647/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3241 - accuracy: 0.8972 - val_loss: 0.4992 - val_accuracy: 0.8493\n",
      "Epoch 648/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3244 - accuracy: 0.8969 - val_loss: 0.4996 - val_accuracy: 0.8499\n",
      "Epoch 649/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3238 - accuracy: 0.8971 - val_loss: 0.4990 - val_accuracy: 0.8502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3264 - accuracy: 0.8963 - val_loss: 0.5016 - val_accuracy: 0.8490\n",
      "Epoch 651/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3232 - accuracy: 0.8978 - val_loss: 0.5012 - val_accuracy: 0.8487\n",
      "Epoch 652/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3241 - accuracy: 0.8972 - val_loss: 0.5011 - val_accuracy: 0.8499\n",
      "Epoch 653/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3231 - accuracy: 0.8976 - val_loss: 0.5002 - val_accuracy: 0.8488\n",
      "Epoch 654/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3231 - accuracy: 0.8979 - val_loss: 0.4996 - val_accuracy: 0.8500\n",
      "Epoch 655/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3252 - accuracy: 0.8964 - val_loss: 0.5015 - val_accuracy: 0.8493\n",
      "Epoch 656/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3231 - accuracy: 0.8975 - val_loss: 0.5002 - val_accuracy: 0.8495\n",
      "Epoch 657/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3226 - accuracy: 0.8977 - val_loss: 0.5015 - val_accuracy: 0.8489\n",
      "Epoch 658/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3229 - accuracy: 0.8977 - val_loss: 0.5014 - val_accuracy: 0.8494\n",
      "Epoch 659/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3224 - accuracy: 0.8976 - val_loss: 0.5010 - val_accuracy: 0.8500\n",
      "Epoch 660/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3226 - accuracy: 0.8976 - val_loss: 0.5009 - val_accuracy: 0.8499\n",
      "Epoch 661/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3225 - accuracy: 0.8975 - val_loss: 0.5017 - val_accuracy: 0.8492\n",
      "Epoch 662/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3228 - accuracy: 0.8978 - val_loss: 0.5007 - val_accuracy: 0.8489\n",
      "Epoch 663/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3223 - accuracy: 0.8974 - val_loss: 0.5015 - val_accuracy: 0.8489\n",
      "Epoch 664/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3222 - accuracy: 0.8975 - val_loss: 0.5012 - val_accuracy: 0.8480\n",
      "Epoch 665/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3223 - accuracy: 0.8981 - val_loss: 0.5011 - val_accuracy: 0.8497\n",
      "Epoch 666/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3218 - accuracy: 0.8979 - val_loss: 0.5006 - val_accuracy: 0.8485\n",
      "Epoch 667/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3217 - accuracy: 0.8979 - val_loss: 0.5007 - val_accuracy: 0.8496\n",
      "Epoch 668/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3215 - accuracy: 0.8978 - val_loss: 0.5040 - val_accuracy: 0.8484\n",
      "Epoch 669/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3213 - accuracy: 0.8978 - val_loss: 0.5026 - val_accuracy: 0.8494\n",
      "Epoch 670/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3213 - accuracy: 0.8985 - val_loss: 0.5005 - val_accuracy: 0.8508\n",
      "Epoch 671/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3202 - accuracy: 0.8985 - val_loss: 0.5031 - val_accuracy: 0.8484\n",
      "Epoch 672/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3202 - accuracy: 0.8994 - val_loss: 0.5020 - val_accuracy: 0.8501\n",
      "Epoch 673/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3217 - accuracy: 0.8979 - val_loss: 0.5031 - val_accuracy: 0.8498\n",
      "Epoch 674/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3209 - accuracy: 0.8982 - val_loss: 0.5031 - val_accuracy: 0.8503\n",
      "Epoch 675/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3208 - accuracy: 0.8984 - val_loss: 0.5044 - val_accuracy: 0.8500\n",
      "Epoch 676/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3200 - accuracy: 0.8986 - val_loss: 0.5025 - val_accuracy: 0.8502\n",
      "Epoch 677/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3202 - accuracy: 0.8989 - val_loss: 0.5034 - val_accuracy: 0.8495\n",
      "Epoch 678/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3196 - accuracy: 0.8990 - val_loss: 0.5028 - val_accuracy: 0.8496\n",
      "Epoch 679/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3190 - accuracy: 0.8986 - val_loss: 0.5043 - val_accuracy: 0.8497\n",
      "Epoch 680/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3206 - accuracy: 0.8985 - val_loss: 0.5045 - val_accuracy: 0.8500\n",
      "Epoch 681/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3204 - accuracy: 0.8985 - val_loss: 0.5031 - val_accuracy: 0.8503\n",
      "Epoch 682/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3199 - accuracy: 0.8985 - val_loss: 0.5043 - val_accuracy: 0.8496\n",
      "Epoch 683/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3187 - accuracy: 0.8992 - val_loss: 0.5035 - val_accuracy: 0.8495\n",
      "Epoch 684/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3197 - accuracy: 0.8992 - val_loss: 0.5044 - val_accuracy: 0.8496\n",
      "Epoch 685/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3186 - accuracy: 0.8995 - val_loss: 0.5042 - val_accuracy: 0.8490\n",
      "Epoch 686/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3182 - accuracy: 0.8993 - val_loss: 0.5032 - val_accuracy: 0.8492\n",
      "Epoch 687/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3185 - accuracy: 0.8994 - val_loss: 0.5041 - val_accuracy: 0.8502\n",
      "Epoch 688/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3178 - accuracy: 0.8991 - val_loss: 0.5033 - val_accuracy: 0.8495\n",
      "Epoch 689/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3176 - accuracy: 0.9001 - val_loss: 0.5047 - val_accuracy: 0.8497\n",
      "Epoch 690/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3189 - accuracy: 0.8992 - val_loss: 0.5045 - val_accuracy: 0.8504\n",
      "Epoch 691/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3184 - accuracy: 0.8990 - val_loss: 0.5032 - val_accuracy: 0.8504\n",
      "Epoch 692/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3177 - accuracy: 0.8994 - val_loss: 0.5041 - val_accuracy: 0.8508\n",
      "Epoch 693/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3171 - accuracy: 0.9001 - val_loss: 0.5044 - val_accuracy: 0.8508\n",
      "Epoch 694/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3180 - accuracy: 0.8992 - val_loss: 0.5045 - val_accuracy: 0.8496\n",
      "Epoch 695/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3175 - accuracy: 0.9000 - val_loss: 0.5061 - val_accuracy: 0.8498\n",
      "Epoch 696/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3176 - accuracy: 0.8998 - val_loss: 0.5052 - val_accuracy: 0.8505\n",
      "Epoch 697/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3176 - accuracy: 0.8997 - val_loss: 0.5064 - val_accuracy: 0.8498\n",
      "Epoch 698/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3184 - accuracy: 0.9000 - val_loss: 0.5068 - val_accuracy: 0.8504\n",
      "Epoch 699/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3157 - accuracy: 0.9006 - val_loss: 0.5058 - val_accuracy: 0.8502\n",
      "Epoch 700/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3181 - accuracy: 0.8992 - val_loss: 0.5044 - val_accuracy: 0.8504\n",
      "Epoch 701/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3164 - accuracy: 0.8999 - val_loss: 0.5051 - val_accuracy: 0.8499\n",
      "Epoch 702/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3168 - accuracy: 0.9005 - val_loss: 0.5052 - val_accuracy: 0.8501\n",
      "Epoch 703/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3155 - accuracy: 0.9009 - val_loss: 0.5045 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 704/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3153 - accuracy: 0.9009 - val_loss: 0.5036 - val_accuracy: 0.8508\n",
      "Epoch 705/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3151 - accuracy: 0.9004 - val_loss: 0.5045 - val_accuracy: 0.8501\n",
      "Epoch 706/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3165 - accuracy: 0.9001 - val_loss: 0.5062 - val_accuracy: 0.8499\n",
      "Epoch 707/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3153 - accuracy: 0.9004 - val_loss: 0.5074 - val_accuracy: 0.8495\n",
      "Epoch 708/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3156 - accuracy: 0.9011 - val_loss: 0.5063 - val_accuracy: 0.8501\n",
      "Epoch 709/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3153 - accuracy: 0.9001 - val_loss: 0.5070 - val_accuracy: 0.8500\n",
      "Epoch 710/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3157 - accuracy: 0.9007 - val_loss: 0.5056 - val_accuracy: 0.8516\n",
      "Epoch 711/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3152 - accuracy: 0.9009 - val_loss: 0.5063 - val_accuracy: 0.8506\n",
      "Epoch 712/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3152 - accuracy: 0.9005 - val_loss: 0.5075 - val_accuracy: 0.8506\n",
      "Epoch 713/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3154 - accuracy: 0.9001 - val_loss: 0.5073 - val_accuracy: 0.8502\n",
      "Epoch 714/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3140 - accuracy: 0.9013 - val_loss: 0.5063 - val_accuracy: 0.8509\n",
      "Epoch 715/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3149 - accuracy: 0.9013 - val_loss: 0.5069 - val_accuracy: 0.8508\n",
      "Epoch 716/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3150 - accuracy: 0.9007 - val_loss: 0.5067 - val_accuracy: 0.8503\n",
      "Epoch 717/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3151 - accuracy: 0.9009 - val_loss: 0.5064 - val_accuracy: 0.8499\n",
      "Epoch 718/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3144 - accuracy: 0.9002 - val_loss: 0.5072 - val_accuracy: 0.8506\n",
      "Epoch 719/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3137 - accuracy: 0.9012 - val_loss: 0.5057 - val_accuracy: 0.8506\n",
      "Epoch 720/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3147 - accuracy: 0.9006 - val_loss: 0.5078 - val_accuracy: 0.8504\n",
      "Epoch 721/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3144 - accuracy: 0.9011 - val_loss: 0.5083 - val_accuracy: 0.8510\n",
      "Epoch 722/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3131 - accuracy: 0.9011 - val_loss: 0.5076 - val_accuracy: 0.8504\n",
      "Epoch 723/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3131 - accuracy: 0.9015 - val_loss: 0.5068 - val_accuracy: 0.8506\n",
      "Epoch 724/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3131 - accuracy: 0.9016 - val_loss: 0.5078 - val_accuracy: 0.8503\n",
      "Epoch 725/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3135 - accuracy: 0.9019 - val_loss: 0.5063 - val_accuracy: 0.8518\n",
      "Epoch 726/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3128 - accuracy: 0.9020 - val_loss: 0.5071 - val_accuracy: 0.8507\n",
      "Epoch 727/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3126 - accuracy: 0.9014 - val_loss: 0.5084 - val_accuracy: 0.8503\n",
      "Epoch 728/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3129 - accuracy: 0.9017 - val_loss: 0.5072 - val_accuracy: 0.8511\n",
      "Epoch 729/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3134 - accuracy: 0.9013 - val_loss: 0.5087 - val_accuracy: 0.8506\n",
      "Epoch 730/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3119 - accuracy: 0.9021 - val_loss: 0.5070 - val_accuracy: 0.8513\n",
      "Epoch 731/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3117 - accuracy: 0.9019 - val_loss: 0.5078 - val_accuracy: 0.8513\n",
      "Epoch 732/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3119 - accuracy: 0.9018 - val_loss: 0.5087 - val_accuracy: 0.8507\n",
      "Epoch 733/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3121 - accuracy: 0.9017 - val_loss: 0.5063 - val_accuracy: 0.8522\n",
      "Epoch 734/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3121 - accuracy: 0.9016 - val_loss: 0.5082 - val_accuracy: 0.8511\n",
      "Epoch 735/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3122 - accuracy: 0.9016 - val_loss: 0.5081 - val_accuracy: 0.8519\n",
      "Epoch 736/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3117 - accuracy: 0.9016 - val_loss: 0.5083 - val_accuracy: 0.8513\n",
      "Epoch 737/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3120 - accuracy: 0.9014 - val_loss: 0.5085 - val_accuracy: 0.8521\n",
      "Epoch 738/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3118 - accuracy: 0.9019 - val_loss: 0.5078 - val_accuracy: 0.8513\n",
      "Epoch 739/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3109 - accuracy: 0.9023 - val_loss: 0.5091 - val_accuracy: 0.8501\n",
      "Epoch 740/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3114 - accuracy: 0.9023 - val_loss: 0.5092 - val_accuracy: 0.8515\n",
      "Epoch 741/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3118 - accuracy: 0.9019 - val_loss: 0.5090 - val_accuracy: 0.8517\n",
      "Epoch 742/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3107 - accuracy: 0.9022 - val_loss: 0.5092 - val_accuracy: 0.8509\n",
      "Epoch 743/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3117 - accuracy: 0.9019 - val_loss: 0.5084 - val_accuracy: 0.8518\n",
      "Epoch 744/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3115 - accuracy: 0.9018 - val_loss: 0.5102 - val_accuracy: 0.8518\n",
      "Epoch 745/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3103 - accuracy: 0.9027 - val_loss: 0.5092 - val_accuracy: 0.8517\n",
      "Epoch 746/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3113 - accuracy: 0.9023 - val_loss: 0.5091 - val_accuracy: 0.8522\n",
      "Epoch 747/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3104 - accuracy: 0.9027 - val_loss: 0.5100 - val_accuracy: 0.8509\n",
      "Epoch 748/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3106 - accuracy: 0.9026 - val_loss: 0.5110 - val_accuracy: 0.8508\n",
      "Epoch 749/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3104 - accuracy: 0.9026 - val_loss: 0.5084 - val_accuracy: 0.8508\n",
      "Epoch 750/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3098 - accuracy: 0.9027 - val_loss: 0.5082 - val_accuracy: 0.8510\n",
      "Epoch 751/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3103 - accuracy: 0.9025 - val_loss: 0.5105 - val_accuracy: 0.8517\n",
      "Epoch 752/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3106 - accuracy: 0.9024 - val_loss: 0.5094 - val_accuracy: 0.8517\n",
      "Epoch 753/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3096 - accuracy: 0.9030 - val_loss: 0.5091 - val_accuracy: 0.8519\n",
      "Epoch 754/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3092 - accuracy: 0.9028 - val_loss: 0.5107 - val_accuracy: 0.8511\n",
      "Epoch 755/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3090 - accuracy: 0.9030 - val_loss: 0.5099 - val_accuracy: 0.8507\n",
      "Epoch 756/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3090 - accuracy: 0.9034 - val_loss: 0.5103 - val_accuracy: 0.8512\n",
      "Epoch 757/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3088 - accuracy: 0.9032 - val_loss: 0.5108 - val_accuracy: 0.8510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 758/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3094 - accuracy: 0.9032 - val_loss: 0.5095 - val_accuracy: 0.8515\n",
      "Epoch 759/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3094 - accuracy: 0.9030 - val_loss: 0.5104 - val_accuracy: 0.8519\n",
      "Epoch 760/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3088 - accuracy: 0.9029 - val_loss: 0.5104 - val_accuracy: 0.8516\n",
      "Epoch 761/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3094 - accuracy: 0.9028 - val_loss: 0.5114 - val_accuracy: 0.8519\n",
      "Epoch 762/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3087 - accuracy: 0.9031 - val_loss: 0.5114 - val_accuracy: 0.8506\n",
      "Epoch 763/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3087 - accuracy: 0.9029 - val_loss: 0.5107 - val_accuracy: 0.8517\n",
      "Epoch 764/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3079 - accuracy: 0.9036 - val_loss: 0.5110 - val_accuracy: 0.8509\n",
      "Epoch 765/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3077 - accuracy: 0.9036 - val_loss: 0.5109 - val_accuracy: 0.8511\n",
      "Epoch 766/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3081 - accuracy: 0.9027 - val_loss: 0.5098 - val_accuracy: 0.8512\n",
      "Epoch 767/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3077 - accuracy: 0.9038 - val_loss: 0.5117 - val_accuracy: 0.8522\n",
      "Epoch 768/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3079 - accuracy: 0.9039 - val_loss: 0.5123 - val_accuracy: 0.8518\n",
      "Epoch 769/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3084 - accuracy: 0.9032 - val_loss: 0.5110 - val_accuracy: 0.8522\n",
      "Epoch 770/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3077 - accuracy: 0.9037 - val_loss: 0.5114 - val_accuracy: 0.8507\n",
      "Epoch 771/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3074 - accuracy: 0.9035 - val_loss: 0.5132 - val_accuracy: 0.8507\n",
      "Epoch 772/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3072 - accuracy: 0.9036 - val_loss: 0.5116 - val_accuracy: 0.8514\n",
      "Epoch 773/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3075 - accuracy: 0.9034 - val_loss: 0.5124 - val_accuracy: 0.8517\n",
      "Epoch 774/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3077 - accuracy: 0.9033 - val_loss: 0.5112 - val_accuracy: 0.8525\n",
      "Epoch 775/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3071 - accuracy: 0.9039 - val_loss: 0.5124 - val_accuracy: 0.8514\n",
      "Epoch 776/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3076 - accuracy: 0.9037 - val_loss: 0.5111 - val_accuracy: 0.8515\n",
      "Epoch 777/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3065 - accuracy: 0.9045 - val_loss: 0.5122 - val_accuracy: 0.8523\n",
      "Epoch 778/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3071 - accuracy: 0.9038 - val_loss: 0.5105 - val_accuracy: 0.8519\n",
      "Epoch 779/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3058 - accuracy: 0.9040 - val_loss: 0.5111 - val_accuracy: 0.8515\n",
      "Epoch 780/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3067 - accuracy: 0.9039 - val_loss: 0.5124 - val_accuracy: 0.8519\n",
      "Epoch 781/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3069 - accuracy: 0.9038 - val_loss: 0.5134 - val_accuracy: 0.8520\n",
      "Epoch 782/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3067 - accuracy: 0.9040 - val_loss: 0.5121 - val_accuracy: 0.8521\n",
      "Epoch 783/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3062 - accuracy: 0.9037 - val_loss: 0.5132 - val_accuracy: 0.8520\n",
      "Epoch 784/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3065 - accuracy: 0.9037 - val_loss: 0.5132 - val_accuracy: 0.8519\n",
      "Epoch 785/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3059 - accuracy: 0.9042 - val_loss: 0.5131 - val_accuracy: 0.8515\n",
      "Epoch 786/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3063 - accuracy: 0.9042 - val_loss: 0.5112 - val_accuracy: 0.8523\n",
      "Epoch 787/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3041 - accuracy: 0.9057 - val_loss: 0.5131 - val_accuracy: 0.8519\n",
      "Epoch 788/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3059 - accuracy: 0.9046 - val_loss: 0.5137 - val_accuracy: 0.8525\n",
      "Epoch 789/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3050 - accuracy: 0.9047 - val_loss: 0.5116 - val_accuracy: 0.8524\n",
      "Epoch 790/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3056 - accuracy: 0.9036 - val_loss: 0.5127 - val_accuracy: 0.8514\n",
      "Epoch 791/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3049 - accuracy: 0.9042 - val_loss: 0.5136 - val_accuracy: 0.8514\n",
      "Epoch 792/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3043 - accuracy: 0.9054 - val_loss: 0.5136 - val_accuracy: 0.8525\n",
      "Epoch 793/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3046 - accuracy: 0.9046 - val_loss: 0.5135 - val_accuracy: 0.8517\n",
      "Epoch 794/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3046 - accuracy: 0.9045 - val_loss: 0.5129 - val_accuracy: 0.8522\n",
      "Epoch 795/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3041 - accuracy: 0.9050 - val_loss: 0.5145 - val_accuracy: 0.8519\n",
      "Epoch 796/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3057 - accuracy: 0.9042 - val_loss: 0.5124 - val_accuracy: 0.8519\n",
      "Epoch 797/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3038 - accuracy: 0.9049 - val_loss: 0.5152 - val_accuracy: 0.8521\n",
      "Epoch 798/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3042 - accuracy: 0.9051 - val_loss: 0.5139 - val_accuracy: 0.8516\n",
      "Epoch 799/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3035 - accuracy: 0.9046 - val_loss: 0.5136 - val_accuracy: 0.8528\n",
      "Epoch 800/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3035 - accuracy: 0.9054 - val_loss: 0.5143 - val_accuracy: 0.8525\n",
      "Epoch 801/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3028 - accuracy: 0.9054 - val_loss: 0.5142 - val_accuracy: 0.8530\n",
      "Epoch 802/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3046 - accuracy: 0.9051 - val_loss: 0.5117 - val_accuracy: 0.8537\n",
      "Epoch 803/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3029 - accuracy: 0.9056 - val_loss: 0.5130 - val_accuracy: 0.8533\n",
      "Epoch 804/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3026 - accuracy: 0.9056 - val_loss: 0.5138 - val_accuracy: 0.8524\n",
      "Epoch 805/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3032 - accuracy: 0.9058 - val_loss: 0.5153 - val_accuracy: 0.8529\n",
      "Epoch 806/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3032 - accuracy: 0.9054 - val_loss: 0.5161 - val_accuracy: 0.8523\n",
      "Epoch 807/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3026 - accuracy: 0.9060 - val_loss: 0.5151 - val_accuracy: 0.8524\n",
      "Epoch 808/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3034 - accuracy: 0.9048 - val_loss: 0.5149 - val_accuracy: 0.8533\n",
      "Epoch 809/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3022 - accuracy: 0.9059 - val_loss: 0.5156 - val_accuracy: 0.8531\n",
      "Epoch 810/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3016 - accuracy: 0.9059 - val_loss: 0.5147 - val_accuracy: 0.8527\n",
      "Epoch 811/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3021 - accuracy: 0.9053 - val_loss: 0.5146 - val_accuracy: 0.8527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3025 - accuracy: 0.9053 - val_loss: 0.5155 - val_accuracy: 0.8529\n",
      "Epoch 813/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3035 - accuracy: 0.9054 - val_loss: 0.5148 - val_accuracy: 0.8526\n",
      "Epoch 814/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3021 - accuracy: 0.9054 - val_loss: 0.5176 - val_accuracy: 0.8515\n",
      "Epoch 815/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3001 - accuracy: 0.9070 - val_loss: 0.5146 - val_accuracy: 0.8535\n",
      "Epoch 816/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3021 - accuracy: 0.9051 - val_loss: 0.5157 - val_accuracy: 0.8526\n",
      "Epoch 817/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3024 - accuracy: 0.9050 - val_loss: 0.5161 - val_accuracy: 0.8543\n",
      "Epoch 818/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3021 - accuracy: 0.9058 - val_loss: 0.5179 - val_accuracy: 0.8528\n",
      "Epoch 819/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3021 - accuracy: 0.9059 - val_loss: 0.5161 - val_accuracy: 0.8530\n",
      "Epoch 820/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3009 - accuracy: 0.9065 - val_loss: 0.5147 - val_accuracy: 0.8540\n",
      "Epoch 821/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3011 - accuracy: 0.9063 - val_loss: 0.5155 - val_accuracy: 0.8540\n",
      "Epoch 822/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3010 - accuracy: 0.9060 - val_loss: 0.5159 - val_accuracy: 0.8528\n",
      "Epoch 823/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3020 - accuracy: 0.9063 - val_loss: 0.5167 - val_accuracy: 0.8535\n",
      "Epoch 824/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3010 - accuracy: 0.9064 - val_loss: 0.5176 - val_accuracy: 0.8525\n",
      "Epoch 825/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3012 - accuracy: 0.9064 - val_loss: 0.5177 - val_accuracy: 0.8527\n",
      "Epoch 826/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3022 - accuracy: 0.9054 - val_loss: 0.5174 - val_accuracy: 0.8534\n",
      "Epoch 827/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3000 - accuracy: 0.9068 - val_loss: 0.5171 - val_accuracy: 0.8528\n",
      "Epoch 828/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3008 - accuracy: 0.9058 - val_loss: 0.5182 - val_accuracy: 0.8523\n",
      "Epoch 829/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3018 - accuracy: 0.9059 - val_loss: 0.5183 - val_accuracy: 0.8529\n",
      "Epoch 830/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3001 - accuracy: 0.9064 - val_loss: 0.5172 - val_accuracy: 0.8526\n",
      "Epoch 831/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3003 - accuracy: 0.9062 - val_loss: 0.5178 - val_accuracy: 0.8528\n",
      "Epoch 832/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2995 - accuracy: 0.9066 - val_loss: 0.5180 - val_accuracy: 0.8522\n",
      "Epoch 833/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3005 - accuracy: 0.9070 - val_loss: 0.5180 - val_accuracy: 0.8522\n",
      "Epoch 834/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3002 - accuracy: 0.9065 - val_loss: 0.5172 - val_accuracy: 0.8529\n",
      "Epoch 835/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2995 - accuracy: 0.9068 - val_loss: 0.5187 - val_accuracy: 0.8532\n",
      "Epoch 836/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2996 - accuracy: 0.9066 - val_loss: 0.5170 - val_accuracy: 0.8533\n",
      "Epoch 837/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3006 - accuracy: 0.9063 - val_loss: 0.5175 - val_accuracy: 0.8532\n",
      "Epoch 838/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.3001 - accuracy: 0.9066 - val_loss: 0.5187 - val_accuracy: 0.8522\n",
      "Epoch 839/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2996 - accuracy: 0.9069 - val_loss: 0.5181 - val_accuracy: 0.8536\n",
      "Epoch 840/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2988 - accuracy: 0.9073 - val_loss: 0.5171 - val_accuracy: 0.8538\n",
      "Epoch 841/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2994 - accuracy: 0.9069 - val_loss: 0.5176 - val_accuracy: 0.8539\n",
      "Epoch 842/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2995 - accuracy: 0.9066 - val_loss: 0.5167 - val_accuracy: 0.8530\n",
      "Epoch 843/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2987 - accuracy: 0.9074 - val_loss: 0.5190 - val_accuracy: 0.8535\n",
      "Epoch 844/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2985 - accuracy: 0.9072 - val_loss: 0.5191 - val_accuracy: 0.8533\n",
      "Epoch 845/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2995 - accuracy: 0.9062 - val_loss: 0.5190 - val_accuracy: 0.8528\n",
      "Epoch 846/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2998 - accuracy: 0.9073 - val_loss: 0.5203 - val_accuracy: 0.8536\n",
      "Epoch 847/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2988 - accuracy: 0.9068 - val_loss: 0.5200 - val_accuracy: 0.8539\n",
      "Epoch 848/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2986 - accuracy: 0.9070 - val_loss: 0.5190 - val_accuracy: 0.8535\n",
      "Epoch 849/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2983 - accuracy: 0.9077 - val_loss: 0.5183 - val_accuracy: 0.8534\n",
      "Epoch 850/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2995 - accuracy: 0.9064 - val_loss: 0.5200 - val_accuracy: 0.8535\n",
      "Epoch 851/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2980 - accuracy: 0.9078 - val_loss: 0.5184 - val_accuracy: 0.8538\n",
      "Epoch 852/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2982 - accuracy: 0.9072 - val_loss: 0.5183 - val_accuracy: 0.8552\n",
      "Epoch 853/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2974 - accuracy: 0.9075 - val_loss: 0.5173 - val_accuracy: 0.8543\n",
      "Epoch 854/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2977 - accuracy: 0.9074 - val_loss: 0.5198 - val_accuracy: 0.8534\n",
      "Epoch 855/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2980 - accuracy: 0.9072 - val_loss: 0.5199 - val_accuracy: 0.8531\n",
      "Epoch 856/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2986 - accuracy: 0.9074 - val_loss: 0.5206 - val_accuracy: 0.8543\n",
      "Epoch 857/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2973 - accuracy: 0.9076 - val_loss: 0.5204 - val_accuracy: 0.8526\n",
      "Epoch 858/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2980 - accuracy: 0.9072 - val_loss: 0.5188 - val_accuracy: 0.8532\n",
      "Epoch 859/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2966 - accuracy: 0.9076 - val_loss: 0.5192 - val_accuracy: 0.8536\n",
      "Epoch 860/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2975 - accuracy: 0.9072 - val_loss: 0.5194 - val_accuracy: 0.8529\n",
      "Epoch 861/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2962 - accuracy: 0.9085 - val_loss: 0.5204 - val_accuracy: 0.8532\n",
      "Epoch 862/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2977 - accuracy: 0.9077 - val_loss: 0.5200 - val_accuracy: 0.8529\n",
      "Epoch 863/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2970 - accuracy: 0.9081 - val_loss: 0.5205 - val_accuracy: 0.8536\n",
      "Epoch 864/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2967 - accuracy: 0.9071 - val_loss: 0.5215 - val_accuracy: 0.8536\n",
      "Epoch 865/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2961 - accuracy: 0.9083 - val_loss: 0.5212 - val_accuracy: 0.8533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 866/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2960 - accuracy: 0.9080 - val_loss: 0.5202 - val_accuracy: 0.8536\n",
      "Epoch 867/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2964 - accuracy: 0.9079 - val_loss: 0.5210 - val_accuracy: 0.8534\n",
      "Epoch 868/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2960 - accuracy: 0.9080 - val_loss: 0.5204 - val_accuracy: 0.8546\n",
      "Epoch 869/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2953 - accuracy: 0.9087 - val_loss: 0.5206 - val_accuracy: 0.8539\n",
      "Epoch 870/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2960 - accuracy: 0.9087 - val_loss: 0.5208 - val_accuracy: 0.8545\n",
      "Epoch 871/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2961 - accuracy: 0.9083 - val_loss: 0.5224 - val_accuracy: 0.8533\n",
      "Epoch 872/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2972 - accuracy: 0.9078 - val_loss: 0.5212 - val_accuracy: 0.8544\n",
      "Epoch 873/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2961 - accuracy: 0.9081 - val_loss: 0.5217 - val_accuracy: 0.8531\n",
      "Epoch 874/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2965 - accuracy: 0.9076 - val_loss: 0.5219 - val_accuracy: 0.8534\n",
      "Epoch 875/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2958 - accuracy: 0.9080 - val_loss: 0.5219 - val_accuracy: 0.8536\n",
      "Epoch 876/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2958 - accuracy: 0.9080 - val_loss: 0.5222 - val_accuracy: 0.8542\n",
      "Epoch 877/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2939 - accuracy: 0.9091 - val_loss: 0.5211 - val_accuracy: 0.8546\n",
      "Epoch 878/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2957 - accuracy: 0.9076 - val_loss: 0.5221 - val_accuracy: 0.8538\n",
      "Epoch 879/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2966 - accuracy: 0.9082 - val_loss: 0.5231 - val_accuracy: 0.8542\n",
      "Epoch 880/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2964 - accuracy: 0.9075 - val_loss: 0.5234 - val_accuracy: 0.8546\n",
      "Epoch 881/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2952 - accuracy: 0.9083 - val_loss: 0.5219 - val_accuracy: 0.8534\n",
      "Epoch 882/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2959 - accuracy: 0.9079 - val_loss: 0.5215 - val_accuracy: 0.8535\n",
      "Epoch 883/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2947 - accuracy: 0.9089 - val_loss: 0.5221 - val_accuracy: 0.8536\n",
      "Epoch 884/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2956 - accuracy: 0.9081 - val_loss: 0.5238 - val_accuracy: 0.8528\n",
      "Epoch 885/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2949 - accuracy: 0.9088 - val_loss: 0.5231 - val_accuracy: 0.8544\n",
      "Epoch 886/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2936 - accuracy: 0.9093 - val_loss: 0.5234 - val_accuracy: 0.8549\n",
      "Epoch 887/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2942 - accuracy: 0.9088 - val_loss: 0.5215 - val_accuracy: 0.8542\n",
      "Epoch 888/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2938 - accuracy: 0.9091 - val_loss: 0.5213 - val_accuracy: 0.8547\n",
      "Epoch 889/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2937 - accuracy: 0.9091 - val_loss: 0.5223 - val_accuracy: 0.8547\n",
      "Epoch 890/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2937 - accuracy: 0.9091 - val_loss: 0.5233 - val_accuracy: 0.8541\n",
      "Epoch 891/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2944 - accuracy: 0.9083 - val_loss: 0.5239 - val_accuracy: 0.8540\n",
      "Epoch 892/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2958 - accuracy: 0.9081 - val_loss: 0.5221 - val_accuracy: 0.8540\n",
      "Epoch 893/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2936 - accuracy: 0.9093 - val_loss: 0.5229 - val_accuracy: 0.8540\n",
      "Epoch 894/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2948 - accuracy: 0.9081 - val_loss: 0.5245 - val_accuracy: 0.8544\n",
      "Epoch 895/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2939 - accuracy: 0.9091 - val_loss: 0.5255 - val_accuracy: 0.8540\n",
      "Epoch 896/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2936 - accuracy: 0.9091 - val_loss: 0.5240 - val_accuracy: 0.8557\n",
      "Epoch 897/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2939 - accuracy: 0.9086 - val_loss: 0.5242 - val_accuracy: 0.8540\n",
      "Epoch 898/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2948 - accuracy: 0.9088 - val_loss: 0.5247 - val_accuracy: 0.8544\n",
      "Epoch 899/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2925 - accuracy: 0.9090 - val_loss: 0.5256 - val_accuracy: 0.8536\n",
      "Epoch 900/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2927 - accuracy: 0.9097 - val_loss: 0.5265 - val_accuracy: 0.8538\n",
      "Epoch 901/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2927 - accuracy: 0.9094 - val_loss: 0.5239 - val_accuracy: 0.8549\n",
      "Epoch 902/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2934 - accuracy: 0.9090 - val_loss: 0.5256 - val_accuracy: 0.8542\n",
      "Epoch 903/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2933 - accuracy: 0.9092 - val_loss: 0.5248 - val_accuracy: 0.8552\n",
      "Epoch 904/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2929 - accuracy: 0.9099 - val_loss: 0.5241 - val_accuracy: 0.8546\n",
      "Epoch 905/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2919 - accuracy: 0.9100 - val_loss: 0.5239 - val_accuracy: 0.8542\n",
      "Epoch 906/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2930 - accuracy: 0.9086 - val_loss: 0.5237 - val_accuracy: 0.8546\n",
      "Epoch 907/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2919 - accuracy: 0.9098 - val_loss: 0.5250 - val_accuracy: 0.8541\n",
      "Epoch 908/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2923 - accuracy: 0.9092 - val_loss: 0.5253 - val_accuracy: 0.8545\n",
      "Epoch 909/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2918 - accuracy: 0.9099 - val_loss: 0.5269 - val_accuracy: 0.8538\n",
      "Epoch 910/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2924 - accuracy: 0.9097 - val_loss: 0.5256 - val_accuracy: 0.8545\n",
      "Epoch 911/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2916 - accuracy: 0.9096 - val_loss: 0.5264 - val_accuracy: 0.8542\n",
      "Epoch 912/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2918 - accuracy: 0.9099 - val_loss: 0.5271 - val_accuracy: 0.8548\n",
      "Epoch 913/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2919 - accuracy: 0.9100 - val_loss: 0.5265 - val_accuracy: 0.8550\n",
      "Epoch 914/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2916 - accuracy: 0.9099 - val_loss: 0.5265 - val_accuracy: 0.8541\n",
      "Epoch 915/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2921 - accuracy: 0.9091 - val_loss: 0.5250 - val_accuracy: 0.8541\n",
      "Epoch 916/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2924 - accuracy: 0.9095 - val_loss: 0.5251 - val_accuracy: 0.8551\n",
      "Epoch 917/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2916 - accuracy: 0.9097 - val_loss: 0.5261 - val_accuracy: 0.8552\n",
      "Epoch 918/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2926 - accuracy: 0.9096 - val_loss: 0.5252 - val_accuracy: 0.8542\n",
      "Epoch 919/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2908 - accuracy: 0.9100 - val_loss: 0.5275 - val_accuracy: 0.8538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 920/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2909 - accuracy: 0.9103 - val_loss: 0.5282 - val_accuracy: 0.8541\n",
      "Epoch 921/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2908 - accuracy: 0.9103 - val_loss: 0.5244 - val_accuracy: 0.8544\n",
      "Epoch 922/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2900 - accuracy: 0.9102 - val_loss: 0.5256 - val_accuracy: 0.8546\n",
      "Epoch 923/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2908 - accuracy: 0.9104 - val_loss: 0.5270 - val_accuracy: 0.8535\n",
      "Epoch 924/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2905 - accuracy: 0.9103 - val_loss: 0.5255 - val_accuracy: 0.8545\n",
      "Epoch 925/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2912 - accuracy: 0.9097 - val_loss: 0.5266 - val_accuracy: 0.8544\n",
      "Epoch 926/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2911 - accuracy: 0.9095 - val_loss: 0.5268 - val_accuracy: 0.8555\n",
      "Epoch 927/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2899 - accuracy: 0.9106 - val_loss: 0.5253 - val_accuracy: 0.8543\n",
      "Epoch 928/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2908 - accuracy: 0.9100 - val_loss: 0.5252 - val_accuracy: 0.8540\n",
      "Epoch 929/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2908 - accuracy: 0.9102 - val_loss: 0.5268 - val_accuracy: 0.8545\n",
      "Epoch 930/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2909 - accuracy: 0.9100 - val_loss: 0.5266 - val_accuracy: 0.8544\n",
      "Epoch 931/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2902 - accuracy: 0.9103 - val_loss: 0.5271 - val_accuracy: 0.8548\n",
      "Epoch 932/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2893 - accuracy: 0.9115 - val_loss: 0.5253 - val_accuracy: 0.8553\n",
      "Epoch 933/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2894 - accuracy: 0.9107 - val_loss: 0.5273 - val_accuracy: 0.8553\n",
      "Epoch 934/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2903 - accuracy: 0.9105 - val_loss: 0.5299 - val_accuracy: 0.8540\n",
      "Epoch 935/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2891 - accuracy: 0.9104 - val_loss: 0.5274 - val_accuracy: 0.8550\n",
      "Epoch 936/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2901 - accuracy: 0.9103 - val_loss: 0.5287 - val_accuracy: 0.8548\n",
      "Epoch 937/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2897 - accuracy: 0.9106 - val_loss: 0.5286 - val_accuracy: 0.8553\n",
      "Epoch 938/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2899 - accuracy: 0.9099 - val_loss: 0.5280 - val_accuracy: 0.8545\n",
      "Epoch 939/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2890 - accuracy: 0.9108 - val_loss: 0.5281 - val_accuracy: 0.8539\n",
      "Epoch 940/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2896 - accuracy: 0.9104 - val_loss: 0.5283 - val_accuracy: 0.8548\n",
      "Epoch 941/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2887 - accuracy: 0.9110 - val_loss: 0.5280 - val_accuracy: 0.8544\n",
      "Epoch 942/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2899 - accuracy: 0.9104 - val_loss: 0.5290 - val_accuracy: 0.8548\n",
      "Epoch 943/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2896 - accuracy: 0.9102 - val_loss: 0.5276 - val_accuracy: 0.8558\n",
      "Epoch 944/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2893 - accuracy: 0.9108 - val_loss: 0.5291 - val_accuracy: 0.8547\n",
      "Epoch 945/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2891 - accuracy: 0.9116 - val_loss: 0.5281 - val_accuracy: 0.8550\n",
      "Epoch 946/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2883 - accuracy: 0.9109 - val_loss: 0.5279 - val_accuracy: 0.8551\n",
      "Epoch 947/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2878 - accuracy: 0.9115 - val_loss: 0.5287 - val_accuracy: 0.8540\n",
      "Epoch 948/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2880 - accuracy: 0.9112 - val_loss: 0.5309 - val_accuracy: 0.8542\n",
      "Epoch 949/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.9116 - val_loss: 0.5291 - val_accuracy: 0.8551\n",
      "Epoch 950/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2895 - accuracy: 0.9105 - val_loss: 0.5287 - val_accuracy: 0.8550\n",
      "Epoch 951/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2886 - accuracy: 0.9112 - val_loss: 0.5292 - val_accuracy: 0.8552\n",
      "Epoch 952/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2883 - accuracy: 0.9112 - val_loss: 0.5303 - val_accuracy: 0.8552\n",
      "Epoch 953/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2888 - accuracy: 0.9108 - val_loss: 0.5322 - val_accuracy: 0.8550\n",
      "Epoch 954/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2893 - accuracy: 0.9104 - val_loss: 0.5310 - val_accuracy: 0.8552\n",
      "Epoch 955/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2874 - accuracy: 0.9115 - val_loss: 0.5297 - val_accuracy: 0.8554\n",
      "Epoch 956/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2882 - accuracy: 0.9108 - val_loss: 0.5295 - val_accuracy: 0.8542\n",
      "Epoch 957/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2880 - accuracy: 0.9116 - val_loss: 0.5299 - val_accuracy: 0.8547\n",
      "Epoch 958/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2884 - accuracy: 0.9109 - val_loss: 0.5309 - val_accuracy: 0.8548\n",
      "Epoch 959/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.9109 - val_loss: 0.5288 - val_accuracy: 0.8557\n",
      "Epoch 960/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2881 - accuracy: 0.9108 - val_loss: 0.5295 - val_accuracy: 0.8553\n",
      "Epoch 961/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2868 - accuracy: 0.9118 - val_loss: 0.5293 - val_accuracy: 0.8562\n",
      "Epoch 962/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.9114 - val_loss: 0.5311 - val_accuracy: 0.8558\n",
      "Epoch 963/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2874 - accuracy: 0.9112 - val_loss: 0.5299 - val_accuracy: 0.8560\n",
      "Epoch 964/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2864 - accuracy: 0.9114 - val_loss: 0.5296 - val_accuracy: 0.8553\n",
      "Epoch 965/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2876 - accuracy: 0.9113 - val_loss: 0.5293 - val_accuracy: 0.8557\n",
      "Epoch 966/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.9109 - val_loss: 0.5301 - val_accuracy: 0.8548\n",
      "Epoch 967/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2883 - accuracy: 0.9111 - val_loss: 0.5316 - val_accuracy: 0.8553\n",
      "Epoch 968/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2883 - accuracy: 0.9109 - val_loss: 0.5311 - val_accuracy: 0.8555\n",
      "Epoch 969/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2859 - accuracy: 0.9125 - val_loss: 0.5319 - val_accuracy: 0.8547\n",
      "Epoch 970/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2862 - accuracy: 0.9118 - val_loss: 0.5313 - val_accuracy: 0.8543\n",
      "Epoch 971/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2866 - accuracy: 0.9116 - val_loss: 0.5299 - val_accuracy: 0.8545\n",
      "Epoch 972/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2870 - accuracy: 0.9115 - val_loss: 0.5313 - val_accuracy: 0.8552\n",
      "Epoch 973/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2861 - accuracy: 0.9119 - val_loss: 0.5312 - val_accuracy: 0.8554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 974/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.9113 - val_loss: 0.5298 - val_accuracy: 0.8558\n",
      "Epoch 975/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2858 - accuracy: 0.9121 - val_loss: 0.5288 - val_accuracy: 0.8561\n",
      "Epoch 976/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2866 - accuracy: 0.9112 - val_loss: 0.5296 - val_accuracy: 0.8557\n",
      "Epoch 977/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2873 - accuracy: 0.9110 - val_loss: 0.5325 - val_accuracy: 0.8561\n",
      "Epoch 978/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2859 - accuracy: 0.9120 - val_loss: 0.5314 - val_accuracy: 0.8551\n",
      "Epoch 979/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2861 - accuracy: 0.9121 - val_loss: 0.5321 - val_accuracy: 0.8550\n",
      "Epoch 980/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2870 - accuracy: 0.9115 - val_loss: 0.5325 - val_accuracy: 0.8559\n",
      "Epoch 981/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2871 - accuracy: 0.9108 - val_loss: 0.5323 - val_accuracy: 0.8551\n",
      "Epoch 982/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2844 - accuracy: 0.9119 - val_loss: 0.5325 - val_accuracy: 0.8550\n",
      "Epoch 983/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2855 - accuracy: 0.9119 - val_loss: 0.5320 - val_accuracy: 0.8561\n",
      "Epoch 984/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2859 - accuracy: 0.9112 - val_loss: 0.5320 - val_accuracy: 0.8554\n",
      "Epoch 985/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2851 - accuracy: 0.9125 - val_loss: 0.5323 - val_accuracy: 0.8562\n",
      "Epoch 986/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2860 - accuracy: 0.9117 - val_loss: 0.5325 - val_accuracy: 0.8554\n",
      "Epoch 987/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2852 - accuracy: 0.9123 - val_loss: 0.5328 - val_accuracy: 0.8553\n",
      "Epoch 988/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2855 - accuracy: 0.9121 - val_loss: 0.5328 - val_accuracy: 0.8561\n",
      "Epoch 989/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2852 - accuracy: 0.9119 - val_loss: 0.5317 - val_accuracy: 0.8549\n",
      "Epoch 990/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2855 - accuracy: 0.9121 - val_loss: 0.5316 - val_accuracy: 0.8558\n",
      "Epoch 991/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2845 - accuracy: 0.9127 - val_loss: 0.5328 - val_accuracy: 0.8554\n",
      "Epoch 992/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2852 - accuracy: 0.9117 - val_loss: 0.5323 - val_accuracy: 0.8557\n",
      "Epoch 993/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2840 - accuracy: 0.9125 - val_loss: 0.5328 - val_accuracy: 0.8551\n",
      "Epoch 994/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2844 - accuracy: 0.9125 - val_loss: 0.5312 - val_accuracy: 0.8558\n",
      "Epoch 995/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2850 - accuracy: 0.9124 - val_loss: 0.5327 - val_accuracy: 0.8558\n",
      "Epoch 996/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2861 - accuracy: 0.9113 - val_loss: 0.5353 - val_accuracy: 0.8553\n",
      "Epoch 997/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2841 - accuracy: 0.9130 - val_loss: 0.5327 - val_accuracy: 0.8558\n",
      "Epoch 998/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2839 - accuracy: 0.9128 - val_loss: 0.5327 - val_accuracy: 0.8562\n",
      "Epoch 999/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2837 - accuracy: 0.9128 - val_loss: 0.5331 - val_accuracy: 0.8547\n",
      "Epoch 1000/1000\n",
      "216750/216750 [==============================] - 1s 5us/step - loss: 0.2835 - accuracy: 0.9129 - val_loss: 0.5334 - val_accuracy: 0.8567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2c1ef066ac8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = adagrad, loss = \"binary_crossentropy\", metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 1000, callbacks=callbacks, batch_size = 4096, validation_split= 0.15, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though validation loss starts increasing in the end, an improvement in accuracy is observed. This may be due to outputs of model being closer to the 0.5 decision threshold. In this way confidence in each prediction drops, but overall accuracy improves to better than 85%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9756645 , 0.75092804, 0.00140766, ..., 0.91801   , 0.6943645 ,\n",
       "       0.02278773], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final = np.array([i > 0.5 for i in predictions.flatten()], dtype = np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.84     18347\n",
      "           1       0.95      0.80      0.87     26653\n",
      "\n",
      "    accuracy                           0.86     45000\n",
      "   macro avg       0.86      0.87      0.86     45000\n",
      "weighted avg       0.88      0.86      0.86     45000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions_final, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report yields good pricision and recall values, along with an over f1 score value for the positive class. Final accuracy in test set is 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  predictions.flatten())\n",
    "auc = metrics.roc_auc_score(y_test, predictions.flatten())\n",
    "plt.plot(fpr,tpr,label=\"AUC = \"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
